{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e29ecf8-d6a5-42db-9c18-49e768a26248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Path /home/jovyan/butterfly/src/notebooks/BI ENCODER TRAINING /ToSubmit_Part2_O1_MNRL_Prefix5_Multi_GPU_Final_Rv1/step_10000/model not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m bi_encoder_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/butterfly/src/notebooks/BI ENCODER TRAINING /ToSubmit_Part2_O1_MNRL_Prefix5_Multi_GPU_Final_Rv1/step_10000/model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Load the bi-encoder model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbi_encoder_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mmax_seq_length \u001b[38;5;241m=\u001b[39m max_seq_length  \u001b[38;5;66;03m# Set max sequence length\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py:296\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_name_or_path):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# Not a path, load from hub\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mor\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[1;32m    300\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Path /home/jovyan/butterfly/src/notebooks/BI ENCODER TRAINING /ToSubmit_Part2_O1_MNRL_Prefix5_Multi_GPU_Final_Rv1/step_10000/model not found"
     ]
    }
   ],
   "source": [
    "#Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_10000\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/BI ENCODER TRAINING /ToSubmit_Part2_O1_MNRL_Prefix5_Multi_GPU_Final_Rv1/step_10000/model\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "model.max_seq_length = max_seq_length  # Set max sequence length\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create properly formatted passages for the legacy attributes\n",
    "# Format: \"passage: [label]. [definition]\" to avoid nested prefixes\n",
    "legacy_attributes_with_prefix = []\n",
    "for i in range(len(legacy_df)):\n",
    "    label = legacy_df['label'].iloc[i]\n",
    "    definition = legacy_df['definition'].iloc[i]\n",
    "    prefixed_text = f\"passage: {label}. {definition}\"\n",
    "    legacy_attributes_with_prefix.append(prefixed_text)\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "    batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/BI ENCODER TRAINING /golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Format queries with proper prefix\n",
    "sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "    batch = sentences_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Modified evaluation approach that uses relative ranking instead of absolute thresholds\n",
    "def get_metrics_by_ranking(K, top_n_results):\n",
    "    \"\"\"\n",
    "    Evaluate using relative ranking - take top N results per query regardless of score\n",
    "    \n",
    "    Args:\n",
    "        K: Number of results to retrieve with FAISS\n",
    "        top_n_results: Number of top results to consider as \"positive\" predictions\n",
    "    \"\"\"\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results using ranking approach\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels from top N results regardless of similarity score\n",
    "        for j in range(min(top_n_results, len(I_faiss[i]))):\n",
    "            idx_pos = I_faiss[i][j]\n",
    "            if idx_pos >= 0:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Keep the original threshold-based evaluation for comparison\n",
    "def get_metrics_by_threshold(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Only test K=500 for both evaluation approaches\n",
    "K = 2000\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5, 0.4]  # Original thresholds\n",
    "top_n_values = [1, 3, 5, 10, 15, 20, 30, 50, 80, 100, 150, 200, 300, 400, 500, 1000, 1500, 1600]  # Top N results to consider\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\")\n",
    "print(f\"\\n=== Testing with K={K} ===\")\n",
    "for threshold in thresholds:\n",
    "    avg_precision, avg_recall, avg_f1 = get_metrics_by_threshold(K, threshold)\n",
    "    print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "print(\"=== EVALUATION APPROACH 2: RANKING-BASED ===\")\n",
    "print(f\"=== Testing with K={K} ===\")\n",
    "for top_n in top_n_values:\n",
    "    if top_n <= K:  # Can't have top_n greater than K\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics_by_ranking(K, top_n)\n",
    "        print(f\"K={K}, Top_N={top_n}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9724a567-29dc-478c-b404-26b09bb698c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Path /home/jovyan/butterfly/src/notebooks/BI ENCODER TRAINING /ToSubmit_Part2_O1_MNRL_Prefix5_Multi_GPU_Final_Rv1/step_10000/model not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m bi_encoder_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/butterfly/src/notebooks/BI ENCODER TRAINING /ToSubmit_Part2_O1_MNRL_Prefix5_Multi_GPU_Final_Rv1/step_10000/model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Load the bi-encoder model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbi_encoder_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mmax_seq_length \u001b[38;5;241m=\u001b[39m max_seq_length  \u001b[38;5;66;03m# Set max sequence length\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py:296\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_name_or_path):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# Not a path, load from hub\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mor\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[1;32m    300\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Path /home/jovyan/butterfly/src/notebooks/BI ENCODER TRAINING /ToSubmit_Part2_O1_MNRL_Prefix5_Multi_GPU_Final_Rv1/step_10000/model not found"
     ]
    }
   ],
   "source": [
    "#Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_10000\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/BI ENCODER TRAINING /ToSubmit_Part2_O1_MNRL_Prefix5_Multi_GPU_Final_Rv1/step_10000/model\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "model.max_seq_length = max_seq_length  # Set max sequence length\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create properly formatted passages for the legacy attributes\n",
    "# Format: \"passage: [label]. [definition]\" to avoid nested prefixes\n",
    "legacy_attributes_with_prefix = []\n",
    "for i in range(len(legacy_df)):\n",
    "    label = legacy_df['label'].iloc[i]\n",
    "    definition = legacy_df['definition'].iloc[i]\n",
    "    prefixed_text = f\"passage: {label}. {definition}\"\n",
    "    legacy_attributes_with_prefix.append(prefixed_text)\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "    batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/BI ENCODER TRAINING /golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Format queries with proper prefix\n",
    "sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "    batch = sentences_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Modified evaluation approach that uses relative ranking instead of absolute thresholds\n",
    "def get_metrics_by_ranking(K, top_n_results):\n",
    "    \"\"\"\n",
    "    Evaluate using relative ranking - take top N results per query regardless of score\n",
    "    \n",
    "    Args:\n",
    "        K: Number of results to retrieve with FAISS\n",
    "        top_n_results: Number of top results to consider as \"positive\" predictions\n",
    "    \"\"\"\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results using ranking approach\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels from top N results regardless of similarity score\n",
    "        for j in range(min(top_n_results, len(I_faiss[i]))):\n",
    "            idx_pos = I_faiss[i][j]\n",
    "            if idx_pos >= 0:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Keep the original threshold-based evaluation for comparison\n",
    "def get_metrics_by_threshold(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Only test K=500 for both evaluation approaches\n",
    "K = 2000\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5, 0.4]  # Original thresholds\n",
    "top_n_values = [1, 3, 5, 10, 15, 20, 30, 50, 80, 100, 150, 200, 300, 400, 500, 1000, 1500, 1600]  # Top N results to consider\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\")\n",
    "print(f\"\\n=== Testing with K={K} ===\")\n",
    "for threshold in thresholds:\n",
    "    avg_precision, avg_recall, avg_f1 = get_metrics_by_threshold(K, threshold)\n",
    "    print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "print(\"=== EVALUATION APPROACH 2: RANKING-BASED ===\")\n",
    "print(f\"=== Testing with K={K} ===\")\n",
    "for top_n in top_n_values:\n",
    "    if top_n <= K:  # Can't have top_n greater than K\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics_by_ranking(K, top_n)\n",
    "        print(f\"K={K}, Top_N={top_n}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23e80482-7ad0-410f-b04a-ad61d201a31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m index\u001b[38;5;241m.\u001b[39madd(legacy_embeddings)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Load sentences from 'golden_dataset_sentences.csv'\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m sentences_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Rename 'Job_ID' to 'job_id' for consistency\u001b[39;00m\n\u001b[1;32m     70\u001b[0m sentences_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob_ID\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv'"
     ]
    }
   ],
   "source": [
    "#Part3_e5_mnrl_DDP_new_v2_Prefix4_Multi_GPU/epoch_1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/BI ENCODER TRAINING /Part3_e5_mnrl_DDP_new_v2_Prefix4_Multi_GPU/epoch_1\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "model.max_seq_length = max_seq_length  # Set max sequence length\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create properly formatted passages for the legacy attributes\n",
    "# Format: \"passage: [label]. [definition]\" to avoid nested prefixes\n",
    "legacy_attributes_with_prefix = []\n",
    "for i in range(len(legacy_df)):\n",
    "    label = legacy_df['label'].iloc[i]\n",
    "    definition = legacy_df['definition'].iloc[i]\n",
    "    prefixed_text = f\"passage: {label}. {definition}\"\n",
    "    legacy_attributes_with_prefix.append(prefixed_text)\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "    batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Format queries with proper prefix\n",
    "sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "    batch = sentences_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Modified evaluation approach that uses relative ranking instead of absolute thresholds\n",
    "def get_metrics_by_ranking(K, top_n_results):\n",
    "    \"\"\"\n",
    "    Evaluate using relative ranking - take top N results per query regardless of score\n",
    "    \n",
    "    Args:\n",
    "        K: Number of results to retrieve with FAISS\n",
    "        top_n_results: Number of top results to consider as \"positive\" predictions\n",
    "    \"\"\"\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results using ranking approach\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels from top N results regardless of similarity score\n",
    "        for j in range(min(top_n_results, len(I_faiss[i]))):\n",
    "            idx_pos = I_faiss[i][j]\n",
    "            if idx_pos >= 0:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Keep the original threshold-based evaluation for comparison\n",
    "def get_metrics_by_threshold(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Only test K=500 for both evaluation approaches\n",
    "K = 2000\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5, 0.4]  # Original thresholds\n",
    "top_n_values = [1, 3, 5, 10, 15, 20, 30, 50, 80, 100, 150, 200, 300, 400, 500, 1000, 1500, 1600]  # Top N results to consider\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\")\n",
    "print(f\"\\n=== Testing with K={K} ===\")\n",
    "for threshold in thresholds:\n",
    "    avg_precision, avg_recall, avg_f1 = get_metrics_by_threshold(K, threshold)\n",
    "    print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "print(\"=== EVALUATION APPROACH 2: RANKING-BASED ===\")\n",
    "print(f\"=== Testing with K={K} ===\")\n",
    "for top_n in top_n_values:\n",
    "    if top_n <= K:  # Can't have top_n greater than K\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics_by_ranking(K, top_n)\n",
    "        print(f\"K={K}, Top_N={top_n}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b7d0078-c215-4d8e-9534-347d771b65b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\n",
      "\n",
      "=== Testing with K=2000 ===\n",
      "K=2000, T=0.9: Precision=0.0076, Recall=0.0001, F1=0.0002\n",
      "K=2000, T=0.85: Precision=0.1706, Recall=0.0045, F1=0.0086\n",
      "K=2000, T=0.8: Precision=0.3336, Recall=0.0159, F1=0.0293\n",
      "K=2000, T=0.7: Precision=0.3074, Recall=0.0678, F1=0.0985\n",
      "K=2000, T=0.5: Precision=0.1045, Recall=0.3858, F1=0.1495\n",
      "K=2000, T=0.4: Precision=0.0492, Recall=0.6053, F1=0.0872\n",
      "=== EVALUATION APPROACH 2: RANKING-BASED ===\n",
      "=== Testing with K=2000 ===\n",
      "K=2000, Top_N=1: Precision=0.2734, Recall=0.1444, F1=0.1763\n",
      "K=2000, Top_N=3: Precision=0.1886, Recall=0.2704, F1=0.2073\n",
      "K=2000, Top_N=5: Precision=0.1496, Recall=0.3383, F1=0.1950\n",
      "K=2000, Top_N=10: Precision=0.1049, Recall=0.4355, F1=0.1605\n",
      "K=2000, Top_N=15: Precision=0.0839, Recall=0.4999, F1=0.1377\n",
      "K=2000, Top_N=20: Precision=0.0713, Recall=0.5438, F1=0.1214\n",
      "K=2000, Top_N=30: Precision=0.0555, Recall=0.6004, F1=0.0987\n",
      "K=2000, Top_N=50: Precision=0.0399, Recall=0.6691, F1=0.0738\n",
      "K=2000, Top_N=80: Precision=0.0295, Recall=0.7318, F1=0.0560\n",
      "K=2000, Top_N=100: Precision=0.0255, Recall=0.7597, F1=0.0488\n",
      "K=2000, Top_N=150: Precision=0.0194, Recall=0.7992, F1=0.0376\n",
      "K=2000, Top_N=200: Precision=0.0159, Recall=0.8257, F1=0.0311\n",
      "K=2000, Top_N=300: Precision=0.0121, Recall=0.8550, F1=0.0238\n",
      "K=2000, Top_N=400: Precision=0.0100, Recall=0.8713, F1=0.0197\n",
      "K=2000, Top_N=500: Precision=0.0087, Recall=0.8860, F1=0.0171\n",
      "K=2000, Top_N=1000: Precision=0.0057, Recall=0.9145, F1=0.0112\n",
      "K=2000, Top_N=1500: Precision=0.0045, Recall=0.9270, F1=0.0090\n",
      "K=2000, Top_N=1600: Precision=0.0044, Recall=0.9284, F1=0.0087\n"
     ]
    }
   ],
   "source": [
    "#Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_10000\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_10000/model\"\n",
    "butterfly/src/notebooks/20250324_155949_Rv4_bi_encoder_finetuned\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "model.max_seq_length = max_seq_length  # Set max sequence length\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create properly formatted passages for the legacy attributes\n",
    "# Format: \"passage: [label]. [definition]\" to avoid nested prefixes\n",
    "legacy_attributes_with_prefix = []\n",
    "for i in range(len(legacy_df)):\n",
    "    label = legacy_df['label'].iloc[i]\n",
    "    definition = legacy_df['definition'].iloc[i]\n",
    "    prefixed_text = f\"passage: {label}. {definition}\"\n",
    "    legacy_attributes_with_prefix.append(prefixed_text)\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "    batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Format queries with proper prefix\n",
    "sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "    batch = sentences_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Modified evaluation approach that uses relative ranking instead of absolute thresholds\n",
    "def get_metrics_by_ranking(K, top_n_results):\n",
    "    \"\"\"\n",
    "    Evaluate using relative ranking - take top N results per query regardless of score\n",
    "    \n",
    "    Args:\n",
    "        K: Number of results to retrieve with FAISS\n",
    "        top_n_results: Number of top results to consider as \"positive\" predictions\n",
    "    \"\"\"\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results using ranking approach\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels from top N results regardless of similarity score\n",
    "        for j in range(min(top_n_results, len(I_faiss[i]))):\n",
    "            idx_pos = I_faiss[i][j]\n",
    "            if idx_pos >= 0:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Keep the original threshold-based evaluation for comparison\n",
    "def get_metrics_by_threshold(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Only test K=500 for both evaluation approaches\n",
    "K = 2000\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5, 0.4]  # Original thresholds\n",
    "top_n_values = [1, 3, 5, 10, 15, 20, 30, 50, 80, 100, 150, 200, 300, 400, 500, 1000, 1500, 1600]  # Top N results to consider\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\")\n",
    "print(f\"\\n=== Testing with K={K} ===\")\n",
    "for threshold in thresholds:\n",
    "    avg_precision, avg_recall, avg_f1 = get_metrics_by_threshold(K, threshold)\n",
    "    print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "print(\"=== EVALUATION APPROACH 2: RANKING-BASED ===\")\n",
    "print(f\"=== Testing with K={K} ===\")\n",
    "for top_n in top_n_values:\n",
    "    if top_n <= K:  # Can't have top_n greater than K\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics_by_ranking(K, top_n)\n",
    "        print(f\"K={K}, Top_N={top_n}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab27e7b8-c0ca-4f10-9cf4-32b98ffb0eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.4.1, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\n",
      "\n",
      "=== Testing with K=2000 ===\n",
      "K=2000, T=0.9: Precision=0.0000, Recall=0.0000, F1=0.0000\n",
      "K=2000, T=0.85: Precision=0.0051, Recall=0.0001, F1=0.0002\n",
      "K=2000, T=0.8: Precision=0.0396, Recall=0.0006, F1=0.0011\n",
      "K=2000, T=0.7: Precision=0.2022, Recall=0.0070, F1=0.0132\n",
      "K=2000, T=0.5: Precision=0.2248, Recall=0.1884, F1=0.1721\n",
      "K=2000, T=0.4: Precision=0.0870, Recall=0.4179, F1=0.1306\n",
      "=== EVALUATION APPROACH 2: RANKING-BASED ===\n",
      "=== Testing with K=2000 ===\n",
      "K=2000, Top_N=1: Precision=0.2525, Recall=0.1334, F1=0.1634\n",
      "K=2000, Top_N=3: Precision=0.1750, Recall=0.2379, F1=0.1868\n",
      "K=2000, Top_N=5: Precision=0.1393, Recall=0.3010, F1=0.1780\n",
      "K=2000, Top_N=10: Precision=0.0997, Recall=0.3942, F1=0.1500\n",
      "K=2000, Top_N=15: Precision=0.0800, Recall=0.4516, F1=0.1293\n",
      "K=2000, Top_N=20: Precision=0.0676, Recall=0.4934, F1=0.1139\n",
      "K=2000, Top_N=30: Precision=0.0529, Recall=0.5472, F1=0.0933\n",
      "K=2000, Top_N=50: Precision=0.0389, Recall=0.6197, F1=0.0715\n",
      "K=2000, Top_N=80: Precision=0.0288, Recall=0.6808, F1=0.0544\n",
      "K=2000, Top_N=100: Precision=0.0249, Recall=0.7105, F1=0.0476\n",
      "K=2000, Top_N=150: Precision=0.0190, Recall=0.7551, F1=0.0367\n",
      "K=2000, Top_N=200: Precision=0.0157, Recall=0.7853, F1=0.0305\n",
      "K=2000, Top_N=300: Precision=0.0121, Recall=0.8215, F1=0.0237\n",
      "K=2000, Top_N=400: Precision=0.0100, Recall=0.8467, F1=0.0198\n",
      "K=2000, Top_N=500: Precision=0.0087, Recall=0.8619, F1=0.0172\n",
      "K=2000, Top_N=1000: Precision=0.0058, Recall=0.9014, F1=0.0115\n",
      "K=2000, Top_N=1500: Precision=0.0046, Recall=0.9162, F1=0.0092\n",
      "K=2000, Top_N=1600: Precision=0.0045, Recall=0.9186, F1=0.0089\n"
     ]
    }
   ],
   "source": [
    "#20250324_155949_Rv4_bi_encoder_finetuned\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/20250324_155949_Rv4_bi_encoder_finetuned\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "model.max_seq_length = max_seq_length  # Set max sequence length\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create properly formatted passages for the legacy attributes\n",
    "# Format: \"passage: [label]. [definition]\" to avoid nested prefixes\n",
    "legacy_attributes_with_prefix = []\n",
    "for i in range(len(legacy_df)):\n",
    "    label = legacy_df['label'].iloc[i]\n",
    "    definition = legacy_df['definition'].iloc[i]\n",
    "    prefixed_text = f\"passage: {label}. {definition}\"\n",
    "    legacy_attributes_with_prefix.append(prefixed_text)\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "    batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Format queries with proper prefix\n",
    "sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "    batch = sentences_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Modified evaluation approach that uses relative ranking instead of absolute thresholds\n",
    "def get_metrics_by_ranking(K, top_n_results):\n",
    "    \"\"\"\n",
    "    Evaluate using relative ranking - take top N results per query regardless of score\n",
    "    \n",
    "    Args:\n",
    "        K: Number of results to retrieve with FAISS\n",
    "        top_n_results: Number of top results to consider as \"positive\" predictions\n",
    "    \"\"\"\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results using ranking approach\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels from top N results regardless of similarity score\n",
    "        for j in range(min(top_n_results, len(I_faiss[i]))):\n",
    "            idx_pos = I_faiss[i][j]\n",
    "            if idx_pos >= 0:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Keep the original threshold-based evaluation for comparison\n",
    "def get_metrics_by_threshold(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Only test K=500 for both evaluation approaches\n",
    "K = 2000\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5, 0.4]  # Original thresholds\n",
    "top_n_values = [1, 3, 5, 10, 15, 20, 30, 50, 80, 100, 150, 200, 300, 400, 500, 1000, 1500, 1600]  # Top N results to consider\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\")\n",
    "print(f\"\\n=== Testing with K={K} ===\")\n",
    "for threshold in thresholds:\n",
    "    avg_precision, avg_recall, avg_f1 = get_metrics_by_threshold(K, threshold)\n",
    "    print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "print(\"=== EVALUATION APPROACH 2: RANKING-BASED ===\")\n",
    "print(f\"=== Testing with K={K} ===\")\n",
    "for top_n in top_n_values:\n",
    "    if top_n <= K:  # Can't have top_n greater than K\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics_by_ranking(K, top_n)\n",
    "        print(f\"K={K}, Top_N={top_n}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88e41fa9-c23c-4feb-bc3b-f7e89151d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\n",
      "\n",
      "=== Testing with K=2000 ===\n",
      "K=2000, T=0.9: Precision=0.0000, Recall=0.0000, F1=0.0000\n",
      "K=2000, T=0.85: Precision=0.0480, Recall=0.0014, F1=0.0027\n",
      "K=2000, T=0.8: Precision=0.2034, Recall=0.0054, F1=0.0103\n",
      "K=2000, T=0.7: Precision=0.4041, Recall=0.0456, F1=0.0770\n",
      "K=2000, T=0.5: Precision=0.1385, Recall=0.3239, F1=0.1761\n",
      "K=2000, T=0.4: Precision=0.0625, Recall=0.5165, F1=0.1071\n",
      "=== EVALUATION APPROACH 2: RANKING-BASED ===\n",
      "=== Testing with K=2000 ===\n",
      "K=2000, Top_N=1: Precision=0.2515, Recall=0.1320, F1=0.1619\n",
      "K=2000, Top_N=3: Precision=0.1742, Recall=0.2407, F1=0.1879\n",
      "K=2000, Top_N=5: Precision=0.1387, Recall=0.3046, F1=0.1789\n",
      "K=2000, Top_N=10: Precision=0.0967, Recall=0.4007, F1=0.1488\n",
      "K=2000, Top_N=15: Precision=0.0775, Recall=0.4535, F1=0.1270\n",
      "K=2000, Top_N=20: Precision=0.0661, Recall=0.4946, F1=0.1125\n",
      "K=2000, Top_N=30: Precision=0.0519, Recall=0.5521, F1=0.0924\n",
      "K=2000, Top_N=50: Precision=0.0378, Recall=0.6157, F1=0.0697\n",
      "K=2000, Top_N=80: Precision=0.0281, Recall=0.6725, F1=0.0531\n",
      "K=2000, Top_N=100: Precision=0.0243, Recall=0.6964, F1=0.0464\n",
      "K=2000, Top_N=150: Precision=0.0186, Recall=0.7401, F1=0.0359\n",
      "K=2000, Top_N=200: Precision=0.0154, Recall=0.7726, F1=0.0300\n",
      "K=2000, Top_N=300: Precision=0.0118, Recall=0.8084, F1=0.0232\n",
      "K=2000, Top_N=400: Precision=0.0098, Recall=0.8289, F1=0.0193\n",
      "K=2000, Top_N=500: Precision=0.0085, Recall=0.8438, F1=0.0169\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 209\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m top_n \u001b[38;5;129;01min\u001b[39;00m top_n_values:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m top_n \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m K:  \u001b[38;5;66;03m# Can't have top_n greater than K\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m         avg_precision, avg_recall, avg_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_metrics_by_ranking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mK\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Top_N=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Precision=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_recall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 120\u001b[0m, in \u001b[0;36mget_metrics_by_ranking\u001b[0;34m(K, top_n_results)\u001b[0m\n\u001b[1;32m    118\u001b[0m         idx_pos \u001b[38;5;241m=\u001b[39m I_faiss[i][j]\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m idx_pos \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 120\u001b[0m             label \u001b[38;5;241m=\u001b[39m \u001b[43mlegacy_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_pos\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    121\u001b[0m             job_predictions[job_id]\u001b[38;5;241m.\u001b[39madd(normalize_attribute(label))\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:1141\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m-> 1141\u001b[0m     \u001b[43mcheck_dict_or_set_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(key) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m   1143\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mlist\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m is_iterator(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py:2683\u001b[0m, in \u001b[0;36mcheck_dict_or_set_indexers\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_dict_or_set_indexers\u001b[39m(key) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m \u001b[38;5;124;03m    Check if the indexer is or contains a dict or set, which is no longer allowed.\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m-> 2683\u001b[0m         \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2684\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m   2685\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mset\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[1;32m   2686\u001b[0m     ):\n\u001b[1;32m   2687\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2688\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a set as an indexer is not supported. Use a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2689\u001b[0m         )\n\u001b[1;32m   2691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2692\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m   2693\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m   2694\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[1;32m   2695\u001b[0m     ):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_20000\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/final_model\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "model.max_seq_length = max_seq_length  # Set max sequence length\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create properly formatted passages for the legacy attributes\n",
    "# Format: \"passage: [label]. [definition]\" to avoid nested prefixes\n",
    "legacy_attributes_with_prefix = []\n",
    "for i in range(len(legacy_df)):\n",
    "    label = legacy_df['label'].iloc[i]\n",
    "    definition = legacy_df['definition'].iloc[i]\n",
    "    prefixed_text = f\"passage: {label}. {definition}\"\n",
    "    legacy_attributes_with_prefix.append(prefixed_text)\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "    batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Format queries with proper prefix\n",
    "sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "    batch = sentences_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Modified evaluation approach that uses relative ranking instead of absolute thresholds\n",
    "def get_metrics_by_ranking(K, top_n_results):\n",
    "    \"\"\"\n",
    "    Evaluate using relative ranking - take top N results per query regardless of score\n",
    "    \n",
    "    Args:\n",
    "        K: Number of results to retrieve with FAISS\n",
    "        top_n_results: Number of top results to consider as \"positive\" predictions\n",
    "    \"\"\"\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results using ranking approach\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels from top N results regardless of similarity score\n",
    "        for j in range(min(top_n_results, len(I_faiss[i]))):\n",
    "            idx_pos = I_faiss[i][j]\n",
    "            if idx_pos >= 0:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Keep the original threshold-based evaluation for comparison\n",
    "def get_metrics_by_threshold(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Only test K=500 for both evaluation approaches\n",
    "K = 2000\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5, 0.4]  # Original thresholds\n",
    "top_n_values = [1, 3, 5, 10, 15, 20, 30, 50, 80, 100, 150, 200, 300, 400, 500, 1000, 1500, 1600]  # Top N results to consider\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\")\n",
    "print(f\"\\n=== Testing with K={K} ===\")\n",
    "for threshold in thresholds:\n",
    "    avg_precision, avg_recall, avg_f1 = get_metrics_by_threshold(K, threshold)\n",
    "    print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "print(\"=== EVALUATION APPROACH 2: RANKING-BASED ===\")\n",
    "print(f\"=== Testing with K={K} ===\")\n",
    "for top_n in top_n_values:\n",
    "    if top_n <= K:  # Can't have top_n greater than K\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics_by_ranking(K, top_n)\n",
    "        print(f\"K={K}, Top_N={top_n}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61bc5797-49ed-4375-b691-cd21c7c09167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "Evaluating model: epoch_1\n",
      "Recall for epoch_1: 0.9233\n",
      "\n",
      "Evaluating model: epoch_4\n",
      "Recall for epoch_4: 0.9046\n",
      "\n",
      "Evaluating model: step_70000\n",
      "Recall for step_70000: 0.9041\n",
      "\n",
      "Evaluating model: step_20000\n",
      "Recall for step_20000: 0.9195\n",
      "\n",
      "Evaluating model: step_50000\n",
      "Recall for step_50000: 0.9103\n",
      "\n",
      "Evaluating model: epoch_5\n",
      "Recall for epoch_5: 0.9038\n",
      "\n",
      "Evaluating model: step_30000\n",
      "Recall for step_30000: 0.9175\n",
      "\n",
      "Evaluating model: step_80000\n",
      "Recall for step_80000: 0.9026\n",
      "\n",
      "Evaluating model: epoch_3\n",
      "Recall for epoch_3: 0.9047\n",
      "\n",
      "Evaluating model: step_10000\n",
      "Recall for step_10000: 0.9284\n",
      "\n",
      "Evaluating model: step_40000\n",
      "Recall for step_40000: 0.9063\n",
      "\n",
      "Evaluating model: step_60000\n",
      "Recall for step_60000: 0.9036\n",
      "\n",
      "Evaluating model: epoch_2\n",
      "Recall for epoch_2: 0.9108\n",
      "\n",
      "Evaluating model: final_model\n",
      "Recall for final_model: 0.9038\n",
      "\n",
      "==================================================\n",
      "MODEL EVALUATION SUMMARY (K=2000, Top_N=1600)\n",
      "==================================================\n",
      "1. step_10000: Recall = 0.9284\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_10000/model\n",
      "--------------------------------------------------\n",
      "2. epoch_1: Recall = 0.9233\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/epoch_1/model\n",
      "--------------------------------------------------\n",
      "3. step_20000: Recall = 0.9195\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_20000/model\n",
      "--------------------------------------------------\n",
      "4. step_30000: Recall = 0.9175\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_30000/model\n",
      "--------------------------------------------------\n",
      "5. epoch_2: Recall = 0.9108\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/epoch_2/model\n",
      "--------------------------------------------------\n",
      "6. step_50000: Recall = 0.9103\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_50000/model\n",
      "--------------------------------------------------\n",
      "7. step_40000: Recall = 0.9063\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_40000/model\n",
      "--------------------------------------------------\n",
      "8. epoch_3: Recall = 0.9047\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/epoch_3/model\n",
      "--------------------------------------------------\n",
      "9. epoch_4: Recall = 0.9046\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/epoch_4/model\n",
      "--------------------------------------------------\n",
      "10. step_70000: Recall = 0.9041\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_70000/model\n",
      "--------------------------------------------------\n",
      "11. epoch_5: Recall = 0.9038\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/epoch_5/model\n",
      "--------------------------------------------------\n",
      "12. final_model: Recall = 0.9038\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/final_model\n",
      "--------------------------------------------------\n",
      "13. step_60000: Recall = 0.9036\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_60000/model\n",
      "--------------------------------------------------\n",
      "14. step_80000: Recall = 0.9026\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_80000/model\n",
      "--------------------------------------------------\n",
      "\n",
      "BEST MODEL: step_10000\n",
      "Recall: 0.9284\n",
      "Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_10000/model\n"
     ]
    }
   ],
   "source": [
    "#LOOP THROUGH ALL\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Base directory path\n",
    "base_dir = \"/home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final\"\n",
    "\n",
    "# Load the datasets once since they don't change\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Prepare the data\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Prepare ground truth\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "def evaluate_model(model_path, K=2000, top_n=1600):\n",
    "    \"\"\"Evaluate a single model and return its recall score\"\"\"\n",
    "    try:\n",
    "        # Load the bi-encoder model\n",
    "        model = SentenceTransformer(model_path)\n",
    "        model.max_seq_length = max_seq_length\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.to(device)\n",
    "        \n",
    "        # Create formatted passages for legacy attributes\n",
    "        legacy_attributes_with_prefix = []\n",
    "        for i in range(len(legacy_df)):\n",
    "            label = legacy_df['label'].iloc[i]\n",
    "            definition = legacy_df['definition'].iloc[i]\n",
    "            prefixed_text = f\"passage: {label}. {definition}\"\n",
    "            legacy_attributes_with_prefix.append(prefixed_text)\n",
    "        \n",
    "        # Encode with normalization, batch for large datasets\n",
    "        batch_size = 128\n",
    "        legacy_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "            batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "            batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "            legacy_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = legacy_embeddings.shape[1]\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        index.add(legacy_embeddings)\n",
    "        \n",
    "        # Format queries with proper prefix\n",
    "        sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "        \n",
    "        # Encode sentences in batches\n",
    "        sentence_embeddings = []\n",
    "        for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "            batch = sentences_with_prefix[i:i+batch_size]\n",
    "            batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "            sentence_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "        \n",
    "        # Get search results\n",
    "        D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "        \n",
    "        # Process results using ranking approach\n",
    "        job_predictions = {}\n",
    "        for i, row in enumerate(sentences_df.itertuples()):\n",
    "            job_id = row.job_id\n",
    "            \n",
    "            if job_id not in job_predictions:\n",
    "                job_predictions[job_id] = set()\n",
    "            \n",
    "            # Get labels from top N results regardless of similarity score\n",
    "            for j in range(min(top_n, len(I_faiss[i]))):\n",
    "                idx_pos = I_faiss[i][j]\n",
    "                if idx_pos >= 0:\n",
    "                    label = legacy_df['label'].iloc[idx_pos]\n",
    "                    job_predictions[job_id].add(normalize_attribute(label))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        recalls = []\n",
    "        \n",
    "        for job_id in ground_truth:\n",
    "            gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "            pred = set(job_predictions.get(job_id, []))\n",
    "            tp = len(gt.intersection(pred))\n",
    "            fn = len(gt - pred)\n",
    "            recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        avg_recall = np.mean(recalls)\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        del legacy_embeddings\n",
    "        del sentence_embeddings\n",
    "        del index\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return avg_recall\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model at {model_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Find all model directories\n",
    "model_results = []\n",
    "\n",
    "# Look for all directories containing 'epoch' or 'step' in the base directory\n",
    "for folder_name in os.listdir(base_dir):\n",
    "    if 'epoch' in folder_name or 'step' in folder_name:\n",
    "        model_path = os.path.join(base_dir, folder_name, 'model')\n",
    "        if os.path.exists(model_path) and os.path.isdir(model_path):\n",
    "            print(f\"\\nEvaluating model: {folder_name}\")\n",
    "            recall = evaluate_model(model_path)\n",
    "            if recall is not None:\n",
    "                model_results.append({\n",
    "                    'folder': folder_name,\n",
    "                    'path': model_path,\n",
    "                    'recall': recall\n",
    "                })\n",
    "                print(f\"Recall for {folder_name}: {recall:.4f}\")\n",
    "\n",
    "# Also check the final_model if it exists\n",
    "final_model_path = os.path.join(base_dir, 'final_model')\n",
    "if os.path.exists(final_model_path) and os.path.isdir(final_model_path):\n",
    "    print(f\"\\nEvaluating model: final_model\")\n",
    "    recall = evaluate_model(final_model_path)\n",
    "    if recall is not None:\n",
    "        model_results.append({\n",
    "            'folder': 'final_model',\n",
    "            'path': final_model_path,\n",
    "            'recall': recall\n",
    "        })\n",
    "        print(f\"Recall for final_model: {recall:.4f}\")\n",
    "\n",
    "# Sort results by recall (descending)\n",
    "model_results = sorted(model_results, key=lambda x: x['recall'], reverse=True)\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL EVALUATION SUMMARY (K=2000, Top_N=1600)\")\n",
    "print(\"=\"*50)\n",
    "for i, result in enumerate(model_results):\n",
    "    print(f\"{i+1}. {result['folder']}: Recall = {result['recall']:.4f}\")\n",
    "    print(f\"   Path: {result['path']}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "if model_results:\n",
    "    best_model = model_results[0]\n",
    "    print(f\"\\nBEST MODEL: {best_model['folder']}\")\n",
    "    print(f\"Recall: {best_model['recall']:.4f}\")\n",
    "    print(f\"Path: {best_model['path']}\")\n",
    "else:\n",
    "    print(\"\\nNo models found or all evaluations failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d951a8b-0bae-47fb-972c-9d4c6bd1888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "Evaluating model: epoch_1\n",
      "Recall for epoch_1: 0.9233\n",
      "\n",
      "Evaluating model: epoch_4\n",
      "Recall for epoch_4: 0.9046\n",
      "\n",
      "Evaluating model: step_70000\n",
      "Recall for step_70000: 0.9041\n",
      "\n",
      "Evaluating model: step_20000\n",
      "Recall for step_20000: 0.9195\n",
      "\n",
      "Evaluating model: step_50000\n",
      "Recall for step_50000: 0.9103\n",
      "\n",
      "Evaluating model: epoch_5\n",
      "Recall for epoch_5: 0.9038\n",
      "\n",
      "Evaluating model: step_30000\n",
      "Recall for step_30000: 0.9175\n",
      "\n",
      "Evaluating model: step_80000\n",
      "Recall for step_80000: 0.9026\n",
      "\n",
      "Evaluating model: epoch_3\n",
      "Recall for epoch_3: 0.9047\n",
      "\n",
      "Evaluating model: step_10000\n",
      "Recall for step_10000: 0.9284\n",
      "\n",
      "Evaluating model: step_40000\n",
      "Recall for step_40000: 0.9063\n",
      "\n",
      "Evaluating model: step_60000\n",
      "Recall for step_60000: 0.9036\n",
      "\n",
      "Evaluating model: epoch_2\n",
      "Recall for epoch_2: 0.9108\n",
      "\n",
      "Evaluating model: final_model\n",
      "Recall for final_model: 0.9038\n",
      "\n",
      "==================================================\n",
      "MODEL EVALUATION SUMMARY (K=2000, Top_N=1600)\n",
      "==================================================\n",
      "1. step_10000: Recall = 0.9284\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_10000/model\n",
      "--------------------------------------------------\n",
      "2. epoch_1: Recall = 0.9233\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/epoch_1/model\n",
      "--------------------------------------------------\n",
      "3. step_20000: Recall = 0.9195\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_20000/model\n",
      "--------------------------------------------------\n",
      "4. step_30000: Recall = 0.9175\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_30000/model\n",
      "--------------------------------------------------\n",
      "5. epoch_2: Recall = 0.9108\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/epoch_2/model\n",
      "--------------------------------------------------\n",
      "6. step_50000: Recall = 0.9103\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_50000/model\n",
      "--------------------------------------------------\n",
      "7. step_40000: Recall = 0.9063\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_40000/model\n",
      "--------------------------------------------------\n",
      "8. epoch_3: Recall = 0.9047\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/epoch_3/model\n",
      "--------------------------------------------------\n",
      "9. epoch_4: Recall = 0.9046\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/epoch_4/model\n",
      "--------------------------------------------------\n",
      "10. step_70000: Recall = 0.9041\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_70000/model\n",
      "--------------------------------------------------\n",
      "11. epoch_5: Recall = 0.9038\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/epoch_5/model\n",
      "--------------------------------------------------\n",
      "12. final_model: Recall = 0.9038\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/final_model\n",
      "--------------------------------------------------\n",
      "13. step_60000: Recall = 0.9036\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_60000/model\n",
      "--------------------------------------------------\n",
      "14. step_80000: Recall = 0.9026\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_80000/model\n",
      "--------------------------------------------------\n",
      "\n",
      "BEST MODEL: step_10000\n",
      "Recall: 0.9284\n",
      "Path: /home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_10000/model\n"
     ]
    }
   ],
   "source": [
    "#LOOP THROUGH ALL with Prefix (Part2_O1_MNRL_Prefix5_Multi_GPU_Final)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Base directory path\n",
    "base_dir = \"/home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final\"\n",
    "\n",
    "# Load the datasets once since they don't change\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Prepare the data\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Prepare ground truth\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "def evaluate_model(model_path, K=2000, top_n=1600):\n",
    "    \"\"\"Evaluate a single model and return its recall score\"\"\"\n",
    "    try:\n",
    "        # Load the bi-encoder model\n",
    "        model = SentenceTransformer(model_path)\n",
    "        model.max_seq_length = max_seq_length\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.to(device)\n",
    "        \n",
    "        # Create formatted passages for legacy attributes\n",
    "        legacy_attributes_with_prefix = []\n",
    "        for i in range(len(legacy_df)):\n",
    "            label = legacy_df['label'].iloc[i]\n",
    "            definition = legacy_df['definition'].iloc[i]\n",
    "            prefixed_text = f\"passage: {label}. {definition}\"\n",
    "            legacy_attributes_with_prefix.append(prefixed_text)\n",
    "        \n",
    "        # Encode with normalization, batch for large datasets\n",
    "        batch_size = 128\n",
    "        legacy_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "            batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "            batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "            legacy_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = legacy_embeddings.shape[1]\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        index.add(legacy_embeddings)\n",
    "        \n",
    "        # Format queries with proper prefix\n",
    "        sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "        \n",
    "        # Encode sentences in batches\n",
    "        sentence_embeddings = []\n",
    "        for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "            batch = sentences_with_prefix[i:i+batch_size]\n",
    "            batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "            sentence_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "        \n",
    "        # Get search results\n",
    "        D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "        \n",
    "        # Process results using ranking approach\n",
    "        job_predictions = {}\n",
    "        for i, row in enumerate(sentences_df.itertuples()):\n",
    "            job_id = row.job_id\n",
    "            \n",
    "            if job_id not in job_predictions:\n",
    "                job_predictions[job_id] = set()\n",
    "            \n",
    "            # Get labels from top N results regardless of similarity score\n",
    "            for j in range(min(top_n, len(I_faiss[i]))):\n",
    "                idx_pos = I_faiss[i][j]\n",
    "                if idx_pos >= 0:\n",
    "                    label = legacy_df['label'].iloc[idx_pos]\n",
    "                    job_predictions[job_id].add(normalize_attribute(label))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        recalls = []\n",
    "        \n",
    "        for job_id in ground_truth:\n",
    "            gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "            pred = set(job_predictions.get(job_id, []))\n",
    "            tp = len(gt.intersection(pred))\n",
    "            fn = len(gt - pred)\n",
    "            recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        avg_recall = np.mean(recalls)\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        del legacy_embeddings\n",
    "        del sentence_embeddings\n",
    "        del index\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return avg_recall\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model at {model_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Find all model directories\n",
    "model_results = []\n",
    "\n",
    "# Look for all directories containing 'epoch' or 'step' in the base directory\n",
    "for folder_name in os.listdir(base_dir):\n",
    "    if 'epoch' in folder_name or 'step' in folder_name:\n",
    "        model_path = os.path.join(base_dir, folder_name, 'model')\n",
    "        if os.path.exists(model_path) and os.path.isdir(model_path):\n",
    "            print(f\"\\nEvaluating model: {folder_name}\")\n",
    "            recall = evaluate_model(model_path)\n",
    "            if recall is not None:\n",
    "                model_results.append({\n",
    "                    'folder': folder_name,\n",
    "                    'path': model_path,\n",
    "                    'recall': recall\n",
    "                })\n",
    "                print(f\"Recall for {folder_name}: {recall:.4f}\")\n",
    "\n",
    "# Also check the final_model if it exists\n",
    "final_model_path = os.path.join(base_dir, 'final_model')\n",
    "if os.path.exists(final_model_path) and os.path.isdir(final_model_path):\n",
    "    print(f\"\\nEvaluating model: final_model\")\n",
    "    recall = evaluate_model(final_model_path)\n",
    "    if recall is not None:\n",
    "        model_results.append({\n",
    "            'folder': 'final_model',\n",
    "            'path': final_model_path,\n",
    "            'recall': recall\n",
    "        })\n",
    "        print(f\"Recall for final_model: {recall:.4f}\")\n",
    "\n",
    "# Sort results by recall (descending)\n",
    "model_results = sorted(model_results, key=lambda x: x['recall'], reverse=True)\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL EVALUATION SUMMARY (K=2000, Top_N=1600)\")\n",
    "print(\"=\"*50)\n",
    "for i, result in enumerate(model_results):\n",
    "    print(f\"{i+1}. {result['folder']}: Recall = {result['recall']:.4f}\")\n",
    "    print(f\"   Path: {result['path']}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "if model_results:\n",
    "    best_model = model_results[0]\n",
    "    print(f\"\\nBEST MODEL: {best_model['folder']}\")\n",
    "    print(f\"Recall: {best_model['recall']:.4f}\")\n",
    "    print(f\"Path: {best_model['path']}\")\n",
    "else:\n",
    "    print(\"\\nNo models found or all evaluations failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b128f28-0e63-412b-8a42-84713e4ae2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "Evaluating model: epoch_1\n",
      "Recall for epoch_1: 0.9251\n",
      "\n",
      "Evaluating model: epoch_4\n",
      "Recall for epoch_4: 0.9079\n",
      "\n",
      "Evaluating model: step_20000\n",
      "Recall for step_20000: 0.9215\n",
      "\n",
      "Evaluating model: step_50000\n",
      "Recall for step_50000: 0.9122\n",
      "\n",
      "Evaluating model: step_30000\n",
      "Recall for step_30000: 0.9197\n",
      "\n",
      "Evaluating model: epoch_3\n",
      "Recall for epoch_3: 0.9083\n",
      "\n",
      "Evaluating model: step_10000\n",
      "Recall for step_10000: 0.9290\n",
      "\n",
      "Evaluating model: step_40000\n",
      "Recall for step_40000: 0.9074\n",
      "\n",
      "Evaluating model: step_60000\n",
      "Recall for step_60000: 0.9077\n",
      "\n",
      "Evaluating model: epoch_2\n",
      "Recall for epoch_2: 0.9148\n",
      "\n",
      "==================================================\n",
      "MODEL EVALUATION SUMMARY (K=2000, Top_N=1600)\n",
      "==================================================\n",
      "1. step_10000: Recall = 0.9290\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/runs/e5_improved/step_10000/model\n",
      "--------------------------------------------------\n",
      "2. epoch_1: Recall = 0.9251\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/runs/e5_improved/epoch_1/model\n",
      "--------------------------------------------------\n",
      "3. step_20000: Recall = 0.9215\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/runs/e5_improved/step_20000/model\n",
      "--------------------------------------------------\n",
      "4. step_30000: Recall = 0.9197\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/runs/e5_improved/step_30000/model\n",
      "--------------------------------------------------\n",
      "5. epoch_2: Recall = 0.9148\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/runs/e5_improved/epoch_2/model\n",
      "--------------------------------------------------\n",
      "6. step_50000: Recall = 0.9122\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/runs/e5_improved/step_50000/model\n",
      "--------------------------------------------------\n",
      "7. epoch_3: Recall = 0.9083\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/runs/e5_improved/epoch_3/model\n",
      "--------------------------------------------------\n",
      "8. epoch_4: Recall = 0.9079\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/runs/e5_improved/epoch_4/model\n",
      "--------------------------------------------------\n",
      "9. step_60000: Recall = 0.9077\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/runs/e5_improved/step_60000/model\n",
      "--------------------------------------------------\n",
      "10. step_40000: Recall = 0.9074\n",
      "   Path: /home/jovyan/butterfly/src/notebooks/runs/e5_improved/step_40000/model\n",
      "--------------------------------------------------\n",
      "\n",
      "BEST MODEL: step_10000\n",
      "Recall: 0.9290\n",
      "Path: /home/jovyan/butterfly/src/notebooks/runs/e5_improved/step_10000/model\n"
     ]
    }
   ],
   "source": [
    "#LOOP THROUGH ALL with Prefix (Part2_O1_MNRL_Prefix5_Multi_GPU_Final)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Base directory path\n",
    "base_dir = \"/home/jovyan/butterfly/src/notebooks/runs/e5_improved\"\n",
    "\n",
    "# Load the datasets once since they don't change\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Prepare the data\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Prepare ground truth\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "def evaluate_model(model_path, K=2000, top_n=1600):\n",
    "    \"\"\"Evaluate a single model and return its recall score\"\"\"\n",
    "    try:\n",
    "        # Load the bi-encoder model\n",
    "        model = SentenceTransformer(model_path)\n",
    "        model.max_seq_length = max_seq_length\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.to(device)\n",
    "        \n",
    "        # Create formatted passages for legacy attributes\n",
    "        legacy_attributes_with_prefix = []\n",
    "        for i in range(len(legacy_df)):\n",
    "            label = legacy_df['label'].iloc[i]\n",
    "            definition = legacy_df['definition'].iloc[i]\n",
    "            prefixed_text = f\"passage: {label}. {definition}\"\n",
    "            legacy_attributes_with_prefix.append(prefixed_text)\n",
    "        \n",
    "        # Encode with normalization, batch for large datasets\n",
    "        batch_size = 128\n",
    "        legacy_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "            batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "            batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "            legacy_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = legacy_embeddings.shape[1]\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        index.add(legacy_embeddings)\n",
    "        \n",
    "        # Format queries with proper prefix\n",
    "        sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "        \n",
    "        # Encode sentences in batches\n",
    "        sentence_embeddings = []\n",
    "        for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "            batch = sentences_with_prefix[i:i+batch_size]\n",
    "            batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "            sentence_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "        \n",
    "        # Get search results\n",
    "        D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "        \n",
    "        # Process results using ranking approach\n",
    "        job_predictions = {}\n",
    "        for i, row in enumerate(sentences_df.itertuples()):\n",
    "            job_id = row.job_id\n",
    "            \n",
    "            if job_id not in job_predictions:\n",
    "                job_predictions[job_id] = set()\n",
    "            \n",
    "            # Get labels from top N results regardless of similarity score\n",
    "            for j in range(min(top_n, len(I_faiss[i]))):\n",
    "                idx_pos = I_faiss[i][j]\n",
    "                if idx_pos >= 0:\n",
    "                    label = legacy_df['label'].iloc[idx_pos]\n",
    "                    job_predictions[job_id].add(normalize_attribute(label))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        recalls = []\n",
    "        \n",
    "        for job_id in ground_truth:\n",
    "            gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "            pred = set(job_predictions.get(job_id, []))\n",
    "            tp = len(gt.intersection(pred))\n",
    "            fn = len(gt - pred)\n",
    "            recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        avg_recall = np.mean(recalls)\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        del legacy_embeddings\n",
    "        del sentence_embeddings\n",
    "        del index\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return avg_recall\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating model at {model_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Find all model directories\n",
    "model_results = []\n",
    "\n",
    "# Look for all directories containing 'epoch' or 'step' in the base directory\n",
    "for folder_name in os.listdir(base_dir):\n",
    "    if 'epoch' in folder_name or 'step' in folder_name:\n",
    "        model_path = os.path.join(base_dir, folder_name, 'model')\n",
    "        if os.path.exists(model_path) and os.path.isdir(model_path):\n",
    "            print(f\"\\nEvaluating model: {folder_name}\")\n",
    "            recall = evaluate_model(model_path)\n",
    "            if recall is not None:\n",
    "                model_results.append({\n",
    "                    'folder': folder_name,\n",
    "                    'path': model_path,\n",
    "                    'recall': recall\n",
    "                })\n",
    "                print(f\"Recall for {folder_name}: {recall:.4f}\")\n",
    "\n",
    "# Also check the final_model if it exists\n",
    "final_model_path = os.path.join(base_dir, 'final_model')\n",
    "if os.path.exists(final_model_path) and os.path.isdir(final_model_path):\n",
    "    print(f\"\\nEvaluating model: final_model\")\n",
    "    recall = evaluate_model(final_model_path)\n",
    "    if recall is not None:\n",
    "        model_results.append({\n",
    "            'folder': 'final_model',\n",
    "            'path': final_model_path,\n",
    "            'recall': recall\n",
    "        })\n",
    "        print(f\"Recall for final_model: {recall:.4f}\")\n",
    "\n",
    "# Sort results by recall (descending)\n",
    "model_results = sorted(model_results, key=lambda x: x['recall'], reverse=True)\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL EVALUATION SUMMARY (K=2000, Top_N=1600)\")\n",
    "print(\"=\"*50)\n",
    "for i, result in enumerate(model_results):\n",
    "    print(f\"{i+1}. {result['folder']}: Recall = {result['recall']:.4f}\")\n",
    "    print(f\"   Path: {result['path']}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "if model_results:\n",
    "    best_model = model_results[0]\n",
    "    print(f\"\\nBEST MODEL: {best_model['folder']}\")\n",
    "    print(f\"Recall: {best_model['recall']:.4f}\")\n",
    "    print(f\"Path: {best_model['path']}\")\n",
    "else:\n",
    "    print(\"\\nNo models found or all evaluations failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c5dfc04-a3cd-4a0a-b14c-c465a4e75ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\n",
      "\n",
      "=== Testing with K=2000 ===\n",
      "K=2000, T=0.9: Precision=0.0051, Recall=0.0001, F1=0.0002\n",
      "K=2000, T=0.85: Precision=0.0896, Recall=0.0022, F1=0.0042\n",
      "K=2000, T=0.8: Precision=0.2866, Recall=0.0103, F1=0.0196\n",
      "K=2000, T=0.7: Precision=0.3202, Recall=0.0586, F1=0.0893\n",
      "K=2000, T=0.5: Precision=0.1057, Recall=0.3947, F1=0.1511\n",
      "K=2000, T=0.4: Precision=0.0476, Recall=0.6169, F1=0.0845\n",
      "=== EVALUATION APPROACH 2: RANKING-BASED ===\n",
      "=== Testing with K=2000 ===\n",
      "K=2000, Top_N=1: Precision=0.2720, Recall=0.1460, F1=0.1777\n",
      "K=2000, Top_N=3: Precision=0.1915, Recall=0.2740, F1=0.2106\n",
      "K=2000, Top_N=5: Precision=0.1507, Recall=0.3419, F1=0.1968\n",
      "K=2000, Top_N=10: Precision=0.1059, Recall=0.4444, F1=0.1627\n",
      "K=2000, Top_N=15: Precision=0.0845, Recall=0.5039, F1=0.1387\n",
      "K=2000, Top_N=20: Precision=0.0714, Recall=0.5456, F1=0.1217\n",
      "K=2000, Top_N=30: Precision=0.0559, Recall=0.6054, F1=0.0994\n",
      "K=2000, Top_N=50: Precision=0.0401, Recall=0.6728, F1=0.0743\n",
      "K=2000, Top_N=80: Precision=0.0295, Recall=0.7312, F1=0.0560\n",
      "K=2000, Top_N=100: Precision=0.0255, Recall=0.7538, F1=0.0487\n",
      "K=2000, Top_N=150: Precision=0.0194, Recall=0.7985, F1=0.0376\n",
      "K=2000, Top_N=200: Precision=0.0160, Recall=0.8234, F1=0.0312\n",
      "K=2000, Top_N=300: Precision=0.0122, Recall=0.8544, F1=0.0240\n",
      "K=2000, Top_N=400: Precision=0.0101, Recall=0.8714, F1=0.0199\n",
      "K=2000, Top_N=500: Precision=0.0087, Recall=0.8847, F1=0.0173\n",
      "K=2000, Top_N=1000: Precision=0.0057, Recall=0.9136, F1=0.0113\n",
      "K=2000, Top_N=1500: Precision=0.0046, Recall=0.9267, F1=0.0091\n",
      "K=2000, Top_N=1600: Precision=0.0044, Recall=0.9288, F1=0.0088\n"
     ]
    }
   ],
   "source": [
    "#Part3_e5_mnrl_DDP_new_v2_Prefix4_Multi_GPU/epoch_1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/Part2_O1_MNRL_Prefix5_Multi_GPU_Final/step_10000/model\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "model.max_seq_length = max_seq_length  # Set max sequence length\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create properly formatted passages for the legacy attributes\n",
    "# Format: \"passage: [label]. [definition]\" to avoid nested prefixes\n",
    "legacy_attributes_with_prefix = []\n",
    "for i in range(len(legacy_df)):\n",
    "    label = legacy_df['label'].iloc[i]\n",
    "    definition = legacy_df['definition'].iloc[i]\n",
    "    prefixed_text = f\"passage: {label}. {definition}\"\n",
    "    legacy_attributes_with_prefix.append(prefixed_text)\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "    batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Format queries with proper prefix\n",
    "sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "    batch = sentences_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Modified evaluation approach that uses relative ranking instead of absolute thresholds\n",
    "def get_metrics_by_ranking(K, top_n_results):\n",
    "    \"\"\"\n",
    "    Evaluate using relative ranking - take top N results per query regardless of score\n",
    "    \n",
    "    Args:\n",
    "        K: Number of results to retrieve with FAISS\n",
    "        top_n_results: Number of top results to consider as \"positive\" predictions\n",
    "    \"\"\"\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results using ranking approach\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels from top N results regardless of similarity score\n",
    "        for j in range(min(top_n_results, len(I_faiss[i]))):\n",
    "            idx_pos = I_faiss[i][j]\n",
    "            if idx_pos >= 0:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Keep the original threshold-based evaluation for comparison\n",
    "def get_metrics_by_threshold(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Only test K=500 for both evaluation approaches\n",
    "K = 2000\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5, 0.4]  # Original thresholds\n",
    "top_n_values = [1, 3, 5, 10, 15, 20, 30, 50, 80, 100, 150, 200, 300, 400, 500, 1000, 1500, 1600]  # Top N results to consider\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\")\n",
    "print(f\"\\n=== Testing with K={K} ===\")\n",
    "for threshold in thresholds:\n",
    "    avg_precision, avg_recall, avg_f1 = get_metrics_by_threshold(K, threshold)\n",
    "    print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "print(\"=== EVALUATION APPROACH 2: RANKING-BASED ===\")\n",
    "print(f\"=== Testing with K={K} ===\")\n",
    "for top_n in top_n_values:\n",
    "    if top_n <= K:  # Can't have top_n greater than K\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics_by_ranking(K, top_n)\n",
    "        print(f\"K={K}, Top_N={top_n}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034dccf8-8615-4e6e-8a8a-da44edc5e3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 4.1.0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, T=0.9: Precision=0.2606, Recall=0.0227, F1=0.0385\n",
      "K=30, T=0.85: Precision=0.2331, Recall=0.0519, F1=0.0748\n",
      "K=30, T=0.8: Precision=0.1923, Recall=0.0945, F1=0.1098\n",
      "K=30, T=0.7: Precision=0.1204, Recall=0.2140, F1=0.1389\n",
      "K=30, T=0.5: Precision=0.0525, Recall=0.4529, F1=0.0908\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, T=0.9: Precision=0.2604, Recall=0.0227, F1=0.0385\n",
      "K=50, T=0.85: Precision=0.2310, Recall=0.0522, F1=0.0744\n",
      "K=50, T=0.8: Precision=0.1888, Recall=0.0961, F1=0.1088\n",
      "K=50, T=0.7: Precision=0.1154, Recall=0.2202, F1=0.1354\n",
      "K=50, T=0.5: Precision=0.0424, Recall=0.4978, F1=0.0758\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, T=0.9: Precision=0.2604, Recall=0.0227, F1=0.0385\n",
      "K=80, T=0.85: Precision=0.2309, Recall=0.0522, F1=0.0743\n",
      "K=80, T=0.8: Precision=0.1874, Recall=0.0964, F1=0.1080\n",
      "K=80, T=0.7: Precision=0.1123, Recall=0.2228, F1=0.1326\n",
      "K=80, T=0.5: Precision=0.0357, Recall=0.5328, F1=0.0649\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, T=0.9: Precision=0.2604, Recall=0.0227, F1=0.0385\n",
      "K=100, T=0.85: Precision=0.2309, Recall=0.0522, F1=0.0743\n",
      "K=100, T=0.8: Precision=0.1874, Recall=0.0964, F1=0.1080\n",
      "K=100, T=0.7: Precision=0.1113, Recall=0.2231, F1=0.1315\n",
      "K=100, T=0.5: Precision=0.0332, Recall=0.5464, F1=0.0609\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, T=0.9: Precision=0.2604, Recall=0.0227, F1=0.0385\n",
      "K=120, T=0.85: Precision=0.2309, Recall=0.0522, F1=0.0743\n",
      "K=120, T=0.8: Precision=0.1874, Recall=0.0964, F1=0.1080\n",
      "K=120, T=0.7: Precision=0.1110, Recall=0.2237, F1=0.1312\n",
      "K=120, T=0.5: Precision=0.0316, Recall=0.5574, F1=0.0582\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, T=0.9: Precision=0.2604, Recall=0.0227, F1=0.0385\n",
      "K=200, T=0.85: Precision=0.2309, Recall=0.0522, F1=0.0743\n",
      "K=200, T=0.8: Precision=0.1874, Recall=0.0964, F1=0.1080\n",
      "K=200, T=0.7: Precision=0.1107, Recall=0.2245, F1=0.1310\n",
      "K=200, T=0.5: Precision=0.0284, Recall=0.5781, F1=0.0525\n",
      "\n",
      "=== EVALUATION APPROACH 2: RANKING-BASED ===\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, Top_N=1: Precision=0.1928, Recall=0.1024, F1=0.1249\n",
      "K=30, Top_N=3: Precision=0.1409, Recall=0.1971, F1=0.1529\n",
      "K=30, Top_N=5: Precision=0.1127, Recall=0.2522, F1=0.1461\n",
      "K=30, Top_N=10: Precision=0.0810, Recall=0.3382, F1=0.1246\n",
      "K=30, Top_N=15: Precision=0.0667, Recall=0.3924, F1=0.1094\n",
      "K=30, Top_N=20: Precision=0.0576, Recall=0.4295, F1=0.0977\n",
      "K=30, Top_N=30: Precision=0.0466, Recall=0.4877, F1=0.0825\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, Top_N=1: Precision=0.1928, Recall=0.1024, F1=0.1249\n",
      "K=50, Top_N=3: Precision=0.1409, Recall=0.1971, F1=0.1529\n",
      "K=50, Top_N=5: Precision=0.1127, Recall=0.2522, F1=0.1461\n",
      "K=50, Top_N=10: Precision=0.0810, Recall=0.3382, F1=0.1246\n",
      "K=50, Top_N=15: Precision=0.0667, Recall=0.3924, F1=0.1094\n",
      "K=50, Top_N=20: Precision=0.0576, Recall=0.4295, F1=0.0977\n",
      "K=50, Top_N=30: Precision=0.0466, Recall=0.4877, F1=0.0825\n",
      "K=50, Top_N=50: Precision=0.0347, Recall=0.5556, F1=0.0639\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, Top_N=1: Precision=0.1928, Recall=0.1024, F1=0.1249\n",
      "K=80, Top_N=3: Precision=0.1409, Recall=0.1971, F1=0.1529\n",
      "K=80, Top_N=5: Precision=0.1127, Recall=0.2522, F1=0.1461\n",
      "K=80, Top_N=10: Precision=0.0810, Recall=0.3382, F1=0.1246\n",
      "K=80, Top_N=15: Precision=0.0667, Recall=0.3924, F1=0.1094\n",
      "K=80, Top_N=20: Precision=0.0576, Recall=0.4295, F1=0.0977\n",
      "K=80, Top_N=30: Precision=0.0466, Recall=0.4877, F1=0.0825\n",
      "K=80, Top_N=50: Precision=0.0347, Recall=0.5556, F1=0.0639\n",
      "K=80, Top_N=80: Precision=0.0262, Recall=0.6181, F1=0.0494\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, Top_N=1: Precision=0.1928, Recall=0.1024, F1=0.1249\n",
      "K=100, Top_N=3: Precision=0.1409, Recall=0.1971, F1=0.1529\n",
      "K=100, Top_N=5: Precision=0.1127, Recall=0.2522, F1=0.1461\n",
      "K=100, Top_N=10: Precision=0.0810, Recall=0.3382, F1=0.1246\n",
      "K=100, Top_N=15: Precision=0.0667, Recall=0.3924, F1=0.1094\n",
      "K=100, Top_N=20: Precision=0.0576, Recall=0.4295, F1=0.0977\n",
      "K=100, Top_N=30: Precision=0.0466, Recall=0.4877, F1=0.0825\n",
      "K=100, Top_N=50: Precision=0.0347, Recall=0.5556, F1=0.0639\n",
      "K=100, Top_N=80: Precision=0.0262, Recall=0.6181, F1=0.0494\n",
      "K=100, Top_N=100: Precision=0.0229, Recall=0.6465, F1=0.0436\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, Top_N=1: Precision=0.1928, Recall=0.1024, F1=0.1249\n",
      "K=120, Top_N=3: Precision=0.1409, Recall=0.1971, F1=0.1529\n",
      "K=120, Top_N=5: Precision=0.1127, Recall=0.2522, F1=0.1461\n",
      "K=120, Top_N=10: Precision=0.0810, Recall=0.3382, F1=0.1246\n",
      "K=120, Top_N=15: Precision=0.0667, Recall=0.3924, F1=0.1094\n",
      "K=120, Top_N=20: Precision=0.0576, Recall=0.4295, F1=0.0977\n",
      "K=120, Top_N=30: Precision=0.0466, Recall=0.4877, F1=0.0825\n",
      "K=120, Top_N=50: Precision=0.0347, Recall=0.5556, F1=0.0639\n",
      "K=120, Top_N=80: Precision=0.0262, Recall=0.6181, F1=0.0494\n",
      "K=120, Top_N=100: Precision=0.0229, Recall=0.6465, F1=0.0436\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, Top_N=1: Precision=0.1928, Recall=0.1024, F1=0.1249\n",
      "K=200, Top_N=3: Precision=0.1409, Recall=0.1971, F1=0.1529\n",
      "K=200, Top_N=5: Precision=0.1127, Recall=0.2522, F1=0.1461\n",
      "K=200, Top_N=10: Precision=0.0810, Recall=0.3382, F1=0.1246\n",
      "K=200, Top_N=15: Precision=0.0667, Recall=0.3924, F1=0.1094\n",
      "K=200, Top_N=20: Precision=0.0576, Recall=0.4295, F1=0.0977\n",
      "K=200, Top_N=30: Precision=0.0466, Recall=0.4877, F1=0.0825\n",
      "K=200, Top_N=50: Precision=0.0347, Recall=0.5556, F1=0.0639\n",
      "K=200, Top_N=80: Precision=0.0262, Recall=0.6181, F1=0.0494\n",
      "K=200, Top_N=100: Precision=0.0229, Recall=0.6465, F1=0.0436\n",
      "K=200, Top_N=150: Precision=0.0180, Recall=0.7011, F1=0.0348\n",
      "K=200, Top_N=200: Precision=0.0152, Recall=0.7394, F1=0.0296\n",
      "\n",
      "=== BEST CONFIGURATIONS ===\n",
      "\n",
      "Best Threshold Configuration for Recall:\n",
      "K=200, T=0.5\n",
      "Precision=0.0284, Recall=0.5781, F1=0.0525\n",
      "\n",
      "Best Ranking Configuration for Recall:\n",
      "K=200, Top_N=200\n",
      "Precision=0.0152, Recall=0.7394, F1=0.0296\n",
      "\n",
      "Best Threshold Configuration for F1:\n",
      "K=30, T=0.7\n",
      "Precision=0.1204, Recall=0.2140, F1=0.1389\n",
      "\n",
      "Best Ranking Configuration for F1:\n",
      "K=30, Top_N=3\n",
      "Precision=0.1409, Recall=0.1971, F1=0.1529\n",
      "\n",
      "=== COMPARISON OF METHODS ===\n",
      "Ranking-based method achieved better recall\n",
      "Ranking-based method achieved better F1 score\n"
     ]
    }
   ],
   "source": [
    "#PROPER - e5_mnrl_DDP_new/epoch_2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_new/epoch_2\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "model.max_seq_length = max_seq_length  # Set max sequence length\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create properly formatted passages for the legacy attributes\n",
    "# Format: \"passage: [label]. [definition]\" to avoid nested prefixes\n",
    "legacy_attributes_with_prefix = []\n",
    "for i in range(len(legacy_df)):\n",
    "    label = legacy_df['label'].iloc[i]\n",
    "    definition = legacy_df['definition'].iloc[i]\n",
    "    prefixed_text = f\"passage: {label}. {definition}\"\n",
    "    legacy_attributes_with_prefix.append(prefixed_text)\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "    batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Format queries with proper prefix\n",
    "sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "    batch = sentences_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Modified evaluation approach that uses relative ranking instead of absolute thresholds\n",
    "def get_metrics_by_ranking(K, top_n_results):\n",
    "    \"\"\"\n",
    "    Evaluate using relative ranking - take top N results per query regardless of score\n",
    "    \n",
    "    Args:\n",
    "        K: Number of results to retrieve with FAISS\n",
    "        top_n_results: Number of top results to consider as \"positive\" predictions\n",
    "    \"\"\"\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results using ranking approach\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels from top N results regardless of similarity score\n",
    "        for j in range(min(top_n_results, len(I_faiss[i]))):\n",
    "            idx_pos = I_faiss[i][j]\n",
    "            if idx_pos >= 0:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Keep the original threshold-based evaluation for comparison\n",
    "def get_metrics_by_threshold(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Test both evaluation approaches\n",
    "K_values = [30, 50, 80, 100, 120, 200]\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5]  # Keeping your original thresholds\n",
    "top_n_values = [1, 3, 5, 10, 15, 20, 30, 50, 80, 100, 150, 200]  # Top N results to consider\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\")\n",
    "threshold_results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for threshold in thresholds:\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics_by_threshold(K, threshold)\n",
    "        threshold_results.append({\n",
    "            'K': K,\n",
    "            'Threshold': threshold,\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1': avg_f1\n",
    "        })\n",
    "        print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 2: RANKING-BASED ===\")\n",
    "ranking_results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for top_n in top_n_values:\n",
    "        if top_n <= K:  # Can't have top_n greater than K\n",
    "            avg_precision, avg_recall, avg_f1 = get_metrics_by_ranking(K, top_n)\n",
    "            ranking_results.append({\n",
    "                'K': K,\n",
    "                'Top_N': top_n,\n",
    "                'Precision': avg_precision,\n",
    "                'Recall': avg_recall,\n",
    "                'F1': avg_f1\n",
    "            })\n",
    "            print(f\"K={K}, Top_N={top_n}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "# Find the best configurations\n",
    "best_threshold_recall = max(threshold_results, key=lambda x: x['Recall'])\n",
    "best_ranking_recall = max(ranking_results, key=lambda x: x['Recall'])\n",
    "best_threshold_f1 = max(threshold_results, key=lambda x: x['F1'])\n",
    "best_ranking_f1 = max(ranking_results, key=lambda x: x['F1'])\n",
    "\n",
    "print(\"\\n=== BEST CONFIGURATIONS ===\")\n",
    "print(\"\\nBest Threshold Configuration for Recall:\")\n",
    "print(f\"K={best_threshold_recall['K']}, T={best_threshold_recall['Threshold']}\")\n",
    "print(f\"Precision={best_threshold_recall['Precision']:.4f}, Recall={best_threshold_recall['Recall']:.4f}, F1={best_threshold_recall['F1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Ranking Configuration for Recall:\")\n",
    "print(f\"K={best_ranking_recall['K']}, Top_N={best_ranking_recall['Top_N']}\")\n",
    "print(f\"Precision={best_ranking_recall['Precision']:.4f}, Recall={best_ranking_recall['Recall']:.4f}, F1={best_ranking_recall['F1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Threshold Configuration for F1:\")\n",
    "print(f\"K={best_threshold_f1['K']}, T={best_threshold_f1['Threshold']}\")\n",
    "print(f\"Precision={best_threshold_f1['Precision']:.4f}, Recall={best_threshold_f1['Recall']:.4f}, F1={best_threshold_f1['F1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Ranking Configuration for F1:\")\n",
    "print(f\"K={best_ranking_f1['K']}, Top_N={best_ranking_f1['Top_N']}\")\n",
    "print(f\"Precision={best_ranking_f1['Precision']:.4f}, Recall={best_ranking_f1['Recall']:.4f}, F1={best_ranking_f1['F1']:.4f}\")\n",
    "\n",
    "# Compare overall best performance between methods\n",
    "print(\"\\n=== COMPARISON OF METHODS ===\")\n",
    "if best_ranking_recall['Recall'] > best_threshold_recall['Recall']:\n",
    "    print(\"Ranking-based method achieved better recall\")\n",
    "else:\n",
    "    print(\"Threshold-based method achieved better recall\")\n",
    "\n",
    "if best_ranking_f1['F1'] > best_threshold_f1['F1']:\n",
    "    print(\"Ranking-based method achieved better F1 score\")\n",
    "else:\n",
    "    print(\"Threshold-based method achieved better F1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d902dc9d-0618-4711-a84e-04225a996143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 4.1.0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, T=0.9: Precision=0.2353, Recall=0.0642, F1=0.0875\n",
      "K=30, T=0.85: Precision=0.1652, Recall=0.1324, F1=0.1260\n",
      "K=30, T=0.8: Precision=0.1106, Recall=0.2052, F1=0.1287\n",
      "K=30, T=0.7: Precision=0.0596, Recall=0.3560, F1=0.0981\n",
      "K=30, T=0.5: Precision=0.0441, Recall=0.4521, F1=0.0780\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, T=0.9: Precision=0.2335, Recall=0.0643, F1=0.0870\n",
      "K=50, T=0.85: Precision=0.1622, Recall=0.1341, F1=0.1248\n",
      "K=50, T=0.8: Precision=0.1063, Recall=0.2116, F1=0.1256\n",
      "K=50, T=0.7: Precision=0.0524, Recall=0.3860, F1=0.0890\n",
      "K=50, T=0.5: Precision=0.0340, Recall=0.5185, F1=0.0624\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, T=0.9: Precision=0.2333, Recall=0.0644, F1=0.0870\n",
      "K=80, T=0.85: Precision=0.1612, Recall=0.1344, F1=0.1243\n",
      "K=80, T=0.8: Precision=0.1037, Recall=0.2144, F1=0.1233\n",
      "K=80, T=0.7: Precision=0.0477, Recall=0.4055, F1=0.0825\n",
      "K=80, T=0.5: Precision=0.0263, Recall=0.5803, F1=0.0495\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, T=0.9: Precision=0.2333, Recall=0.0644, F1=0.0870\n",
      "K=100, T=0.85: Precision=0.1612, Recall=0.1344, F1=0.1242\n",
      "K=100, T=0.8: Precision=0.1032, Recall=0.2150, F1=0.1229\n",
      "K=100, T=0.7: Precision=0.0462, Recall=0.4114, F1=0.0804\n",
      "K=100, T=0.5: Precision=0.0234, Recall=0.6112, F1=0.0445\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, T=0.9: Precision=0.2333, Recall=0.0644, F1=0.0870\n",
      "K=120, T=0.85: Precision=0.1611, Recall=0.1344, F1=0.1242\n",
      "K=120, T=0.8: Precision=0.1029, Recall=0.2152, F1=0.1226\n",
      "K=120, T=0.7: Precision=0.0452, Recall=0.4141, F1=0.0787\n",
      "K=120, T=0.5: Precision=0.0211, Recall=0.6315, F1=0.0405\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, T=0.9: Precision=0.2333, Recall=0.0644, F1=0.0870\n",
      "K=200, T=0.85: Precision=0.1611, Recall=0.1344, F1=0.1242\n",
      "K=200, T=0.8: Precision=0.1029, Recall=0.2153, F1=0.1226\n",
      "K=200, T=0.7: Precision=0.0437, Recall=0.4202, F1=0.0765\n",
      "K=200, T=0.5: Precision=0.0161, Recall=0.6836, F1=0.0313\n",
      "\n",
      "=== EVALUATION APPROACH 2: RANKING-BASED ===\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, Top_N=1: Precision=0.1818, Recall=0.0990, F1=0.1203\n",
      "K=30, Top_N=3: Precision=0.1280, Recall=0.1856, F1=0.1420\n",
      "K=30, Top_N=5: Precision=0.1037, Recall=0.2323, F1=0.1346\n",
      "K=30, Top_N=10: Precision=0.0772, Recall=0.3155, F1=0.1177\n",
      "K=30, Top_N=15: Precision=0.0631, Recall=0.3670, F1=0.1031\n",
      "K=30, Top_N=20: Precision=0.0545, Recall=0.4034, F1=0.0925\n",
      "K=30, Top_N=30: Precision=0.0437, Recall=0.4553, F1=0.0774\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, Top_N=1: Precision=0.1818, Recall=0.0990, F1=0.1203\n",
      "K=50, Top_N=3: Precision=0.1280, Recall=0.1856, F1=0.1420\n",
      "K=50, Top_N=5: Precision=0.1037, Recall=0.2323, F1=0.1346\n",
      "K=50, Top_N=10: Precision=0.0772, Recall=0.3155, F1=0.1177\n",
      "K=50, Top_N=15: Precision=0.0631, Recall=0.3670, F1=0.1031\n",
      "K=50, Top_N=20: Precision=0.0545, Recall=0.4034, F1=0.0925\n",
      "K=50, Top_N=30: Precision=0.0437, Recall=0.4553, F1=0.0774\n",
      "K=50, Top_N=50: Precision=0.0334, Recall=0.5246, F1=0.0614\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, Top_N=1: Precision=0.1818, Recall=0.0990, F1=0.1203\n",
      "K=80, Top_N=3: Precision=0.1280, Recall=0.1856, F1=0.1420\n",
      "K=80, Top_N=5: Precision=0.1037, Recall=0.2323, F1=0.1346\n",
      "K=80, Top_N=10: Precision=0.0772, Recall=0.3155, F1=0.1177\n",
      "K=80, Top_N=15: Precision=0.0631, Recall=0.3670, F1=0.1031\n",
      "K=80, Top_N=20: Precision=0.0545, Recall=0.4034, F1=0.0925\n",
      "K=80, Top_N=30: Precision=0.0437, Recall=0.4553, F1=0.0774\n",
      "K=80, Top_N=50: Precision=0.0334, Recall=0.5246, F1=0.0614\n",
      "K=80, Top_N=80: Precision=0.0254, Recall=0.5908, F1=0.0480\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, Top_N=1: Precision=0.1818, Recall=0.0990, F1=0.1203\n",
      "K=100, Top_N=3: Precision=0.1280, Recall=0.1856, F1=0.1420\n",
      "K=100, Top_N=5: Precision=0.1037, Recall=0.2323, F1=0.1346\n",
      "K=100, Top_N=10: Precision=0.0772, Recall=0.3155, F1=0.1177\n",
      "K=100, Top_N=15: Precision=0.0631, Recall=0.3670, F1=0.1031\n",
      "K=100, Top_N=20: Precision=0.0545, Recall=0.4034, F1=0.0925\n",
      "K=100, Top_N=30: Precision=0.0437, Recall=0.4553, F1=0.0774\n",
      "K=100, Top_N=50: Precision=0.0334, Recall=0.5246, F1=0.0614\n",
      "K=100, Top_N=80: Precision=0.0254, Recall=0.5908, F1=0.0480\n",
      "K=100, Top_N=100: Precision=0.0225, Recall=0.6248, F1=0.0428\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, Top_N=1: Precision=0.1818, Recall=0.0990, F1=0.1203\n",
      "K=120, Top_N=3: Precision=0.1280, Recall=0.1856, F1=0.1420\n",
      "K=120, Top_N=5: Precision=0.1037, Recall=0.2323, F1=0.1346\n",
      "K=120, Top_N=10: Precision=0.0772, Recall=0.3155, F1=0.1177\n",
      "K=120, Top_N=15: Precision=0.0631, Recall=0.3670, F1=0.1031\n",
      "K=120, Top_N=20: Precision=0.0545, Recall=0.4034, F1=0.0925\n",
      "K=120, Top_N=30: Precision=0.0437, Recall=0.4553, F1=0.0774\n",
      "K=120, Top_N=50: Precision=0.0334, Recall=0.5246, F1=0.0614\n",
      "K=120, Top_N=80: Precision=0.0254, Recall=0.5908, F1=0.0480\n",
      "K=120, Top_N=100: Precision=0.0225, Recall=0.6248, F1=0.0428\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, Top_N=1: Precision=0.1818, Recall=0.0990, F1=0.1203\n",
      "K=200, Top_N=3: Precision=0.1280, Recall=0.1856, F1=0.1420\n",
      "K=200, Top_N=5: Precision=0.1037, Recall=0.2323, F1=0.1346\n",
      "K=200, Top_N=10: Precision=0.0772, Recall=0.3155, F1=0.1177\n",
      "K=200, Top_N=15: Precision=0.0631, Recall=0.3670, F1=0.1031\n",
      "K=200, Top_N=20: Precision=0.0545, Recall=0.4034, F1=0.0925\n",
      "K=200, Top_N=30: Precision=0.0437, Recall=0.4553, F1=0.0774\n",
      "K=200, Top_N=50: Precision=0.0334, Recall=0.5246, F1=0.0614\n",
      "K=200, Top_N=80: Precision=0.0254, Recall=0.5908, F1=0.0480\n",
      "K=200, Top_N=100: Precision=0.0225, Recall=0.6248, F1=0.0428\n",
      "K=200, Top_N=150: Precision=0.0176, Recall=0.6775, F1=0.0341\n",
      "K=200, Top_N=200: Precision=0.0149, Recall=0.7135, F1=0.0289\n",
      "\n",
      "=== BEST CONFIGURATIONS ===\n",
      "\n",
      "Best Threshold Configuration for Recall:\n",
      "K=200, T=0.5\n",
      "Precision=0.0161, Recall=0.6836, F1=0.0313\n",
      "\n",
      "Best Ranking Configuration for Recall:\n",
      "K=200, Top_N=200\n",
      "Precision=0.0149, Recall=0.7135, F1=0.0289\n",
      "\n",
      "Best Threshold Configuration for F1:\n",
      "K=30, T=0.8\n",
      "Precision=0.1106, Recall=0.2052, F1=0.1287\n",
      "\n",
      "Best Ranking Configuration for F1:\n",
      "K=30, Top_N=3\n",
      "Precision=0.1280, Recall=0.1856, F1=0.1420\n",
      "\n",
      "=== COMPARISON OF METHODS ===\n",
      "Ranking-based method achieved better recall\n",
      "Ranking-based method achieved better F1 score\n"
     ]
    }
   ],
   "source": [
    "#PROPER - e5_mnrl_DDP_new/epoch_3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_new/epoch_3\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "model.max_seq_length = max_seq_length  # Set max sequence length\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create properly formatted passages for the legacy attributes\n",
    "# Format: \"passage: [label]. [definition]\" to avoid nested prefixes\n",
    "legacy_attributes_with_prefix = []\n",
    "for i in range(len(legacy_df)):\n",
    "    label = legacy_df['label'].iloc[i]\n",
    "    definition = legacy_df['definition'].iloc[i]\n",
    "    prefixed_text = f\"passage: {label}. {definition}\"\n",
    "    legacy_attributes_with_prefix.append(prefixed_text)\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "    batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Format queries with proper prefix\n",
    "sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "    batch = sentences_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Modified evaluation approach that uses relative ranking instead of absolute thresholds\n",
    "def get_metrics_by_ranking(K, top_n_results):\n",
    "    \"\"\"\n",
    "    Evaluate using relative ranking - take top N results per query regardless of score\n",
    "    \n",
    "    Args:\n",
    "        K: Number of results to retrieve with FAISS\n",
    "        top_n_results: Number of top results to consider as \"positive\" predictions\n",
    "    \"\"\"\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results using ranking approach\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels from top N results regardless of similarity score\n",
    "        for j in range(min(top_n_results, len(I_faiss[i]))):\n",
    "            idx_pos = I_faiss[i][j]\n",
    "            if idx_pos >= 0:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Keep the original threshold-based evaluation for comparison\n",
    "def get_metrics_by_threshold(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Test both evaluation approaches\n",
    "K_values = [30, 50, 80, 100, 120, 200]\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5]  # Keeping your original thresholds\n",
    "top_n_values = [1, 3, 5, 10, 15, 20, 30, 50, 80, 100, 150, 200]  # Top N results to consider\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\")\n",
    "threshold_results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for threshold in thresholds:\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics_by_threshold(K, threshold)\n",
    "        threshold_results.append({\n",
    "            'K': K,\n",
    "            'Threshold': threshold,\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1': avg_f1\n",
    "        })\n",
    "        print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 2: RANKING-BASED ===\")\n",
    "ranking_results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for top_n in top_n_values:\n",
    "        if top_n <= K:  # Can't have top_n greater than K\n",
    "            avg_precision, avg_recall, avg_f1 = get_metrics_by_ranking(K, top_n)\n",
    "            ranking_results.append({\n",
    "                'K': K,\n",
    "                'Top_N': top_n,\n",
    "                'Precision': avg_precision,\n",
    "                'Recall': avg_recall,\n",
    "                'F1': avg_f1\n",
    "            })\n",
    "            print(f\"K={K}, Top_N={top_n}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "# Find the best configurations\n",
    "best_threshold_recall = max(threshold_results, key=lambda x: x['Recall'])\n",
    "best_ranking_recall = max(ranking_results, key=lambda x: x['Recall'])\n",
    "best_threshold_f1 = max(threshold_results, key=lambda x: x['F1'])\n",
    "best_ranking_f1 = max(ranking_results, key=lambda x: x['F1'])\n",
    "\n",
    "print(\"\\n=== BEST CONFIGURATIONS ===\")\n",
    "print(\"\\nBest Threshold Configuration for Recall:\")\n",
    "print(f\"K={best_threshold_recall['K']}, T={best_threshold_recall['Threshold']}\")\n",
    "print(f\"Precision={best_threshold_recall['Precision']:.4f}, Recall={best_threshold_recall['Recall']:.4f}, F1={best_threshold_recall['F1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Ranking Configuration for Recall:\")\n",
    "print(f\"K={best_ranking_recall['K']}, Top_N={best_ranking_recall['Top_N']}\")\n",
    "print(f\"Precision={best_ranking_recall['Precision']:.4f}, Recall={best_ranking_recall['Recall']:.4f}, F1={best_ranking_recall['F1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Threshold Configuration for F1:\")\n",
    "print(f\"K={best_threshold_f1['K']}, T={best_threshold_f1['Threshold']}\")\n",
    "print(f\"Precision={best_threshold_f1['Precision']:.4f}, Recall={best_threshold_f1['Recall']:.4f}, F1={best_threshold_f1['F1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Ranking Configuration for F1:\")\n",
    "print(f\"K={best_ranking_f1['K']}, Top_N={best_ranking_f1['Top_N']}\")\n",
    "print(f\"Precision={best_ranking_f1['Precision']:.4f}, Recall={best_ranking_f1['Recall']:.4f}, F1={best_ranking_f1['F1']:.4f}\")\n",
    "\n",
    "# Compare overall best performance between methods\n",
    "print(\"\\n=== COMPARISON OF METHODS ===\")\n",
    "if best_ranking_recall['Recall'] > best_threshold_recall['Recall']:\n",
    "    print(\"Ranking-based method achieved better recall\")\n",
    "else:\n",
    "    print(\"Threshold-based method achieved better recall\")\n",
    "\n",
    "if best_ranking_f1['F1'] > best_threshold_f1['F1']:\n",
    "    print(\"Ranking-based method achieved better F1 score\")\n",
    "else:\n",
    "    print(\"Threshold-based method achieved better F1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7a9053-50ae-4702-b5f5-1f7e3c6af10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, T=0.9: Precision=0.2610, Recall=0.0196, F1=0.0343\n",
      "K=30, T=0.85: Precision=0.2295, Recall=0.0496, F1=0.0724\n",
      "K=30, T=0.8: Precision=0.1707, Recall=0.0955, F1=0.1079\n",
      "K=30, T=0.7: Precision=0.0987, Recall=0.2253, F1=0.1253\n",
      "K=30, T=0.5: Precision=0.0471, Recall=0.4509, F1=0.0828\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, T=0.9: Precision=0.2610, Recall=0.0196, F1=0.0343\n",
      "K=50, T=0.85: Precision=0.2283, Recall=0.0496, F1=0.0719\n",
      "K=50, T=0.8: Precision=0.1674, Recall=0.0964, F1=0.1064\n",
      "K=50, T=0.7: Precision=0.0930, Recall=0.2331, F1=0.1204\n",
      "K=50, T=0.5: Precision=0.0371, Recall=0.5030, F1=0.0673\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, T=0.9: Precision=0.2610, Recall=0.0196, F1=0.0343\n",
      "K=80, T=0.85: Precision=0.2283, Recall=0.0496, F1=0.0719\n",
      "K=80, T=0.8: Precision=0.1660, Recall=0.0965, F1=0.1055\n",
      "K=80, T=0.7: Precision=0.0902, Recall=0.2377, F1=0.1177\n",
      "K=80, T=0.5: Precision=0.0305, Recall=0.5464, F1=0.0564\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, T=0.9: Precision=0.2610, Recall=0.0196, F1=0.0343\n",
      "K=100, T=0.85: Precision=0.2283, Recall=0.0496, F1=0.0719\n",
      "K=100, T=0.8: Precision=0.1660, Recall=0.0965, F1=0.1055\n",
      "K=100, T=0.7: Precision=0.0892, Recall=0.2383, F1=0.1167\n",
      "K=100, T=0.5: Precision=0.0280, Recall=0.5637, F1=0.0521\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, T=0.9: Precision=0.2610, Recall=0.0196, F1=0.0343\n",
      "K=120, T=0.85: Precision=0.2283, Recall=0.0496, F1=0.0719\n",
      "K=120, T=0.8: Precision=0.1660, Recall=0.0965, F1=0.1055\n",
      "K=120, T=0.7: Precision=0.0888, Recall=0.2386, F1=0.1161\n",
      "K=120, T=0.5: Precision=0.0263, Recall=0.5791, F1=0.0492\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, T=0.9: Precision=0.2610, Recall=0.0196, F1=0.0343\n",
      "K=200, T=0.85: Precision=0.2283, Recall=0.0496, F1=0.0719\n",
      "K=200, T=0.8: Precision=0.1660, Recall=0.0965, F1=0.1055\n",
      "K=200, T=0.7: Precision=0.0885, Recall=0.2387, F1=0.1158\n",
      "K=200, T=0.5: Precision=0.0223, Recall=0.6056, F1=0.0421\n",
      "\n",
      "=== EVALUATION APPROACH 2: RANKING-BASED ===\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, Top_N=1: Precision=0.1858, Recall=0.1001, F1=0.1212\n",
      "K=30, Top_N=3: Precision=0.1317, Recall=0.1925, F1=0.1461\n",
      "K=30, Top_N=5: Precision=0.1061, Recall=0.2460, F1=0.1398\n",
      "K=30, Top_N=10: Precision=0.0772, Recall=0.3253, F1=0.1193\n",
      "K=30, Top_N=15: Precision=0.0638, Recall=0.3769, F1=0.1046\n",
      "K=30, Top_N=20: Precision=0.0555, Recall=0.4181, F1=0.0944\n",
      "K=30, Top_N=30: Precision=0.0445, Recall=0.4760, F1=0.0789\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, Top_N=1: Precision=0.1858, Recall=0.1001, F1=0.1212\n",
      "K=50, Top_N=3: Precision=0.1317, Recall=0.1925, F1=0.1461\n",
      "K=50, Top_N=5: Precision=0.1061, Recall=0.2460, F1=0.1398\n",
      "K=50, Top_N=10: Precision=0.0772, Recall=0.3253, F1=0.1193\n",
      "K=50, Top_N=15: Precision=0.0638, Recall=0.3769, F1=0.1046\n",
      "K=50, Top_N=20: Precision=0.0555, Recall=0.4181, F1=0.0944\n",
      "K=50, Top_N=30: Precision=0.0445, Recall=0.4760, F1=0.0789\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, Top_N=1: Precision=0.1858, Recall=0.1001, F1=0.1212\n",
      "K=80, Top_N=3: Precision=0.1317, Recall=0.1925, F1=0.1461\n",
      "K=80, Top_N=5: Precision=0.1061, Recall=0.2460, F1=0.1398\n",
      "K=80, Top_N=10: Precision=0.0772, Recall=0.3253, F1=0.1193\n",
      "K=80, Top_N=15: Precision=0.0638, Recall=0.3769, F1=0.1046\n",
      "K=80, Top_N=20: Precision=0.0555, Recall=0.4181, F1=0.0944\n",
      "K=80, Top_N=30: Precision=0.0445, Recall=0.4760, F1=0.0789\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, Top_N=1: Precision=0.1858, Recall=0.1001, F1=0.1212\n",
      "K=100, Top_N=3: Precision=0.1317, Recall=0.1925, F1=0.1461\n",
      "K=100, Top_N=5: Precision=0.1061, Recall=0.2460, F1=0.1398\n",
      "K=100, Top_N=10: Precision=0.0772, Recall=0.3253, F1=0.1193\n",
      "K=100, Top_N=15: Precision=0.0638, Recall=0.3769, F1=0.1046\n",
      "K=100, Top_N=20: Precision=0.0555, Recall=0.4181, F1=0.0944\n",
      "K=100, Top_N=30: Precision=0.0445, Recall=0.4760, F1=0.0789\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, Top_N=1: Precision=0.1858, Recall=0.1001, F1=0.1212\n",
      "K=120, Top_N=3: Precision=0.1317, Recall=0.1925, F1=0.1461\n",
      "K=120, Top_N=5: Precision=0.1061, Recall=0.2460, F1=0.1398\n",
      "K=120, Top_N=10: Precision=0.0772, Recall=0.3253, F1=0.1193\n",
      "K=120, Top_N=15: Precision=0.0638, Recall=0.3769, F1=0.1046\n",
      "K=120, Top_N=20: Precision=0.0555, Recall=0.4181, F1=0.0944\n",
      "K=120, Top_N=30: Precision=0.0445, Recall=0.4760, F1=0.0789\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, Top_N=1: Precision=0.1858, Recall=0.1001, F1=0.1212\n",
      "K=200, Top_N=3: Precision=0.1317, Recall=0.1925, F1=0.1461\n",
      "K=200, Top_N=5: Precision=0.1061, Recall=0.2460, F1=0.1398\n",
      "K=200, Top_N=10: Precision=0.0772, Recall=0.3253, F1=0.1193\n",
      "K=200, Top_N=15: Precision=0.0638, Recall=0.3769, F1=0.1046\n",
      "K=200, Top_N=20: Precision=0.0555, Recall=0.4181, F1=0.0944\n",
      "K=200, Top_N=30: Precision=0.0445, Recall=0.4760, F1=0.0789\n",
      "\n",
      "=== BEST CONFIGURATIONS ===\n",
      "\n",
      "Best Threshold Configuration for Recall:\n",
      "K=200, T=0.5\n",
      "Precision=0.0223, Recall=0.6056, F1=0.0421\n",
      "\n",
      "Best Ranking Configuration for Recall:\n",
      "K=30, Top_N=30\n",
      "Precision=0.0445, Recall=0.4760, F1=0.0789\n",
      "\n",
      "Best Threshold Configuration for F1:\n",
      "K=30, T=0.7\n",
      "Precision=0.0987, Recall=0.2253, F1=0.1253\n",
      "\n",
      "Best Ranking Configuration for F1:\n",
      "K=30, Top_N=3\n",
      "Precision=0.1317, Recall=0.1925, F1=0.1461\n",
      "\n",
      "=== COMPARISON OF METHODS ===\n",
      "Threshold-based method achieved better recall\n",
      "Ranking-based method achieved better F1 score\n"
     ]
    }
   ],
   "source": [
    "#PROPER - e5_mnrl_DDP_with_prefixes_4gpu/epoch_1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_with_prefixes_4gpu/epoch_1\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "model.max_seq_length = max_seq_length  # Set max sequence length\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create properly formatted passages for the legacy attributes\n",
    "# Format: \"passage: [label]. [definition]\" to avoid nested prefixes\n",
    "legacy_attributes_with_prefix = []\n",
    "for i in range(len(legacy_df)):\n",
    "    label = legacy_df['label'].iloc[i]\n",
    "    definition = legacy_df['definition'].iloc[i]\n",
    "    prefixed_text = f\"passage: {label}. {definition}\"\n",
    "    legacy_attributes_with_prefix.append(prefixed_text)\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "    batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Format queries with proper prefix\n",
    "sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "    batch = sentences_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Modified evaluation approach that uses relative ranking instead of absolute thresholds\n",
    "def get_metrics_by_ranking(K, top_n_results):\n",
    "    \"\"\"\n",
    "    Evaluate using relative ranking - take top N results per query regardless of score\n",
    "    \n",
    "    Args:\n",
    "        K: Number of results to retrieve with FAISS\n",
    "        top_n_results: Number of top results to consider as \"positive\" predictions\n",
    "    \"\"\"\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results using ranking approach\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels from top N results regardless of similarity score\n",
    "        for j in range(min(top_n_results, len(I_faiss[i]))):\n",
    "            idx_pos = I_faiss[i][j]\n",
    "            if idx_pos >= 0:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Keep the original threshold-based evaluation for comparison\n",
    "def get_metrics_by_threshold(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Test both evaluation approaches\n",
    "K_values = [30, 50, 80, 100, 120, 200]\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5]  # Keeping your original thresholds\n",
    "top_n_values = [1, 3, 5, 10, 15, 20, 30]  # Top N results to consider\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\")\n",
    "threshold_results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for threshold in thresholds:\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics_by_threshold(K, threshold)\n",
    "        threshold_results.append({\n",
    "            'K': K,\n",
    "            'Threshold': threshold,\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1': avg_f1\n",
    "        })\n",
    "        print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 2: RANKING-BASED ===\")\n",
    "ranking_results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for top_n in top_n_values:\n",
    "        if top_n <= K:  # Can't have top_n greater than K\n",
    "            avg_precision, avg_recall, avg_f1 = get_metrics_by_ranking(K, top_n)\n",
    "            ranking_results.append({\n",
    "                'K': K,\n",
    "                'Top_N': top_n,\n",
    "                'Precision': avg_precision,\n",
    "                'Recall': avg_recall,\n",
    "                'F1': avg_f1\n",
    "            })\n",
    "            print(f\"K={K}, Top_N={top_n}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "# Find the best configurations\n",
    "best_threshold_recall = max(threshold_results, key=lambda x: x['Recall'])\n",
    "best_ranking_recall = max(ranking_results, key=lambda x: x['Recall'])\n",
    "best_threshold_f1 = max(threshold_results, key=lambda x: x['F1'])\n",
    "best_ranking_f1 = max(ranking_results, key=lambda x: x['F1'])\n",
    "\n",
    "print(\"\\n=== BEST CONFIGURATIONS ===\")\n",
    "print(\"\\nBest Threshold Configuration for Recall:\")\n",
    "print(f\"K={best_threshold_recall['K']}, T={best_threshold_recall['Threshold']}\")\n",
    "print(f\"Precision={best_threshold_recall['Precision']:.4f}, Recall={best_threshold_recall['Recall']:.4f}, F1={best_threshold_recall['F1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Ranking Configuration for Recall:\")\n",
    "print(f\"K={best_ranking_recall['K']}, Top_N={best_ranking_recall['Top_N']}\")\n",
    "print(f\"Precision={best_ranking_recall['Precision']:.4f}, Recall={best_ranking_recall['Recall']:.4f}, F1={best_ranking_recall['F1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Threshold Configuration for F1:\")\n",
    "print(f\"K={best_threshold_f1['K']}, T={best_threshold_f1['Threshold']}\")\n",
    "print(f\"Precision={best_threshold_f1['Precision']:.4f}, Recall={best_threshold_f1['Recall']:.4f}, F1={best_threshold_f1['F1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Ranking Configuration for F1:\")\n",
    "print(f\"K={best_ranking_f1['K']}, Top_N={best_ranking_f1['Top_N']}\")\n",
    "print(f\"Precision={best_ranking_f1['Precision']:.4f}, Recall={best_ranking_f1['Recall']:.4f}, F1={best_ranking_f1['F1']:.4f}\")\n",
    "\n",
    "# Compare overall best performance between methods\n",
    "print(\"\\n=== COMPARISON OF METHODS ===\")\n",
    "if best_ranking_recall['Recall'] > best_threshold_recall['Recall']:\n",
    "    print(\"Ranking-based method achieved better recall\")\n",
    "else:\n",
    "    print(\"Threshold-based method achieved better recall\")\n",
    "\n",
    "if best_ranking_f1['F1'] > best_threshold_f1['F1']:\n",
    "    print(\"Ranking-based method achieved better F1 score\")\n",
    "else:\n",
    "    print(\"Threshold-based method achieved better F1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e475cde8-0899-4211-9e2b-7e7d71361871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://nexus.corp.indeed.com/repository/pypi/simple\n",
      "Collecting pandas==2.1.4\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/pandas/2.1.4/pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy==1.11.4\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/scipy/1.11.4/scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn==1.3.2\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/scikit-learn/1.3.2/scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow==2.15.0\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/tensorflow/2.15.0/tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.37.2\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/transformers/4.37.2/transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentence-transformers==2.7.0\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/sentence-transformers/2.7.0/sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1.4) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1.4) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1.4) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.1.4) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.13.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/ml-dtypes/0.2.0/ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/protobuf/4.25.6/protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (79.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.13.2)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/wrapt/1.14.1/wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.71.0)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/tensorboard/2.15.2/tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/tensorflow-estimator/2.15.0/tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/keras/2.15.0/keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.30.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.37.2)\n",
      "  Downloading https://nexus.corp.indeed.com/repository/pypi/packages/tokenizers/0.15.2/tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.7.0) (2.6.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.7.0) (11.2.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2025.3.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.7.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers==2.7.0) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
      "Installing collected packages: wrapt, tensorflow-estimator, scipy, protobuf, ml-dtypes, keras, scikit-learn, pandas, tokenizers, transformers, tensorboard, tensorflow, sentence-transformers\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.17.2\n",
      "    Uninstalling wrapt-1.17.2:\n",
      "      Successfully uninstalled wrapt-1.17.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.14.0\n",
      "    Uninstalling tensorflow-estimator-2.14.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "    Uninstalling protobuf-5.29.4:\n",
      "      Successfully uninstalled protobuf-5.29.4\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml_dtypes 0.5.1\n",
      "    Uninstalling ml_dtypes-0.5.1:\n",
      "      Successfully uninstalled ml_dtypes-0.5.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.9.2\n",
      "    Uninstalling keras-3.9.2:\n",
      "      Successfully uninstalled keras-3.9.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.19.0\n",
      "    Uninstalling tensorboard-2.19.0:\n",
      "      Successfully uninstalled tensorboard-2.19.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awswrangler 3.0.0 requires pandas!=1.5.0,<2.0.0,>=1.2.0, but you have pandas 2.1.4 which is incompatible.\n",
      "indeed-butterfly-client 0.4.15 requires pydantic==1.10.6, but you have pydantic 2.11.3 which is incompatible.\n",
      "kfp 1.8.22 requires absl-py<2,>=0.9, but you have absl-py 2.2.2 which is incompatible.\n",
      "kfp 1.8.22 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.6 which is incompatible.\n",
      "kfp 1.8.22 requires pydantic<2,>=1.8.2, but you have pydantic 2.11.3 which is incompatible.\n",
      "kfp 1.8.22 requires urllib3<2, but you have urllib3 2.4.0 which is incompatible.\n",
      "kfp-pipeline-spec 0.1.16 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.6 which is incompatible.\n",
      "ml-metadata 1.14.0 requires absl-py<2.0.0,>=0.9, but you have absl-py 2.2.2 which is incompatible.\n",
      "ml-metadata 1.14.0 requires protobuf<4,>=3.13, but you have protobuf 4.25.6 which is incompatible.\n",
      "mlflow 2.11.3 requires packaging<24, but you have packaging 25.0 which is incompatible.\n",
      "mlflow 2.11.3 requires pyarrow<16,>=4.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "mlflow 2.11.3 requires pytz<2025, but you have pytz 2025.2 which is incompatible.\n",
      "skl2onnx 1.13 requires scikit-learn<=1.1.1, but you have scikit-learn 1.3.2 which is incompatible.\n",
      "tensorflow-text 2.14.0 requires tensorflow<2.15,>=2.14.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.15.0 which is incompatible.\n",
      "tf2onnx 1.16.1 requires protobuf~=3.20, but you have protobuf 4.25.6 which is incompatible.\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\n",
      "ydata-profiling 4.5.1 requires pandas!=1.4.0,<2.1,>1.1, but you have pandas 2.1.4 which is incompatible.\n",
      "ydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.11.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 pandas-2.1.4 protobuf-4.25.6 scikit-learn-1.3.2 scipy-1.11.4 sentence-transformers-2.7.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tokenizers-0.15.2 transformers-4.37.2 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install \\\n",
    "    pandas==2.1.4 \\\n",
    "    scipy==1.11.4 \\\n",
    "    scikit-learn==1.3.2 \\\n",
    "    tensorflow==2.15.0 \\\n",
    "    transformers==4.37.2 \\\n",
    "    sentence-transformers==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f171c96-627e-4ce3-82e4-653e616dad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.1.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(f\"NumPy version: {np.__version__}\")  # Should be 1.26.4\n",
    "print(f\"Pandas version: {pd.__version__}\")  # Should be 2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3924474c-7faa-4786-9dcb-56ab8fba1f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, T=0.9: Precision=0.0455, Recall=0.0008, F1=0.0017\n",
      "K=30, T=0.85: Precision=0.2453, Recall=0.0063, F1=0.0122\n",
      "K=30, T=0.8: Precision=0.3056, Recall=0.0166, F1=0.0304\n",
      "K=30, T=0.7: Precision=0.3163, Recall=0.0727, F1=0.1027\n",
      "K=30, T=0.5: Precision=0.1071, Recall=0.3985, F1=0.1574\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, T=0.9: Precision=0.0455, Recall=0.0008, F1=0.0017\n",
      "K=50, T=0.85: Precision=0.2453, Recall=0.0063, F1=0.0122\n",
      "K=50, T=0.8: Precision=0.3056, Recall=0.0166, F1=0.0304\n",
      "K=50, T=0.7: Precision=0.3160, Recall=0.0727, F1=0.1025\n",
      "K=50, T=0.5: Precision=0.0974, Recall=0.4127, F1=0.1460\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, T=0.9: Precision=0.0455, Recall=0.0008, F1=0.0017\n",
      "K=80, T=0.85: Precision=0.2453, Recall=0.0063, F1=0.0122\n",
      "K=80, T=0.8: Precision=0.3056, Recall=0.0166, F1=0.0304\n",
      "K=80, T=0.7: Precision=0.3160, Recall=0.0727, F1=0.1025\n",
      "K=80, T=0.5: Precision=0.0925, Recall=0.4198, F1=0.1396\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, T=0.9: Precision=0.0455, Recall=0.0008, F1=0.0017\n",
      "K=100, T=0.85: Precision=0.2453, Recall=0.0063, F1=0.0122\n",
      "K=100, T=0.8: Precision=0.3056, Recall=0.0166, F1=0.0304\n",
      "K=100, T=0.7: Precision=0.3160, Recall=0.0727, F1=0.1025\n",
      "K=100, T=0.5: Precision=0.0908, Recall=0.4218, F1=0.1372\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, T=0.9: Precision=0.0455, Recall=0.0008, F1=0.0017\n",
      "K=120, T=0.85: Precision=0.2453, Recall=0.0063, F1=0.0122\n",
      "K=120, T=0.8: Precision=0.3056, Recall=0.0166, F1=0.0304\n",
      "K=120, T=0.7: Precision=0.3160, Recall=0.0727, F1=0.1025\n",
      "K=120, T=0.5: Precision=0.0898, Recall=0.4230, F1=0.1357\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, T=0.9: Precision=0.0455, Recall=0.0008, F1=0.0017\n",
      "K=200, T=0.85: Precision=0.2453, Recall=0.0063, F1=0.0122\n",
      "K=200, T=0.8: Precision=0.3056, Recall=0.0166, F1=0.0304\n",
      "K=200, T=0.7: Precision=0.3160, Recall=0.0727, F1=0.1025\n",
      "K=200, T=0.5: Precision=0.0886, Recall=0.4243, F1=0.1339\n",
      "\n",
      "=== EVALUATION APPROACH 2: RANKING-BASED ===\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, Top_N=1: Precision=0.2673, Recall=0.1442, F1=0.1752\n",
      "K=30, Top_N=3: Precision=0.1817, Recall=0.2643, F1=0.2011\n",
      "K=30, Top_N=5: Precision=0.1460, Recall=0.3350, F1=0.1904\n",
      "K=30, Top_N=10: Precision=0.1031, Recall=0.4331, F1=0.1575\n",
      "K=30, Top_N=15: Precision=0.0823, Recall=0.4940, F1=0.1349\n",
      "K=30, Top_N=20: Precision=0.0693, Recall=0.5351, F1=0.1182\n",
      "K=30, Top_N=30: Precision=0.0547, Recall=0.5985, F1=0.0973\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, Top_N=1: Precision=0.2673, Recall=0.1442, F1=0.1752\n",
      "K=50, Top_N=3: Precision=0.1817, Recall=0.2643, F1=0.2011\n",
      "K=50, Top_N=5: Precision=0.1460, Recall=0.3350, F1=0.1904\n",
      "K=50, Top_N=10: Precision=0.1031, Recall=0.4331, F1=0.1575\n",
      "K=50, Top_N=15: Precision=0.0823, Recall=0.4940, F1=0.1349\n",
      "K=50, Top_N=20: Precision=0.0693, Recall=0.5351, F1=0.1182\n",
      "K=50, Top_N=30: Precision=0.0547, Recall=0.5985, F1=0.0973\n",
      "K=50, Top_N=50: Precision=0.0397, Recall=0.6688, F1=0.0734\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, Top_N=1: Precision=0.2673, Recall=0.1442, F1=0.1752\n",
      "K=80, Top_N=3: Precision=0.1817, Recall=0.2643, F1=0.2011\n",
      "K=80, Top_N=5: Precision=0.1460, Recall=0.3350, F1=0.1904\n",
      "K=80, Top_N=10: Precision=0.1031, Recall=0.4331, F1=0.1575\n",
      "K=80, Top_N=15: Precision=0.0823, Recall=0.4940, F1=0.1349\n",
      "K=80, Top_N=20: Precision=0.0693, Recall=0.5351, F1=0.1182\n",
      "K=80, Top_N=30: Precision=0.0547, Recall=0.5985, F1=0.0973\n",
      "K=80, Top_N=50: Precision=0.0397, Recall=0.6688, F1=0.0734\n",
      "K=80, Top_N=80: Precision=0.0295, Recall=0.7290, F1=0.0559\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, Top_N=1: Precision=0.2673, Recall=0.1442, F1=0.1752\n",
      "K=100, Top_N=3: Precision=0.1817, Recall=0.2643, F1=0.2011\n",
      "K=100, Top_N=5: Precision=0.1460, Recall=0.3350, F1=0.1904\n",
      "K=100, Top_N=10: Precision=0.1031, Recall=0.4331, F1=0.1575\n",
      "K=100, Top_N=15: Precision=0.0823, Recall=0.4940, F1=0.1349\n",
      "K=100, Top_N=20: Precision=0.0693, Recall=0.5351, F1=0.1182\n",
      "K=100, Top_N=30: Precision=0.0547, Recall=0.5985, F1=0.0973\n",
      "K=100, Top_N=50: Precision=0.0397, Recall=0.6688, F1=0.0734\n",
      "K=100, Top_N=80: Precision=0.0295, Recall=0.7290, F1=0.0559\n",
      "K=100, Top_N=100: Precision=0.0254, Recall=0.7547, F1=0.0486\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, Top_N=1: Precision=0.2673, Recall=0.1442, F1=0.1752\n",
      "K=120, Top_N=3: Precision=0.1817, Recall=0.2643, F1=0.2011\n",
      "K=120, Top_N=5: Precision=0.1460, Recall=0.3350, F1=0.1904\n",
      "K=120, Top_N=10: Precision=0.1031, Recall=0.4331, F1=0.1575\n",
      "K=120, Top_N=15: Precision=0.0823, Recall=0.4940, F1=0.1349\n",
      "K=120, Top_N=20: Precision=0.0693, Recall=0.5351, F1=0.1182\n",
      "K=120, Top_N=30: Precision=0.0547, Recall=0.5985, F1=0.0973\n",
      "K=120, Top_N=50: Precision=0.0397, Recall=0.6688, F1=0.0734\n",
      "K=120, Top_N=80: Precision=0.0295, Recall=0.7290, F1=0.0559\n",
      "K=120, Top_N=100: Precision=0.0254, Recall=0.7547, F1=0.0486\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, Top_N=1: Precision=0.2673, Recall=0.1442, F1=0.1752\n",
      "K=200, Top_N=3: Precision=0.1817, Recall=0.2643, F1=0.2011\n",
      "K=200, Top_N=5: Precision=0.1460, Recall=0.3350, F1=0.1904\n",
      "K=200, Top_N=10: Precision=0.1031, Recall=0.4331, F1=0.1575\n",
      "K=200, Top_N=15: Precision=0.0823, Recall=0.4940, F1=0.1349\n",
      "K=200, Top_N=20: Precision=0.0693, Recall=0.5351, F1=0.1182\n",
      "K=200, Top_N=30: Precision=0.0547, Recall=0.5985, F1=0.0973\n",
      "K=200, Top_N=50: Precision=0.0397, Recall=0.6688, F1=0.0734\n",
      "K=200, Top_N=80: Precision=0.0295, Recall=0.7290, F1=0.0559\n",
      "K=200, Top_N=100: Precision=0.0254, Recall=0.7547, F1=0.0486\n",
      "\n",
      "=== BEST CONFIGURATIONS ===\n",
      "\n",
      "Best Threshold Configuration for Recall:\n",
      "K=200, T=0.5\n",
      "Precision=0.0886, Recall=0.4243, F1=0.1339\n",
      "\n",
      "Best Ranking Configuration for Recall:\n",
      "K=100, Top_N=100\n",
      "Precision=0.0254, Recall=0.7547, F1=0.0486\n",
      "\n",
      "Best Threshold Configuration for F1:\n",
      "K=30, T=0.5\n",
      "Precision=0.1071, Recall=0.3985, F1=0.1574\n",
      "\n",
      "Best Ranking Configuration for F1:\n",
      "K=30, Top_N=3\n",
      "Precision=0.1817, Recall=0.2643, F1=0.2011\n",
      "\n",
      "=== COMPARISON OF METHODS ===\n",
      "Ranking-based method achieved better recall\n",
      "Ranking-based method achieved better F1 score\n"
     ]
    }
   ],
   "source": [
    "#PROPER - e5_mnrl_DDP_with_prefixes_4gpu/epoch_1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Set max sequence length to 256 (increased from 128)\n",
    "max_seq_length = 256\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_with_prefixes/epoch_2_subset\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "model.max_seq_length = max_seq_length  # Set max sequence length\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create properly formatted passages for the legacy attributes\n",
    "# Format: \"passage: [label]. [definition]\" to avoid nested prefixes\n",
    "legacy_attributes_with_prefix = []\n",
    "for i in range(len(legacy_df)):\n",
    "    label = legacy_df['label'].iloc[i]\n",
    "    definition = legacy_df['definition'].iloc[i]\n",
    "    prefixed_text = f\"passage: {label}. {definition}\"\n",
    "    legacy_attributes_with_prefix.append(prefixed_text)\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "    batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Format queries with proper prefix\n",
    "sentences_with_prefix = [f\"query: {sentence}\" for sentence in sentences_df['Sentences']]\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "    batch = sentences_with_prefix[i:i+batch_size]\n",
    "    # Ensure normalization is consistent with training\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Modified evaluation approach that uses relative ranking instead of absolute thresholds\n",
    "def get_metrics_by_ranking(K, top_n_results):\n",
    "    \"\"\"\n",
    "    Evaluate using relative ranking - take top N results per query regardless of score\n",
    "    \n",
    "    Args:\n",
    "        K: Number of results to retrieve with FAISS\n",
    "        top_n_results: Number of top results to consider as \"positive\" predictions\n",
    "    \"\"\"\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results using ranking approach\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels from top N results regardless of similarity score\n",
    "        for j in range(min(top_n_results, len(I_faiss[i]))):\n",
    "            idx_pos = I_faiss[i][j]\n",
    "            if idx_pos >= 0:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Keep the original threshold-based evaluation for comparison\n",
    "def get_metrics_by_threshold(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Test both evaluation approaches\n",
    "K_values = [30, 50, 80, 100, 120, 200]\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5]  # Keeping your original thresholds\n",
    "top_n_values = [1, 3, 5, 10, 15, 20, 30, 50, 80, 100]  # Top N results to consider\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 1: THRESHOLD-BASED ===\")\n",
    "threshold_results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for threshold in thresholds:\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics_by_threshold(K, threshold)\n",
    "        threshold_results.append({\n",
    "            'K': K,\n",
    "            'Threshold': threshold,\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1': avg_f1\n",
    "        })\n",
    "        print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== EVALUATION APPROACH 2: RANKING-BASED ===\")\n",
    "ranking_results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for top_n in top_n_values:\n",
    "        if top_n <= K:  # Can't have top_n greater than K\n",
    "            avg_precision, avg_recall, avg_f1 = get_metrics_by_ranking(K, top_n)\n",
    "            ranking_results.append({\n",
    "                'K': K,\n",
    "                'Top_N': top_n,\n",
    "                'Precision': avg_precision,\n",
    "                'Recall': avg_recall,\n",
    "                'F1': avg_f1\n",
    "            })\n",
    "            print(f\"K={K}, Top_N={top_n}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "# Find the best configurations\n",
    "best_threshold_recall = max(threshold_results, key=lambda x: x['Recall'])\n",
    "best_ranking_recall = max(ranking_results, key=lambda x: x['Recall'])\n",
    "best_threshold_f1 = max(threshold_results, key=lambda x: x['F1'])\n",
    "best_ranking_f1 = max(ranking_results, key=lambda x: x['F1'])\n",
    "\n",
    "print(\"\\n=== BEST CONFIGURATIONS ===\")\n",
    "print(\"\\nBest Threshold Configuration for Recall:\")\n",
    "print(f\"K={best_threshold_recall['K']}, T={best_threshold_recall['Threshold']}\")\n",
    "print(f\"Precision={best_threshold_recall['Precision']:.4f}, Recall={best_threshold_recall['Recall']:.4f}, F1={best_threshold_recall['F1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Ranking Configuration for Recall:\")\n",
    "print(f\"K={best_ranking_recall['K']}, Top_N={best_ranking_recall['Top_N']}\")\n",
    "print(f\"Precision={best_ranking_recall['Precision']:.4f}, Recall={best_ranking_recall['Recall']:.4f}, F1={best_ranking_recall['F1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Threshold Configuration for F1:\")\n",
    "print(f\"K={best_threshold_f1['K']}, T={best_threshold_f1['Threshold']}\")\n",
    "print(f\"Precision={best_threshold_f1['Precision']:.4f}, Recall={best_threshold_f1['Recall']:.4f}, F1={best_threshold_f1['F1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Ranking Configuration for F1:\")\n",
    "print(f\"K={best_ranking_f1['K']}, Top_N={best_ranking_f1['Top_N']}\")\n",
    "print(f\"Precision={best_ranking_f1['Precision']:.4f}, Recall={best_ranking_f1['Recall']:.4f}, F1={best_ranking_f1['F1']:.4f}\")\n",
    "\n",
    "# Compare overall best performance between methods\n",
    "print(\"\\n=== COMPARISON OF METHODS ===\")\n",
    "if best_ranking_recall['Recall'] > best_threshold_recall['Recall']:\n",
    "    print(\"Ranking-based method achieved better recall\")\n",
    "else:\n",
    "    print(\"Threshold-based method achieved better recall\")\n",
    "\n",
    "if best_ranking_f1['F1'] > best_threshold_f1['F1']:\n",
    "    print(\"Ranking-based method achieved better F1 score\")\n",
    "else:\n",
    "    print(\"Threshold-based method achieved better F1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50056b6-b5bb-4820-b732-a85d1ea2c289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-04-22 16:45:49.361898: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-22 16:45:49.361930: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-22 16:45:49.361954: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-22 16:45:49.370433: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-22 16:45:50.268185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, T=0.9: Precision=0.2547, Recall=0.0196, F1=0.0342\n",
      "K=30, T=0.85: Precision=0.2321, Recall=0.0499, F1=0.0729\n",
      "K=30, T=0.8: Precision=0.1745, Recall=0.0964, F1=0.1092\n",
      "K=30, T=0.7: Precision=0.0996, Recall=0.2277, F1=0.1271\n",
      "K=30, T=0.5: Precision=0.0476, Recall=0.4512, F1=0.0836\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, T=0.9: Precision=0.2547, Recall=0.0196, F1=0.0342\n",
      "K=50, T=0.85: Precision=0.2314, Recall=0.0499, F1=0.0727\n",
      "K=50, T=0.8: Precision=0.1708, Recall=0.0970, F1=0.1074\n",
      "K=50, T=0.7: Precision=0.0937, Recall=0.2356, F1=0.1222\n",
      "K=50, T=0.5: Precision=0.0375, Recall=0.5040, F1=0.0681\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, T=0.9: Precision=0.2547, Recall=0.0196, F1=0.0342\n",
      "K=80, T=0.85: Precision=0.2314, Recall=0.0499, F1=0.0727\n",
      "K=80, T=0.8: Precision=0.1699, Recall=0.0972, F1=0.1068\n",
      "K=80, T=0.7: Precision=0.0909, Recall=0.2399, F1=0.1194\n",
      "K=80, T=0.5: Precision=0.0310, Recall=0.5483, F1=0.0573\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, T=0.9: Precision=0.2547, Recall=0.0196, F1=0.0342\n",
      "K=100, T=0.85: Precision=0.2314, Recall=0.0499, F1=0.0727\n",
      "K=100, T=0.8: Precision=0.1699, Recall=0.0972, F1=0.1068\n",
      "K=100, T=0.7: Precision=0.0899, Recall=0.2405, F1=0.1184\n",
      "K=100, T=0.5: Precision=0.0285, Recall=0.5653, F1=0.0530\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, T=0.9: Precision=0.2547, Recall=0.0196, F1=0.0342\n",
      "K=120, T=0.85: Precision=0.2314, Recall=0.0499, F1=0.0727\n",
      "K=120, T=0.8: Precision=0.1699, Recall=0.0972, F1=0.1068\n",
      "K=120, T=0.7: Precision=0.0895, Recall=0.2408, F1=0.1178\n",
      "K=120, T=0.5: Precision=0.0268, Recall=0.5799, F1=0.0500\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, T=0.9: Precision=0.2547, Recall=0.0196, F1=0.0342\n",
      "K=200, T=0.85: Precision=0.2314, Recall=0.0499, F1=0.0727\n",
      "K=200, T=0.8: Precision=0.1699, Recall=0.0972, F1=0.1068\n",
      "K=200, T=0.7: Precision=0.0893, Recall=0.2409, F1=0.1175\n",
      "K=200, T=0.5: Precision=0.0229, Recall=0.6073, F1=0.0431\n",
      "\n",
      "=== Best Configuration for Recall ===\n",
      "K=200, T=0.5\n",
      "Precision=0.0229, Recall=0.6073, F1=0.0431\n",
      "\n",
      "None of the tested configurations achieved 0.9 recall.\n",
      "Suggestions for improving recall:\n",
      "1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\n",
      "2. Try higher K values (e.g., 300, 500, 1000)\n",
      "3. Consider aggregating predictions from multiple threshold levels\n",
      "4. Examine specific jobs with low recall to identify patterns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_with_prefixes_4gpu/epoch_1\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create label text for semantic search (format: label: definition)\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "\n",
    "# Add passage prefix to legacy attributes (these are documents to be retrieved)\n",
    "legacy_attributes_with_prefix = [\"passage: \" + attr for attr in legacy_attributes]\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes_with_prefix), batch_size):\n",
    "    batch = legacy_attributes_with_prefix[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Add query prefix to sentences (these are queries searching for matching passages)\n",
    "sentences_with_prefix = [\"query: \" + sentence for sentence in sentences_df['Sentences']]\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_with_prefix), batch_size):\n",
    "    batch = sentences_with_prefix[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Function to get predictions and metrics for a specific K and threshold\n",
    "def get_metrics(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(normalize_attribute(label))  # Normalize here too\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(job_predictions.get(job_id, []))  # Already normalized\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Test all combinations\n",
    "K_values = [30, 50, 80, 100, 120, 200]\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5]\n",
    "\n",
    "results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for threshold in thresholds:\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics(K, threshold)\n",
    "        results.append({\n",
    "            'K': K,\n",
    "            'Threshold': threshold,\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1': avg_f1\n",
    "        })\n",
    "        print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "# Find the best configuration for recall\n",
    "best_recall = -1\n",
    "best_config = None\n",
    "\n",
    "for result in results:\n",
    "    if result['Recall'] > best_recall:\n",
    "        best_recall = result['Recall']\n",
    "        best_config = result\n",
    "\n",
    "print(\"\\n=== Best Configuration for Recall ===\")\n",
    "print(f\"K={best_config['K']}, T={best_config['Threshold']}\")\n",
    "print(f\"Precision={best_config['Precision']:.4f}, Recall={best_config['Recall']:.4f}, F1={best_config['F1']:.4f}\")\n",
    "\n",
    "# If the best recall is still below 0.9, suggest other approaches\n",
    "if best_recall < 0.9:\n",
    "    print(\"\\nNone of the tested configurations achieved 0.9 recall.\")\n",
    "    print(\"Suggestions for improving recall:\")\n",
    "    print(\"1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\")\n",
    "    print(\"2. Try higher K values (e.g., 300, 500, 1000)\")\n",
    "    print(\"3. Consider aggregating predictions from multiple threshold levels\")\n",
    "    print(\"4. Examine specific jobs with low recall to identify patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b865995-5d4c-461a-af22-d820b7bbe49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in golden_dataset_sentences.csv: ['Job_ID', 'Job_Text', 'Sentences']\n",
      "Columns in golden_dataset_labels.csv: ['job_id', 'uuid', 'suid', 'display_name', 'text_snippet', 'start_index', 'labeling_task', 'label', 'isTitleSnippet']\n",
      "CSV file saved to: /home/jovyan/butterfly/src/notebooks/part3_evaluations/prediction_results.csv\n",
      "FAISS - Average Precision: 0.0252, Average Recall: 0.5336\n",
      "Cosine - Average Precision: 0.0252, Average Recall: 0.5336\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_test/epoch_3\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Concatenate label and definition\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "legacy_embeddings = model.encode(legacy_attributes.tolist(), normalize_embeddings=True)\n",
    "\n",
    "# Build FAISS index with legacy embeddings\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "print(\"Columns in golden_dataset_sentences.csv:\", sentences_df.columns.tolist())\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "sentence_embeddings = model.encode(sentences_df['Sentences'].tolist(), normalize_embeddings=True)\n",
    "\n",
    "# Set hyperparameters\n",
    "K = 100\n",
    "T = 0.6\n",
    "\n",
    "# --- FAISS Mechanism ---\n",
    "D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "extracted_labels_faiss = []\n",
    "for i in range(len(sentences_df)):\n",
    "    top_k_indices = I_faiss[i]\n",
    "    top_k_similarities = D_faiss[i]\n",
    "    labels = [legacy_df['label'].iloc[idx] for idx, sim in zip(top_k_indices, top_k_similarities) if sim > T]\n",
    "    extracted_labels_faiss.append(labels)\n",
    "sentences_df['extracted_labels_faiss'] = extracted_labels_faiss\n",
    "predicted_attributes_faiss = sentences_df.groupby('job_id')['extracted_labels_faiss'].apply(lambda x: list(set(sum(x, [])))).to_dict()\n",
    "\n",
    "# Debug: Print predicted labels for job_id '5626705964'\n",
    "if '5626705964' in predicted_attributes_faiss:\n",
    "    print(\"FAISS Predicted Labels for job_id '5626705964':\", predicted_attributes_faiss['5626705964'])\n",
    "\n",
    "# --- Direct Cosine Similarity Mechanism ---\n",
    "similarity_matrix = np.dot(sentence_embeddings, legacy_embeddings.T)\n",
    "top_k_indices_cosine = np.argsort(similarity_matrix, axis=1)[:, -K:][:, ::-1]\n",
    "top_k_similarities_cosine = np.take_along_axis(similarity_matrix, top_k_indices_cosine, axis=1)\n",
    "extracted_labels_cosine = []\n",
    "for i in range(len(sentences_df)):\n",
    "    indices = top_k_indices_cosine[i]\n",
    "    similarities = top_k_similarities_cosine[i]\n",
    "    labels = [legacy_df['label'].iloc[idx] for idx, sim in zip(indices, similarities) if sim > T]\n",
    "    extracted_labels_cosine.append(labels)\n",
    "sentences_df['extracted_labels_cosine'] = extracted_labels_cosine\n",
    "predicted_attributes_cosine = sentences_df.groupby('job_id')['extracted_labels_cosine'].apply(lambda x: list(set(sum(x, [])))).to_dict()\n",
    "\n",
    "# Debug: Print predicted labels for job_id '5626705964'\n",
    "if '5626705964' in predicted_attributes_cosine:\n",
    "    print(\"Cosine Predicted Labels for job_id '5626705964':\", predicted_attributes_cosine['5626705964'])\n",
    "\n",
    "# --- Evaluation ---\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "print(\"Columns in golden_dataset_labels.csv:\", labels_df.columns.tolist())\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "# Debug: Print ground truth for job_id '5626705964'\n",
    "if '5626705964' in ground_truth:\n",
    "    print(\"Ground Truth Labels for job_id '5626705964':\", ground_truth['5626705964'])\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "output_data = []\n",
    "for job_id in ground_truth:\n",
    "    gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "    pred_faiss = set(normalize_attribute(attr) for attr in predicted_attributes_faiss.get(job_id, []))\n",
    "    tp_faiss = len(gt.intersection(pred_faiss))\n",
    "    fp_faiss = len(pred_faiss - gt)\n",
    "    fn_faiss = len(gt - pred_faiss)\n",
    "    precision_faiss = tp_faiss / (tp_faiss + fp_faiss) if tp_faiss + fp_faiss > 0 else 0\n",
    "    recall_faiss = tp_faiss / (tp_faiss + fn_faiss) if tp_faiss + fn_faiss > 0 else 0\n",
    "    \n",
    "    pred_cosine = set(normalize_attribute(attr) for attr in predicted_attributes_cosine.get(job_id, []))\n",
    "    tp_cosine = len(gt.intersection(pred_cosine))\n",
    "    fp_cosine = len(pred_cosine - gt)\n",
    "    fn_cosine = len(gt - pred_cosine)\n",
    "    precision_cosine = tp_cosine / (tp_cosine + fp_cosine) if tp_cosine + fp_cosine > 0 else 0\n",
    "    recall_cosine = tp_cosine / (tp_cosine + fn_cosine) if tp_cosine + fn_cosine > 0 else 0\n",
    "    \n",
    "    output_data.append({\n",
    "        \"job_id\": job_id,\n",
    "        \"ground_truth\": \", \".join(sorted(ground_truth[job_id])),\n",
    "        \"predicted_faiss\": \", \".join(sorted(predicted_attributes_faiss.get(job_id, []))),\n",
    "        \"predicted_cosine\": \", \".join(sorted(predicted_attributes_cosine.get(job_id, []))),\n",
    "        \"precision_faiss\": precision_faiss,\n",
    "        \"recall_faiss\": recall_faiss,\n",
    "        \"precision_cosine\": precision_cosine,\n",
    "        \"recall_cosine\": recall_cosine\n",
    "    })\n",
    "\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"/home/jovyan/butterfly/src/notebooks/part3_evaluations\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the CSV file\n",
    "output_path = os.path.join(output_dir, \"prediction_results.csv\")\n",
    "output_df.to_csv(output_path, index=False)\n",
    "print(f\"CSV file saved to: {output_path}\")\n",
    "\n",
    "print(f\"FAISS - Average Precision: {output_df['precision_faiss'].mean():.4f}, Average Recall: {output_df['recall_faiss'].mean():.4f}\")\n",
    "print(f\"Cosine - Average Precision: {output_df['precision_cosine'].mean():.4f}, Average Recall: {output_df['recall_cosine'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be9035ca-3056-4f64-b8eb-adfdc48ba952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total labels in legacy.tsv: 23774\n",
      "Total unique display_name values in golden_dataset_labels.csv (POSITIVE): 4251\n",
      "\n",
      "Common labels (present in both): 3914\n",
      "\n",
      "Labels in golden_dataset_labels.csv but not in legacy.tsv: 337\n",
      "\n",
      "Labels in legacy.tsv but not in golden_dataset_labels.csv: 19860\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Extract labels from legacy.tsv\n",
    "legacy_labels = set(legacy_df['label'].str.lower().str.strip())\n",
    "\n",
    "# Extract positive display_name values from golden_dataset_labels.csv\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "golden_labels = set(positive_labels['display_name'].str.lower().str.strip())\n",
    "\n",
    "# Compare the labels\n",
    "common_labels = legacy_labels.intersection(golden_labels)\n",
    "golden_not_in_legacy = golden_labels - legacy_labels\n",
    "legacy_not_in_golden = legacy_labels - golden_labels\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total labels in legacy.tsv: {len(legacy_labels)}\")\n",
    "print(f\"Total unique display_name values in golden_dataset_labels.csv (POSITIVE): {len(golden_labels)}\")\n",
    "print(f\"\\nCommon labels (present in both): {len(common_labels)}\")\n",
    "print(f\"\\nLabels in golden_dataset_labels.csv but not in legacy.tsv: {len(golden_not_in_legacy)}\")\n",
    "#print(sorted(golden_not_in_legacy))\n",
    "print(f\"\\nLabels in legacy.tsv but not in golden_dataset_labels.csv: {len(legacy_not_in_golden)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5010d61f-4660-4f3e-b0ca-f3271c6c67a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'label' column: 0\n",
      "Number of NaN values in 'definition' column: 1647\n",
      "Rows with NaN in either 'label' or 'definition':\n",
      "        suid                                              label definition\n",
      "16225  JBGTS         Veterinary medicine experience (1-2 years)        NaN\n",
      "17857  239HK       Database architecture experience (1-2 years)        NaN\n",
      "18162  39PWM  Highest single-project number of utility compa...        NaN\n",
      "20630  UXBHS  Largest technology infrastructure engineering ...        NaN\n",
      "20631  BY7HE  Veterinary vaccination programs developed (2-3...        NaN\n",
      "...      ...                                                ...        ...\n",
      "24413  MMR9X  Long-term front-end development projects contr...        NaN\n",
      "24414  GJF3M  Largest medical device development budget mana...        NaN\n",
      "24415  BWKGW  Contributing to long-term front-end developmen...        NaN\n",
      "24416  772ZE     Implementing new investor relations strategies        NaN\n",
      "24417  Q4MGA                   Biomedical engineering practices        NaN\n",
      "\n",
      "[1647 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load and inspect defs.tsv\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "print(\"Number of NaN values in 'label' column:\", legacy_df['label'].isna().sum())\n",
    "print(\"Number of NaN values in 'definition' column:\", legacy_df['definition'].isna().sum())\n",
    "print(\"Rows with NaN in either 'label' or 'definition':\")\n",
    "print(legacy_df[legacy_df['label'].isna() | legacy_df['definition'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8acd6c8d-801a-4bb7-b590-e9232e646a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in golden_dataset_sentences.csv: ['Job_ID', 'Job_Text', 'Sentences']\n",
      "Columns in golden_dataset_labels.csv: ['job_id', 'uuid', 'suid', 'display_name', 'text_snippet', 'start_index', 'labeling_task', 'label', 'isTitleSnippet']\n",
      "CSV file saved to: /home/jovyan/butterfly/src/notebooks/part3_evaluations/prediction_results_improved.csv\n",
      "FAISS - Average Precision: 0.0144, Average Recall: 0.6944\n",
      "Recall still below target, implementing more aggressive strategy...\n",
      "Aggressive FAISS - Average Precision: 0.0112, Average Recall: 0.7464\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_test/epoch_3\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Concatenate label and definition\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "legacy_embeddings = model.encode(legacy_attributes.tolist(), normalize_embeddings=True)\n",
    "\n",
    "# Build FAISS index with legacy embeddings\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "# Use IndexFlatIP for inner product (cosine similarity since vectors are normalized)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "print(\"Columns in golden_dataset_sentences.csv:\", sentences_df.columns.tolist())\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Encode sentences\n",
    "sentence_embeddings = model.encode(sentences_df['Sentences'].tolist(), normalize_embeddings=True)\n",
    "\n",
    "# Set hyperparameters - MODIFIED\n",
    "K = 200  # Increased from 100 to capture more potential matches\n",
    "T = 0.4   # Lowered threshold from 0.6 to 0.4 to increase recall\n",
    "\n",
    "# --- FAISS Mechanism ---\n",
    "D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "extracted_labels_faiss = []\n",
    "extracted_scores_faiss = []\n",
    "\n",
    "for i in range(len(sentences_df)):\n",
    "    top_k_indices = I_faiss[i]\n",
    "    top_k_similarities = D_faiss[i]\n",
    "    \n",
    "    # Store labels and their similarity scores\n",
    "    labels_with_scores = [(legacy_df['label'].iloc[idx], sim) \n",
    "                          for idx, sim in zip(top_k_indices, top_k_similarities) \n",
    "                          if sim > T]\n",
    "    \n",
    "    # Extract just the labels\n",
    "    labels = [label for label, _ in labels_with_scores]\n",
    "    scores = [score for _, score in labels_with_scores]\n",
    "    \n",
    "    extracted_labels_faiss.append(labels)\n",
    "    extracted_scores_faiss.append(scores)\n",
    "\n",
    "sentences_df['extracted_labels_faiss'] = extracted_labels_faiss\n",
    "sentences_df['extracted_scores_faiss'] = extracted_scores_faiss\n",
    "\n",
    "# Implement a weighted voting mechanism for job-level aggregation\n",
    "job_predictions_faiss = {}\n",
    "for job_id in sentences_df['job_id'].unique():\n",
    "    job_rows = sentences_df[sentences_df['job_id'] == job_id]\n",
    "    \n",
    "    # Collect all labels with their scores across all sentences for this job\n",
    "    all_labels_with_scores = []\n",
    "    for labels, scores in zip(job_rows['extracted_labels_faiss'], job_rows['extracted_scores_faiss']):\n",
    "        all_labels_with_scores.extend(list(zip(labels, scores)))\n",
    "    \n",
    "    # Group by label and calculate weighted score\n",
    "    label_scores = {}\n",
    "    for label, score in all_labels_with_scores:\n",
    "        if label in label_scores:\n",
    "            label_scores[label] += score\n",
    "        else:\n",
    "            label_scores[label] = score\n",
    "    \n",
    "    # Sort by score and take all labels (no filtering at job level)\n",
    "    sorted_labels = sorted(label_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    job_predictions_faiss[job_id] = [label for label, _ in sorted_labels]\n",
    "\n",
    "# --- Evaluation ---\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "print(\"Columns in golden_dataset_labels.csv:\", labels_df.columns.tolist())\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "# Debug: Print ground truth for job_id '5626705964'\n",
    "if '5626705964' in ground_truth:\n",
    "    print(\"Ground Truth Labels for job_id '5626705964':\", ground_truth['5626705964'])\n",
    "    print(\"Predicted Labels for job_id '5626705964':\", job_predictions_faiss.get('5626705964', []))\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "output_data = []\n",
    "for job_id in ground_truth:\n",
    "    gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "    \n",
    "    pred_faiss = set(normalize_attribute(attr) for attr in job_predictions_faiss.get(job_id, []))\n",
    "    tp_faiss = len(gt.intersection(pred_faiss))\n",
    "    fp_faiss = len(pred_faiss - gt)\n",
    "    fn_faiss = len(gt - pred_faiss)\n",
    "    precision_faiss = tp_faiss / (tp_faiss + fp_faiss) if tp_faiss + fp_faiss > 0 else 0\n",
    "    recall_faiss = tp_faiss / (tp_faiss + fn_faiss) if tp_faiss + fn_faiss > 0 else 0\n",
    "    \n",
    "    output_data.append({\n",
    "        \"job_id\": job_id,\n",
    "        \"ground_truth\": \", \".join(sorted(ground_truth[job_id])),\n",
    "        \"predicted_faiss\": \", \".join(sorted(job_predictions_faiss.get(job_id, []))),\n",
    "        \"precision_faiss\": precision_faiss,\n",
    "        \"recall_faiss\": recall_faiss,\n",
    "        \"f1_faiss\": 2 * precision_faiss * recall_faiss / (precision_faiss + recall_faiss) if precision_faiss + recall_faiss > 0 else 0\n",
    "    })\n",
    "\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"/home/jovyan/butterfly/src/notebooks/part3_evaluations\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the CSV file\n",
    "output_path = os.path.join(output_dir, \"prediction_results_improved.csv\")\n",
    "output_df.to_csv(output_path, index=False)\n",
    "print(f\"CSV file saved to: {output_path}\")\n",
    "\n",
    "print(f\"FAISS - Average Precision: {output_df['precision_faiss'].mean():.4f}, Average Recall: {output_df['recall_faiss'].mean():.4f}\")\n",
    "\n",
    "# If recall is still not high enough, we could implement an even more aggressive approach\n",
    "if output_df['recall_faiss'].mean() < 0.9:\n",
    "    print(\"Recall still below target, implementing more aggressive strategy...\")\n",
    "    \n",
    "    # Very low threshold approach\n",
    "    T_aggressive = 0.2  # Even lower threshold\n",
    "    K_aggressive = 300  # Even more candidates\n",
    "    \n",
    "    D_faiss_agg, I_faiss_agg = index.search(sentence_embeddings, K_aggressive)\n",
    "    job_predictions_aggressive = {}\n",
    "    \n",
    "    for job_id in sentences_df['job_id'].unique():\n",
    "        job_indices = sentences_df[sentences_df['job_id'] == job_id].index.tolist()\n",
    "        \n",
    "        # Collect all labels with their scores across all sentences for this job\n",
    "        all_labels = []\n",
    "        for i in job_indices:\n",
    "            labels = [legacy_df['label'].iloc[idx] for idx, sim in \n",
    "                      zip(I_faiss_agg[i], D_faiss_agg[i]) if sim > T_aggressive]\n",
    "            all_labels.extend(labels)\n",
    "        \n",
    "        # Count frequency of each label\n",
    "        label_counts = Counter(all_labels)\n",
    "        \n",
    "        # Take all labels that appear at least once\n",
    "        job_predictions_aggressive[job_id] = list(label_counts.keys())\n",
    "    \n",
    "    # Evaluate aggressive approach\n",
    "    output_data_aggressive = []\n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred_agg = set(normalize_attribute(attr) for attr in job_predictions_aggressive.get(job_id, []))\n",
    "        tp_agg = len(gt.intersection(pred_agg))\n",
    "        fp_agg = len(pred_agg - gt)\n",
    "        fn_agg = len(gt - pred_agg)\n",
    "        precision_agg = tp_agg / (tp_agg + fp_agg) if tp_agg + fp_agg > 0 else 0\n",
    "        recall_agg = tp_agg / (tp_agg + fn_agg) if tp_agg + fn_agg > 0 else 0\n",
    "        \n",
    "        output_data_aggressive.append({\n",
    "            \"job_id\": job_id,\n",
    "            \"ground_truth\": \", \".join(sorted(ground_truth[job_id])),\n",
    "            \"predicted_aggressive\": \", \".join(sorted(job_predictions_aggressive.get(job_id, []))),\n",
    "            \"precision_aggressive\": precision_agg,\n",
    "            \"recall_aggressive\": recall_agg,\n",
    "            \"f1_aggressive\": 2 * precision_agg * recall_agg / (precision_agg + recall_agg) if precision_agg + recall_agg > 0 else 0\n",
    "        })\n",
    "    \n",
    "    output_df_aggressive = pd.DataFrame(output_data_aggressive)\n",
    "    output_path_aggressive = os.path.join(output_dir, \"prediction_results_aggressive.csv\")\n",
    "    output_df_aggressive.to_csv(output_path_aggressive, index=False)\n",
    "    \n",
    "    print(f\"Aggressive FAISS - Average Precision: {output_df_aggressive['precision_aggressive'].mean():.4f}, Average Recall: {output_df_aggressive['recall_aggressive'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71b2f29d-45f3-4c58-b71f-2fd24c900f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "Loading SentenceTransformer model...\n",
      "Loading definition data...\n",
      "Encoding 24589 legacy attributes...\n",
      "  Encoded 16000/24589 attributes\n",
      "  Encoded 24589/24589 attributes\n",
      "Creating FAISS index with dimension 768...\n",
      "Loading sentence data...\n",
      "Columns in golden_dataset_sentences.csv: ['Job_ID', 'Job_Text', 'Sentences']\n",
      "Encoding 7943 sentences...\n",
      "  Encoded 7943/7943 sentences\n",
      "Performing FAISS search with K=200 and threshold=0.3...\n",
      "Loading ground truth data...\n",
      "Columns in golden_dataset_labels.csv: ['job_id', 'uuid', 'suid', 'display_name', 'text_snippet', 'start_index', 'labeling_task', 'label', 'isTitleSnippet']\n",
      "Sample job 5577404285:\n",
      "  Ground truth: ['ability to lift 50 pounds', 'computer skills', 'dealership', 'detailing', 'email', 'employee discount', 'full-time', 'motorcycle license', 'porter experience']\n",
      "  Predicted: ['Paper production line team management (1-5 employees)', 'Car dealership', 'Gulfstream G650 operation', 'Arkansas', 'Internal job posting program', 'Major shipping companies collaborated with (6-10 companies)', 'High female employees rate', 'Recycled asphalt pavement production', 'Florida', 'Free bicycle parking', 'Embraer 190 operation', 'Flight mechanics', 'Order placement', 'Equal Employment Opportunity (EEO)', 'Class 2 Gasoline Automobile Mechanic License', 'Motorcycle Driver Licence', 'Auto painting', 'Suit required', 'Automotive upholstery', 'Minijob', 'Fixed shift', 'Other (types of filling operations)', 'Resume', 'Post office', 'Nissan Certification', 'Female managers', 'Podded propulsion', 'Cafeteria plan', 'Laptop', 'Motor Vehicle Inspection License', 'Largest team size managed in a technology infrastructure role (More than 20 team members)', 'CACES R485 Category 1', 'Leisure facility discount', 'Manager position hire', '40-60%', 'No necktie OK', 'Bicycle Mechanic Certification', 'Vehicle Operation Skills', 'Chiropractor License', 'Long vacation', 'Eating space provided', \"Driver's License category A\", 'Millennium', 'Individual Number Card required', 'Piano Tuning Technician License', 'Largest company supported by revenue ($101-$500M)', 'Bicycle shop', 'CACES R485 Category 2', 'Completed Apprenticeship for Gunsmiths', 'Cosmetics store', 'Staff meals provided', 'Food provided', 'Generator repair', 'Many 40s in workplace', 'Machine operators trained (6-10 operators)', 'MR Licence', 'Other (asphalt production methods)', 'Completed Apprenticeship for Leather Production & Tannery Technology Specialists', 'Retail management', 'Lead nurturing email', 'Other (inventory management tasks)', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Retail Services', 'Largest team size managed in an executive level role (6-10 team members)', 'NATEF Certification', 'Drug store', 'SKUs managed (More than 1M SKUs)', 'GM Certification', 'Aerial fire service apparatus', 'Completed Apprenticeship for Brewers & Maltsters', '1 month notice period', 'Bihar', 'GWO Working at Heights', 'Largest company supported by revenue ($501M-$1B)', 'SKUs managed (100,000-1M SKUs)', 'Instant interview scheduling', 'Work in various locations', 'Frequent customer interaction', 'Marine weather forecasting', 'Farmers trained on agricultural best practices (More than 200 farmers)', 'Illinois', 'Glider towing experience', 'Car accessory shop', 'Command Master Chief Petty Officer', 'Parking provided', 'Steel detailing', 'Vehicle charging infrastructure', 'Desk work', 'Work in fixed location', 'Staff Supervisor', 'Email A/B testing', 'Completed Apprenticeship for Administrative & Business Operations Specialists for Wholesale & Foreign Trade', 'Bicycle Safety Mechanic Certification', 'Transfer station', 'Limousine', 'Fortune 500 candidate placement', 'Aviation upholstery', 'Indeed', 'Auto restoration', 'Completed Apprenticeship for Body & Vehicle Construction Mechanics', 'Personal car', 'Daily prescriptions dispensed (200-250 prescriptions)', 'Food stand', 'Internal & external discount scheme', \"Driver's License category A1\", 'Articulated robots', 'Completed CME trainings (30-40 hours)', '10-20%', 'Pliers (upholstery tools)', 'Athletes cared for monthly (1-10 athletes)', 'Other (automotive certification)', 'CACES R489 Category 2B', 'Vending machines', 'F&I Certification', 'Fire safety training completed (40-50 hours)', 'Wine store', 'Financing motor vehicles', 'Work Settings', 'Faxing', 'Annual early career full-time hires (21-30 hires)', '3 month notice period', 'Second hand store', 'Major', 'Valet parking', 'Largest team size managed in a technology infrastructure role (6-10 team members)', 'Largest client employee count (25,000-50,000 employees)', 'Boat Licence A under 12 miles', 'Completed Apprenticeship for Automotive Mechatronics Technicians', 'Jeans OK', 'Other (upholstery tools)', 'Company cafeteria', 'Written references', 'Athletes cared for monthly (More than 50 athletes)', 'Marine Engine Driver Grade 2 near Coastal', \"A1 Driver's License\", 'New Mexico', 'No age limitations', 'Company computer', 'Wyoming', 'Locksmith Skill Examination', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Digital & Print Media', 'Spa services', 'Auto glass', 'Funeral directing', 'Watchkeeper Deck Less than 3000 Gross Tonnage (Yachts)', 'Webbing stretchers (upholstery tools)', 'Internal award for outstanding performance received (More than three times)', 'Completed Apprenticeship for Funeral Services Specialists', 'Largest team managed as a chemist (6-10 team members)', 'Hammers (upholstery tools)', 'Mostly mid-career hires', 'Pressing machines', 'Metal building erection', 'Manual transmission', 'Completed Apprenticeship for Bank Clerks', 'Discount store', 'Surfing', 'Oil change', 'Shopping mall', 'Singaporean citizenship', 'References', 'Company tablet', 'Detailing', 'Completed Apprenticeship for Asphalt Builders', 'Towing License', 'Canteen', 'Daily prescriptions dispensed (100-150 prescriptions)', 'Grand opening promotion', 'Operations Chief of Organic Solvents Skill Training Course', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Sports & Fitness', 'Summer vacation', 'Consignment shop', 'Water aerobics', 'ASE Automobile Service Consultant Certification', 'Other (automotive repair & maintenance skill)', 'Height Work License', 'Gas station', 'Completed Apprenticeship for Agricultural Machinery Mechanics', 'MINDBODY', 'Gastronomy', 'AS Level in Panjabi', '4 wheeler license', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Air Traffic', 'Engine Room Watch Rating', 'Completed Apprenticeship for Lightweight Aircraft Builders', 'Funding-related materials', 'Query performance optimized (21-40%)', 'Gulfstream G600 operation', 'Other (upholstery maintenance tasks)', 'Watchkeeper Deck Less than 500 Gross Tonne', 'Other (martial arts skill)', 'Senior Chief Petty Officer', 'Dispensing prescriptions', 'Fixed working hours system', 'Supportive manager', 'Cold-mix asphalt production', 'Annual early career full-time hires (More than 50 hires)', 'AutoRaptor', 'Tracking inventory levels', 'Dairy plant team management (1-10 employees)', 'Completed Apprenticeship for Ice Cream Specialists', 'Mail', 'Small-sized Boat Operation License', 'Category A1 Licence', 'Largest company supported by revenue (More than $1B)', 'Largest client employee count (More than 50,000 employees)', 'Daily prescriptions dispensed (150-200 prescriptions)', 'WhatsApp', 'Standing job', 'Infrequently uses physical strength', 'Structural detailing for transportation structures', 'Retail buying', 'De Havilland Dash 8 aircraft operation', 'Funeral Director Certification', 'Chief Petty Officer', 'Chief Warrant Officer 2', 'Second-hand clothing store', 'Other (maintenance, installation & repair skill)', 'UK shift', 'CACES R489 Category 6', 'General Purpose Hand near Coastal', 'Motor coach', 'Large Sized Motorcycle License', \"Category A2 Driver's License\", 'Gardening and florist technician', 'Completed Apprenticeship for Aircraft Mechanics', 'Rajasthan', 'Tire service', 'Completed Apprenticeship for Vending & Game Machine Specialists', 'Any academic background', 'Ground operations agent experience', 'Marine upholstery', 'Resume with photo required', 'Pizzeria', 'RELEX Solutions', 'Bronze Medallion', \"Driver's License Category A\", 'Largest team size managed in an executive level role (More than 20 team members)', 'Step aerobics', 'Completed apprenticeship for Tire Technicians', 'Completed Apprenticeship for Electronics Technicians Specializing in Motors & Drive Technology', 'Kickboxing', 'Completed Apprenticeship for Sailmakers', 'Medium Vehicle License', 'Internal rate of return (IRR) analysis', 'Maintaining equipment', 'Completed Apprenticeship for Mechatronics Fitters', 'Welcome full-time', 'Fire safety training completed (30-40 hours)', 'Casually looking', 'Shocks & struts', 'Class 1 Automobile Mechanic License', 'Field service', 'Largest team managed as a chemist (1-5 team members)', 'Automotive electronics implementation', 'Kia Certification', 'Order entry', 'Side business', 'Major shipping companies collaborated with (3-5 companies)', 'Automotive mechatronics', 'Email list building', 'Single-engine piston aircraft piloting', '\\x08Kiosk', 'BMW Certification', 'Personal bicycle', 'Other (automotive administration skill)', 'Internal transfer request', 'Completed apprenticeship for Bicycle Mechanics', 'Frequent desk work', 'SKUs managed (100-1,000 SKUs)', 'Yard jockey', 'Specialist', 'ATR 72 aircraft operation', 'Propulsion systems', 'Menu planning', 'Only weekdays', 'Category A2 Licence', '50s can also apply', 'Frequent standing job', 'Descent performance calculations', 'Sport packages & cards', 'Completed Apprenticeship for Metallurgical & Semi-finished Goods Mechanics', 'Bicycle parking', 'Automobile Inspector License', 'Automatic inserting machines', 'Completed apprenticeship for Car-Body Mechanics', 'Automotive Technology', 'Free alcohol', 'Completed Apprenticeship for Plastics & Rubber Processing Mechanics', 'Internal transfer as needed', 'Fax', 'Household services', 'Class 3 Motor Vehicle Chassis Mechanic License', 'Shoe store', 'Indiana', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Tourism & Leisure', 'Frequent individual work', 'Completed Apprenticeship for Plant Mechanics', 'Largest team managed as a chemist (More than 20 team members)', 'Completed apprenticeship for Automotive Assistants', 'Gift voucher', 'Industrial mechanic experience', 'Shift options', 'Managing teams of recruiters', 'Repairing (upholstery maintenance)', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Transportation Services', 'Completed Apprenticeship for Painter & Varnishers', 'Near miss reporting', 'CACES Category 2', 'Pet shop', 'VinSolutions', 'Work as instructed in manual', 'Reserved parking', 'Standard Sized Motorcycle License', 'Completed Apprenticeship for Bicycle Mechanics', 'Air pillow machines', 'Daily prescriptions dispensed (250-300 prescriptions)', 'Telangana', 'Petroleum Land Management Certificate', 'Completed apprenticeship for Car-Body Repairers', 'Porous asphalt production', 'Sneakers OK', 'Equipment repair', 'Motorized Bicycle License', 'Candidates placed with Fortune 500 companies (1-10 candidates)', 'Embraer 175 operation', 'Resume screening', 'Completed CME trainings (40-50 hours)', 'Auto body repair', 'Cooling', 'Multi-engine piston aircraft piloting', 'Spreadsheet tracking', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Express & Postal Services', 'Other (automotive quick service skill)', 'Compressed hours', 'Mud program development', 'Other (types of jigs and fixtures)', 'Rate Transient Analysis (RTA)', 'Improving job placement rates', 'Largest team size managed in an executive level role (11-20 team members)', 'Auto service management', 'Direct hire', 'Specialty store', 'Prescription intake', 'RC book', 'Health club', 'Completed Apprenticeship for Artisan Distillers', 'Completed Apprenticeship for Cycle Mechatronics Technicians', \"Driver's License C1+E\", 'Reward system', 'No question about job changes', 'Food court', 'Diesel Mechanic Certification', 'No heavy lifting', 'Staple guns (upholstery tools)', 'I-CAR Certification', 'SKUs managed (1,000-10,000 SKUs)', '80-100%', 'Texas', 'Chief Mate Less than 3000 Gross Tonnage (Yachts)', 'Employment ice age generation', 'Order of veterinarians registration', 'Dress Codes', 'Skill Training Course for Operation of Transporting Vehicle on Rough Terrain', 'Reciprocating Steam Engine Licence', 'Diversity, equity and inclusion (DEI) turnover reports', 'Registered BAS Agent', 'Completed Apprenticeship for Production Mechanics', 'Other (store operations tasks)', 'Equine experience', 'Electric bike', 'Largest team size managed in an executive level role (1-5 team members)', 'Telephone appointments', 'Bank card ordering', 'Tobacconist', 'Bicycle repair', 'Part-time upon request', 'ZipRecruiter (sourcing)', 'Completed Apprenticeship for Florists', 'Business casual OK', 'Powertrain', 'Lowboy', 'Kenexa BrassRing', 'CACES R482 Category G', 'Completed Apprenticeship for Decorative Metalworkers', 'No attire restriction', \"Driver's License category BE\", 'Daily prescriptions dispensed (1-50 prescriptions)', 'Candidate position', 'Locker provided', 'Class 3 Automobile Mechanic License', 'Major shipping companies collaborated with (1-2 companies)', 'Honda certification', 'Granule filling', 'Class 3 Gasoline Automobile Mechanic License', 'Paper production line team management (6-10 employees)', 'International environment experience (11-20 years)', 'Application Details', 'Class 2 Diesel Automobile Mechanic License', '--Other (asphalt, concrete, or stone plant operations certifications)', 'Lubrication', 'Full-time', 'Petty Officer Third Class', 'High-speed filling machines', 'Annual early career full-time hires (1-10 hires)', 'Largest team size managed in a technology infrastructure role (1-5 team members)', 'Food & beverage discount', '50%', \"Driver's License category D\", 'Hyundai Certification', 'Visual merchandiser experience (4-5 years)', 'Completed Apprenticeship for Vehicle Fitters', 'Deli', 'Subsidized lunch', 'Car maintenance', 'Executive search candidate profiling', 'Proof of graduation required', 'Reservation Management', 'Class 3 Diesel Automobile Mechanic License', 'Reduced working hours', 'North Carolina', 'Electronic appliances store', 'Dealership experience', 'Suspension', 'Annual early career full-time hires', 'Reka-Vouchers or Reka-Money', 'Class 2 Automobile Mechanic License', 'ABS rules', '100-yen shop', 'Re-employment system', 'Motorcycle (delivery method)', 'Drug order put-away', 'Sail propulsion', 'Colonel', 'Accessory shop', 'Tugboat design projects', 'Manual Handling', 'Discounted lunch', 'Email', 'Special Automobile Mechanic License', 'Job share', 'U & I Turns', 'Largest client employee count (1-500 employees)', '90-100%', 'Journeyman Certificate for Florists', 'Certified Press Operator', 'Other (recruitment tools)', 'Proof of expected graduation required', 'Automotive insurance products', 'Apparel shop', 'Liquor store', 'Operational manual for business launch', 'Prospects-focused hire', 'Skill Training Course for Operations Chief of Specific Chemical Substances and Tetraalkyl Lead', 'Retail', 'Adjust shift for study', 'Velvet', 'Direct patient care weekly hours (30-40 hours)', \"Driver's License Category E\", 'Eating Area', 'Tack lifters (upholstery tools)', 'Hot-mix asphalt production', 'Liftgate', 'Largest team managed as a chemist (11-20 team members)', 'Goa', 'Car wash', 'Completed Apprenticeship for Upholsterers', 'Completed apprenticeship for Car-Body Painters', 'Insurance services', '60s also active', 'Manual handling', 'Gift card', 'Automotive buffing', \"Category A1 Driver's License\", 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Sea Transportation', 'SKUs managed (1-100 SKUs)', \"Driver's License category C1E\", '50s also active', 'Role Characteristics', 'Ports and harbors infrastructure', \"Driver's License category AM\", 'Renewable fuels', 'Pachinko parlor', 'General merchandise store', 'Flexible shift', 'US shift', 'Expansion hiring', 'Supervisor of Press Machine Operations Skill Training Course', 'Category A Licence', 'Full-time course', 'Electric drivetrain', 'Personal seal required', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Office Management', 'Candidates placed with Fortune 500 companies (11-20 candidates)', 'Consignment inventory management', 'Completed Apprenticeship for Equine Specialists', 'Medium-range weather forecasting', 'Aerial aerobatics experience', 'Armored vehicle', 'Mostly female employees', 'Embraer 195 operation', 'Dogging Ticket', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Automobile Sales', 'Completed Apprenticeship for Vulcaniser & Tire Mechanics', 'Report cards', 'Hybrid drivetrain', 'Completed Apprenticeship for Aircraft Electronics Technicians', 'Internal award for outstanding performance received (One time)', 'Pedicures', 'Job Levels', 'Department store', 'Car scheme', 'Daily prescriptions dispensed (50-100 prescriptions)', 'Other (locations)', 'Many 50s in workplace', \"A2 Driver's License\", 'DealerSocket', 'Diesel engine repair', \"Chauffeur's Permit\", 'Cycling', 'Check ordering', '60-80%', 'Used car dealership', 'Opening staff', 'Chrysler Certification', 'Picking', 'Airless paint spraying', '2 month notice period', 'Skill Training Course for Operations Chief of Excavating Natural Ground and Shoring', 'Confectionery', 'Cell phone store', 'Tennessee', 'Artificial lift system optimization', 'Automobile mechanic shop', 'New York', 'Largest team size managed in a technology infrastructure role (11-20 team members)', 'Other (vehicle operation skill)', 'Completed apprenticeship for Motorcycle Mechanics', 'Personality-focused employment', '40s also active', 'Alternative fuels', 'Completed Apprenticeship for Ship Mechanics', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Air Transportation Services', \"A-class Driver's License\", 'International environment experience (4-6 years)', 'FOCUS', 'Aerial Lift License', 'Basement floor of department store', 'Largest team managed in a government relations or public affairs role (More than 20 team members)', 'Brake repair', 'Marine Engine Driver Grade 1 near Coastal', 'Shifts & Schedules', 'Monthly variable working hours system', 'Ranch', 'Warrant Officer 1', 'Bike', 'New hire orientation', 'Acura certification', 'Odontology', 'Company bike', 'No PC skills', 'Semi-automatic filling machines', 'Meal voucher', 'Lodging discount', 'Manual Handling Training', 'Completed Apprenticeship for Organ & Harmonium Makers', 'Completed Apprenticeship for Petrol Station Attendants', 'Not Indeed', 'Vehicle maintenance', 'Certified Automotive Sales Agents', 'Fee for service', 'Annual early career full-time hires (11-20 hires)', 'Airframe & Powerplant License', 'Motorcycle License', 'Work ethic', 'Registration basis', 'CACES R489 Category 1B', 'Padding replacement (upholstery maintenance)', 'Maintaining an organized workspace', 'New Jersey', 'Automotive engineering', 'Chief Warrant Officer 5', 'Farmers trained on agricultural best practices (50-100 farmers)', 'Free snacks', \"C1E-class Driver's License\", \"Men's apparel shop\", 'Largest team managed in a government relations or public affairs role (1-5 team members)', \"Driver's License category A2\", 'GM vehicles', 'Annual early career full-time hires (41-50 hires)', \"Driver's License category B\", 'Automotive service', 'Enterprise Committee (EC)', 'Other (inventory management techniques)', 'Candidates placed with Fortune 500 companies (More than 30 candidates)', 'Arizona', '2 Wheeler License', 'Monthly shift schedule', 'Major General', 'Vinyl', 'Full-time healthcare role', 'Bombardier CRJ aircraft operation', 'Class 3 Motorcycle Mechanic License', 'Transmissions', '-- Increased revenue (16-20%)', 'In person', 'NAPA', 'CACES R482 Category D', 'Chief Mate Less than 3000 Gross Tonnage', 'Deputy branch manager (finance)', 'Category Q Licence', 'Professionalism', 'Automobile Mechanic License', 'Uniform provided', 'Motorcycle commute', 'High Altitude Work Vehicle Operation Skill Training Course', 'Asphalt', 'CACES R489 Category 5', 'High-speed ferry design projects', 'Annual Filing Season Program (AFSP)', 'Semi-Medium Vehicle License', 'Locksmithing', 'Automotive warranties', '40s can also apply', 'Accessories OK', 'Uniform', 'Multi-Engine Land (MEL) Rating', 'Mowing', 'Completed Apprenticeship for Plant Mechanics Specializing in Sanitary, Heating & Air Conditioning Systems', 'Paperless', 'General', 'Internal award for outstanding performance received (Two times)', 'Mashing', 'Candidates placed with Fortune 500 companies (21-30 candidates)', 'Ready to work', 'Other (active transportation licenses)', 'Marine Engine Driver Grade 3 near Coastal', 'Variety store', 'Largest team managed in a government relations or public affairs role (6-10 team members)', 'Lunchtime shift', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Rail & Road Traffic', 'Career services', 'Cover letter', 'Reduction in plastics production downtime (More than 20%)', 'Proper lifting technique adherence', 'Aircraft Dispatcher', 'Mixed martial arts', 'Infrequent customer interaction', '75%', 'QuickBooks Certified ProAdvisor', 'Inventory optimization', 'Near convenience store', 'Largest client employee count (5,000-25,000 employees)', 'Planer', 'Interview guaranteed', 'CACES R482 Category B2', 'Other (positions)', 'Sheet metal', 'Scaffold Assembler License', 'Precision Sheet Metal Operator', 'Petty Officer First Class', 'More than 3 month notice period', '0-15 day notice period', 'Louisiana', 'Other (automotive body repair & restoration skill)', 'Daily prescriptions dispensed (More than 300 prescriptions)', 'Other (Retail operations)', 'Pantry vouchers', 'Bookstore', 'Marine propulsion systems', 'CACES R489 Category 2A', 'Store discount', 'Warm-mix asphalt production', 'Mechanic experience', 'Completed Apprenticeship for Non-metallic Minerals Processing Mechanics', 'Furniture store', 'Major shipping companies collaborated with (More than 10 companies)', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Health Services', 'Frequent teamwork', 'Annual early career full-time hires (31-40 hires)', 'Completed Apprenticeship for Motor Vehicle Mechatronics Technicians', 'Lightspeed Retail', 'Youth Yell Certified', 'Rental store', 'YKB', 'OMVIC', 'Largest team managed in a technology role (11-20 team members)', 'Gulfstream G550 operation', 'Employee discount', 'Office cafeteria', \"Driver's License E\", 'No quota', 'High performance endorsement', 'Store opening/closing procedures', 'LOTO certification', 'Email Support', 'Bodybuilding training', 'Jobticket', 'SKUs managed (10,000-100,000 SKUs)', 'Dealership', 'Tyre fitting', 'Exit gift', 'Premium income generated ($100,000-$250,000)', 'Store management', 'Bicycle plan', 'Seating not assigned', 'Completed Apprenticeship for Administrative & Business Operations Specialists Specializing in Freight Forwarding & Logistics Services', 'Email automation', 'Internal award for outstanding performance received (Three times)', 'Rider Driver Licence', 'Ramen shop', 'Volkswagen Certification', 'Completed Apprenticeship for Roller Shutters & Sunshade Mechatronic Technicians', 'Occupation-based hire', 'Automotive lease products', 'Buttonhole creation', 'Cool Biz OK', 'South Carolina', 'Army Reserve', 'Negotiable work start schedule', 'Company classes', 'Farmers trained on agricultural best practices (1-50 farmers)', 'Smoking area available', 'Kentucky', 'Lift Platform License', 'Air Brake Endorsement', 'Tailwheel endorsement', 'Glider piloting', 'MYOB Certified Consultant', 'Able Seafarer  Engine Rating', 'Air brake', 'Tow truck', 'Personal Trainer FRISK License', 'Automotive repair', 'Category P Licence', 'Petty Officer Second Class', 'Direct mail', 'Prescriptions prepared and processed daily (More than 150 prescriptions)', 'Largest team managed in a government relations or public affairs role (11-20 team members)', 'LOTO', 'HMV License', '20-40%', 'Manager (Technology)', 'Completed Apprenticeship for Hydraulic Technicians']\n",
      "CSV file saved to: /home/jovyan/butterfly/src/notebooks/part3_evaluations/prediction_results_high_recall.csv\n",
      "Final Results - Average Precision: 0.0141, Average Recall: 0.7003, Average F1: 0.0275\n",
      "Recall below target, adjusting threshold and rerunning...\n",
      "Aggressive Results - Average Precision: 0.0141, Average Recall: 0.7003, Average F1: 0.0275\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_test/epoch_3\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "print(\"Loading SentenceTransformer model...\")\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "print(\"Loading definition data...\")\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create label text for semantic search (format: label: definition)\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "print(f\"Encoding {len(legacy_attributes)} legacy attributes...\")\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes), batch_size):\n",
    "    batch = legacy_attributes[i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "    if (i+batch_size) % 1000 == 0 or i+batch_size >= len(legacy_attributes):\n",
    "        print(f\"  Encoded {min(i+batch_size, len(legacy_attributes))}/{len(legacy_attributes)} attributes\")\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "print(f\"Creating FAISS index with dimension {dimension}...\")\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "print(\"Loading sentence data...\")\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "print(\"Columns in golden_dataset_sentences.csv:\", sentences_df.columns.tolist())\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Encode sentences in batches\n",
    "print(f\"Encoding {len(sentences_df)} sentences...\")\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_df), batch_size):\n",
    "    batch = sentences_df['Sentences'][i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "    if (i+batch_size) % 1000 == 0 or i+batch_size >= len(sentences_df):\n",
    "        print(f\"  Encoded {min(i+batch_size, len(sentences_df))}/{len(sentences_df)} sentences\")\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Optimized for recall: use high K, low threshold\n",
    "K = 200  # Number of candidates to retrieve per sentence\n",
    "T = 0.3  # Lower threshold to improve recall\n",
    "\n",
    "print(f\"Performing FAISS search with K={K} and threshold={T}...\")\n",
    "D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "\n",
    "# Process results efficiently\n",
    "job_predictions = {}\n",
    "for i, row in enumerate(sentences_df.itertuples()):\n",
    "    job_id = row.job_id\n",
    "    \n",
    "    if job_id not in job_predictions:\n",
    "        job_predictions[job_id] = set()\n",
    "    \n",
    "    # Get labels with similarity above threshold\n",
    "    for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "        if idx_pos >= 0 and similarity > T:\n",
    "            label = legacy_df['label'].iloc[idx_pos]\n",
    "            job_predictions[job_id].add(label)\n",
    "    \n",
    "# Convert sets to lists\n",
    "job_predictions = {job_id: list(labels) for job_id, labels in job_predictions.items()}\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"Loading ground truth data...\")\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "print(\"Columns in golden_dataset_labels.csv:\", labels_df.columns.tolist())\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "# Sample for debugging\n",
    "if ground_truth:\n",
    "    sample_job_id = next(iter(ground_truth))\n",
    "    print(f\"Sample job {sample_job_id}:\")\n",
    "    print(f\"  Ground truth: {ground_truth[sample_job_id]}\")\n",
    "    print(f\"  Predicted: {job_predictions.get(sample_job_id, [])}\")\n",
    "\n",
    "# Calculate metrics\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "output_data = []\n",
    "for job_id in ground_truth:\n",
    "    gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "    \n",
    "    pred = set(normalize_attribute(attr) for attr in job_predictions.get(job_id, []))\n",
    "    tp = len(gt.intersection(pred))\n",
    "    fp = len(pred - gt)\n",
    "    fn = len(gt - pred)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    output_data.append({\n",
    "        \"job_id\": job_id,\n",
    "        \"ground_truth\": \", \".join(sorted(ground_truth[job_id])),\n",
    "        \"predicted\": \", \".join(sorted(job_predictions.get(job_id, []))),\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    })\n",
    "\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"/home/jovyan/butterfly/src/notebooks/part3_evaluations\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the CSV file\n",
    "output_path = os.path.join(output_dir, \"prediction_results_high_recall.csv\")\n",
    "output_df.to_csv(output_path, index=False)\n",
    "print(f\"CSV file saved to: {output_path}\")\n",
    "\n",
    "avg_precision = output_df['precision'].mean()\n",
    "avg_recall = output_df['recall'].mean()\n",
    "avg_f1 = output_df['f1'].mean()\n",
    "\n",
    "print(f\"Final Results - Average Precision: {avg_precision:.4f}, Average Recall: {avg_recall:.4f}, Average F1: {avg_f1:.4f}\")\n",
    "\n",
    "# If recall is still below target, adjust threshold and rerun\n",
    "if avg_recall < 0.9:\n",
    "    print(\"Recall below target, adjusting threshold and rerunning...\")\n",
    "    \n",
    "    # Try with an even lower threshold\n",
    "    T_aggressive = 0.2\n",
    "    \n",
    "    job_predictions_aggressive = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions_aggressive:\n",
    "            job_predictions_aggressive[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above lower threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > T_aggressive:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions_aggressive[job_id].add(label)\n",
    "    \n",
    "    # Convert sets to lists\n",
    "    job_predictions_aggressive = {job_id: list(labels) for job_id, labels in job_predictions_aggressive.items()}\n",
    "    \n",
    "    # Calculate metrics for aggressive approach\n",
    "    output_data_aggressive = []\n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(normalize_attribute(attr) for attr in job_predictions_aggressive.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        output_data_aggressive.append({\n",
    "            \"job_id\": job_id,\n",
    "            \"ground_truth\": \", \".join(sorted(ground_truth[job_id])),\n",
    "            \"predicted\": \", \".join(sorted(job_predictions_aggressive.get(job_id, []))),\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        })\n",
    "    \n",
    "    output_df_aggressive = pd.DataFrame(output_data_aggressive)\n",
    "    \n",
    "    # Save the CSV file for aggressive approach\n",
    "    output_path_aggressive = os.path.join(output_dir, \"prediction_results_very_high_recall.csv\")\n",
    "    output_df_aggressive.to_csv(output_path_aggressive, index=False)\n",
    "    \n",
    "    avg_precision_agg = output_df_aggressive['precision'].mean()\n",
    "    avg_recall_agg = output_df_aggressive['recall'].mean()\n",
    "    avg_f1_agg = output_df_aggressive['f1'].mean()\n",
    "    \n",
    "    print(f\"Aggressive Results - Average Precision: {avg_precision_agg:.4f}, Average Recall: {avg_recall_agg:.4f}, Average F1: {avg_f1_agg:.4f}\")\n",
    "    \n",
    "    # Pick the best approach based on recall target\n",
    "    if avg_recall_agg >= 0.9:\n",
    "        print(\"Using aggressive approach to meet recall target\")\n",
    "        # You may want to save this as your final result if the recall target is crucial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbb3dea6-2e08-4f2e-b352-89129497a05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "Results with T=0.3 - Average Precision: 0.0141, Average Recall: 0.7003, Average F1: 0.0275\n",
      "Results with T=0.2 - Average Precision: 0.0141, Average Recall: 0.7003, Average F1: 0.0275\n",
      "Results with T=0.1 - Average Precision: 0.0141, Average Recall: 0.7003, Average F1: 0.0275\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_test/epoch_3\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create label text for semantic search (format: label: definition)\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes), batch_size):\n",
    "    batch = legacy_attributes[i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_df), batch_size):\n",
    "    batch = sentences_df['Sentences'][i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Optimized for recall: use high K, low threshold\n",
    "K = 200  # Number of candidates to retrieve per sentence\n",
    "T = 0.3  # Lower threshold to improve recall\n",
    "\n",
    "D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "\n",
    "# Process results efficiently\n",
    "job_predictions = {}\n",
    "for i, row in enumerate(sentences_df.itertuples()):\n",
    "    job_id = row.job_id\n",
    "    \n",
    "    if job_id not in job_predictions:\n",
    "        job_predictions[job_id] = set()\n",
    "    \n",
    "    # Get labels with similarity above threshold\n",
    "    for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "        if idx_pos >= 0 and similarity > T:\n",
    "            label = legacy_df['label'].iloc[idx_pos]\n",
    "            job_predictions[job_id].add(label)\n",
    "    \n",
    "# Convert sets to lists\n",
    "job_predictions = {job_id: list(labels) for job_id, labels in job_predictions.items()}\n",
    "\n",
    "# --- Evaluation ---\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "# Calculate metrics\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "output_data = []\n",
    "for job_id in ground_truth:\n",
    "    gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "    \n",
    "    pred = set(normalize_attribute(attr) for attr in job_predictions.get(job_id, []))\n",
    "    tp = len(gt.intersection(pred))\n",
    "    fp = len(pred - gt)\n",
    "    fn = len(gt - pred)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    output_data.append({\n",
    "        \"job_id\": job_id,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    })\n",
    "\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "avg_precision = output_df['precision'].mean()\n",
    "avg_recall = output_df['recall'].mean()\n",
    "avg_f1 = output_df['f1'].mean()\n",
    "\n",
    "print(f\"Results with T={T} - Average Precision: {avg_precision:.4f}, Average Recall: {avg_recall:.4f}, Average F1: {avg_f1:.4f}\")\n",
    "\n",
    "# If recall is still below target, adjust threshold and rerun\n",
    "if avg_recall < 0.9:\n",
    "    # Try with an even lower threshold\n",
    "    T_aggressive = 0.2\n",
    "    \n",
    "    job_predictions_aggressive = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions_aggressive:\n",
    "            job_predictions_aggressive[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above lower threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > T_aggressive:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions_aggressive[job_id].add(label)\n",
    "    \n",
    "    # Convert sets to lists\n",
    "    job_predictions_aggressive = {job_id: list(labels) for job_id, labels in job_predictions_aggressive.items()}\n",
    "    \n",
    "    # Calculate metrics for aggressive approach\n",
    "    output_data_aggressive = []\n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(normalize_attribute(attr) for attr in job_predictions_aggressive.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        output_data_aggressive.append({\n",
    "            \"job_id\": job_id,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        })\n",
    "    \n",
    "    output_df_aggressive = pd.DataFrame(output_data_aggressive)\n",
    "    \n",
    "    avg_precision_agg = output_df_aggressive['precision'].mean()\n",
    "    avg_recall_agg = output_df_aggressive['recall'].mean() \n",
    "    avg_f1_agg = output_df_aggressive['f1'].mean()\n",
    "    \n",
    "    print(f\"Results with T={T_aggressive} - Average Precision: {avg_precision_agg:.4f}, Average Recall: {avg_recall_agg:.4f}, Average F1: {avg_f1_agg:.4f}\")\n",
    "    \n",
    "    # If we need even more recall, try an extremely low threshold\n",
    "    if avg_recall_agg < 0.9:\n",
    "        T_extreme = 0.1\n",
    "        \n",
    "        job_predictions_extreme = {}\n",
    "        for i, row in enumerate(sentences_df.itertuples()):\n",
    "            job_id = row.job_id\n",
    "            \n",
    "            if job_id not in job_predictions_extreme:\n",
    "                job_predictions_extreme[job_id] = set()\n",
    "            \n",
    "            # Get labels with similarity above lower threshold\n",
    "            for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "                if idx_pos >= 0 and similarity > T_extreme:\n",
    "                    label = legacy_df['label'].iloc[idx_pos]\n",
    "                    job_predictions_extreme[job_id].add(label)\n",
    "        \n",
    "        # Convert sets to lists\n",
    "        job_predictions_extreme = {job_id: list(labels) for job_id, labels in job_predictions_extreme.items()}\n",
    "        \n",
    "        # Calculate metrics for extreme approach\n",
    "        output_data_extreme = []\n",
    "        for job_id in ground_truth:\n",
    "            gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "            \n",
    "            pred = set(normalize_attribute(attr) for attr in job_predictions_extreme.get(job_id, []))\n",
    "            tp = len(gt.intersection(pred))\n",
    "            fp = len(pred - gt)\n",
    "            fn = len(gt - pred)\n",
    "            precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "            recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "            \n",
    "            output_data_extreme.append({\n",
    "                \"job_id\": job_id,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1\n",
    "            })\n",
    "        \n",
    "        output_df_extreme = pd.DataFrame(output_data_extreme)\n",
    "        \n",
    "        avg_precision_ext = output_df_extreme['precision'].mean()\n",
    "        avg_recall_ext = output_df_extreme['recall'].mean()\n",
    "        avg_f1_ext = output_df_extreme['f1'].mean()\n",
    "        \n",
    "        print(f\"Results with T={T_extreme} - Average Precision: {avg_precision_ext:.4f}, Average Recall: {avg_recall_ext:.4f}, Average F1: {avg_f1_ext:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cedd0c03-2a26-40b4-a920-9d92abd922f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "Results with T=0.3 - Average Precision: 0.0139, Average Recall: 0.7011, Average F1: 0.0270\n",
      "Results with T=0.2 - Average Precision: 0.0136, Average Recall: 0.7061, Average F1: 0.0266\n",
      "Results with T=0.1 - Average Precision: 0.0136, Average Recall: 0.7061, Average F1: 0.0266\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_optimized/epoch_3\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create label text for semantic search (format: label: definition)\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes), batch_size):\n",
    "    batch = legacy_attributes[i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_df), batch_size):\n",
    "    batch = sentences_df['Sentences'][i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Optimized for recall: use high K, low threshold\n",
    "K = 200  # Number of candidates to retrieve per sentence\n",
    "T = 0.3  # Lower threshold to improve recall\n",
    "\n",
    "D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "\n",
    "# Process results efficiently\n",
    "job_predictions = {}\n",
    "for i, row in enumerate(sentences_df.itertuples()):\n",
    "    job_id = row.job_id\n",
    "    \n",
    "    if job_id not in job_predictions:\n",
    "        job_predictions[job_id] = set()\n",
    "    \n",
    "    # Get labels with similarity above threshold\n",
    "    for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "        if idx_pos >= 0 and similarity > T:\n",
    "            label = legacy_df['label'].iloc[idx_pos]\n",
    "            job_predictions[job_id].add(label)\n",
    "    \n",
    "# Convert sets to lists\n",
    "job_predictions = {job_id: list(labels) for job_id, labels in job_predictions.items()}\n",
    "\n",
    "# --- Evaluation ---\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "# Calculate metrics\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "output_data = []\n",
    "for job_id in ground_truth:\n",
    "    gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "    \n",
    "    pred = set(normalize_attribute(attr) for attr in job_predictions.get(job_id, []))\n",
    "    tp = len(gt.intersection(pred))\n",
    "    fp = len(pred - gt)\n",
    "    fn = len(gt - pred)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    output_data.append({\n",
    "        \"job_id\": job_id,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    })\n",
    "\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "avg_precision = output_df['precision'].mean()\n",
    "avg_recall = output_df['recall'].mean()\n",
    "avg_f1 = output_df['f1'].mean()\n",
    "\n",
    "print(f\"Results with T={T} - Average Precision: {avg_precision:.4f}, Average Recall: {avg_recall:.4f}, Average F1: {avg_f1:.4f}\")\n",
    "\n",
    "# If recall is still below target, adjust threshold and rerun\n",
    "if avg_recall < 0.9:\n",
    "    # Try with an even lower threshold\n",
    "    T_aggressive = 0.2\n",
    "    \n",
    "    job_predictions_aggressive = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions_aggressive:\n",
    "            job_predictions_aggressive[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above lower threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > T_aggressive:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions_aggressive[job_id].add(label)\n",
    "    \n",
    "    # Convert sets to lists\n",
    "    job_predictions_aggressive = {job_id: list(labels) for job_id, labels in job_predictions_aggressive.items()}\n",
    "    \n",
    "    # Calculate metrics for aggressive approach\n",
    "    output_data_aggressive = []\n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(normalize_attribute(attr) for attr in job_predictions_aggressive.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        output_data_aggressive.append({\n",
    "            \"job_id\": job_id,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        })\n",
    "    \n",
    "    output_df_aggressive = pd.DataFrame(output_data_aggressive)\n",
    "    \n",
    "    avg_precision_agg = output_df_aggressive['precision'].mean()\n",
    "    avg_recall_agg = output_df_aggressive['recall'].mean() \n",
    "    avg_f1_agg = output_df_aggressive['f1'].mean()\n",
    "    \n",
    "    print(f\"Results with T={T_aggressive} - Average Precision: {avg_precision_agg:.4f}, Average Recall: {avg_recall_agg:.4f}, Average F1: {avg_f1_agg:.4f}\")\n",
    "    \n",
    "    # If we need even more recall, try an extremely low threshold\n",
    "    if avg_recall_agg < 0.9:\n",
    "        T_extreme = 0.1\n",
    "        \n",
    "        job_predictions_extreme = {}\n",
    "        for i, row in enumerate(sentences_df.itertuples()):\n",
    "            job_id = row.job_id\n",
    "            \n",
    "            if job_id not in job_predictions_extreme:\n",
    "                job_predictions_extreme[job_id] = set()\n",
    "            \n",
    "            # Get labels with similarity above lower threshold\n",
    "            for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "                if idx_pos >= 0 and similarity > T_extreme:\n",
    "                    label = legacy_df['label'].iloc[idx_pos]\n",
    "                    job_predictions_extreme[job_id].add(label)\n",
    "        \n",
    "        # Convert sets to lists\n",
    "        job_predictions_extreme = {job_id: list(labels) for job_id, labels in job_predictions_extreme.items()}\n",
    "        \n",
    "        # Calculate metrics for extreme approach\n",
    "        output_data_extreme = []\n",
    "        for job_id in ground_truth:\n",
    "            gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "            \n",
    "            pred = set(normalize_attribute(attr) for attr in job_predictions_extreme.get(job_id, []))\n",
    "            tp = len(gt.intersection(pred))\n",
    "            fp = len(pred - gt)\n",
    "            fn = len(gt - pred)\n",
    "            precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "            recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "            \n",
    "            output_data_extreme.append({\n",
    "                \"job_id\": job_id,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1\n",
    "            })\n",
    "        \n",
    "        output_df_extreme = pd.DataFrame(output_data_extreme)\n",
    "        \n",
    "        avg_precision_ext = output_df_extreme['precision'].mean()\n",
    "        avg_recall_ext = output_df_extreme['recall'].mean()\n",
    "        avg_f1_ext = output_df_extreme['f1'].mean()\n",
    "        \n",
    "        print(f\"Results with T={T_extreme} - Average Precision: {avg_precision_ext:.4f}, Average Recall: {avg_recall_ext:.4f}, Average F1: {avg_f1_ext:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "621eb2cf-0ad5-4340-ada4-91f1698bd0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "Trying threshold: 0.7\n",
      "Sample job prediction count with T=0.7: 49\n",
      "Results with T=0.7 - Average Precision: 0.0497, Average Recall: 0.3543, Average F1: 0.0825\n",
      "\n",
      "Trying threshold: 0.5\n",
      "Sample job prediction count with T=0.5: 138\n",
      "Results with T=0.5 - Average Precision: 0.0405, Average Recall: 0.4383, Average F1: 0.0718\n",
      "\n",
      "Trying threshold: 0.4\n",
      "Sample job prediction count with T=0.4: 138\n",
      "Results with T=0.4 - Average Precision: 0.0400, Average Recall: 0.4405, Average F1: 0.0711\n",
      "\n",
      "Trying threshold: 0.3\n",
      "Sample job prediction count with T=0.3: 138\n",
      "Results with T=0.3 - Average Precision: 0.0400, Average Recall: 0.4405, Average F1: 0.0711\n",
      "\n",
      "Trying threshold: 0.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_930655/4102879015.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTrying threshold: {threshold}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mavg_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions_and_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     results[threshold] = {\n\u001b[1;32m    142\u001b[0m         \u001b[0;34m'precision'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;34m'recall'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_recall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_930655/4102879015.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(threshold)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Set number of candidates to retrieve - use a higher value for lower thresholds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlegacy_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# Get search results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mD_faiss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI_faiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Process results efficiently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mjob_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/faiss/class_wrappers.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/faiss/swigfaiss_avx2.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, n, x, k, distances, labels, params)\u001b[0m\n\u001b[1;32m   2367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2368\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexFlat_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_test/epoch_3\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create label text for semantic search (format: label: definition)\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes), batch_size):\n",
    "    batch = legacy_attributes[i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_df), batch_size):\n",
    "    batch = sentences_df['Sentences'][i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Function to get predictions with a specific threshold\n",
    "def get_predictions_and_metrics(threshold):\n",
    "    # Set number of candidates to retrieve - use a higher value for lower thresholds\n",
    "    K = min(30, len(legacy_df))\n",
    "    \n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results efficiently\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(label)\n",
    "    \n",
    "    # Convert sets to lists\n",
    "    job_predictions = {job_id: list(labels) for job_id, labels in job_predictions.items()}\n",
    "    \n",
    "    # Calculate metrics\n",
    "    labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "    \n",
    "    # Clean and filter ground truth labels\n",
    "    labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "    positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "    ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "    \n",
    "    def normalize_attribute(attr):\n",
    "        return attr.lower().strip()\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(normalize_attribute(attr) for attr in job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    # Print sample predictions for a job to verify threshold is working\n",
    "    if ground_truth:\n",
    "        sample_job_id = next(iter(ground_truth))\n",
    "        pred_count = len(job_predictions.get(sample_job_id, []))\n",
    "        print(f\"Sample job prediction count with T={threshold}: {pred_count}\")\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1, job_predictions\n",
    "\n",
    "# Try with different thresholds\n",
    "thresholds = [0.7,0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "results = {}\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(f\"\\nTrying threshold: {threshold}\")\n",
    "    avg_precision, avg_recall, avg_f1, predictions = get_predictions_and_metrics(threshold)\n",
    "    results[threshold] = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1': avg_f1,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "    print(f\"Results with T={threshold} - Average Precision: {avg_precision:.4f}, Average Recall: {avg_recall:.4f}, Average F1: {avg_f1:.4f}\")\n",
    "\n",
    "# Find the threshold that gives at least 0.9 recall, or the highest recall if none meet the target\n",
    "best_threshold = None\n",
    "best_recall = -1\n",
    "\n",
    "for threshold, metrics in results.items():\n",
    "    if metrics['recall'] >= 0.9 and (best_threshold is None or threshold > best_threshold):\n",
    "        best_threshold = threshold\n",
    "        best_recall = metrics['recall']\n",
    "    elif best_threshold is None and metrics['recall'] > best_recall:\n",
    "        best_threshold = threshold\n",
    "        best_recall = metrics['recall']\n",
    "\n",
    "if best_threshold is not None:\n",
    "    print(f\"\\nBest threshold: {best_threshold} with recall: {best_recall:.4f}\")\n",
    "    if best_recall < 0.9:\n",
    "        print(\"Note: Unable to achieve 0.9 recall with any tested threshold. You may need to try enhancing your model or data.\")\n",
    "else:\n",
    "    print(\"\\nUnable to find a suitable threshold. Consider checking your data or model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a2681-53a9-418e-b873-c8482e0af305",
   "metadata": {},
   "source": [
    "e5_mnrl_DDP_test/epoch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e79158b-6dfc-4a87-ba69-f57cb0428e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, T=0.9: Precision=0.1655, Recall=0.0888, F1=0.0988\n",
      "K=30, T=0.85: Precision=0.1091, Recall=0.1625, F1=0.1130\n",
      "K=30, T=0.8: Precision=0.0741, Recall=0.2352, F1=0.1024\n",
      "K=30, T=0.7: Precision=0.0497, Recall=0.3543, F1=0.0825\n",
      "K=30, T=0.5: Precision=0.0405, Recall=0.4383, F1=0.0718\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, T=0.9: Precision=0.1642, Recall=0.0894, F1=0.0983\n",
      "K=50, T=0.85: Precision=0.1061, Recall=0.1664, F1=0.1110\n",
      "K=50, T=0.8: Precision=0.0691, Recall=0.2468, F1=0.0974\n",
      "K=50, T=0.7: Precision=0.0425, Recall=0.3891, F1=0.0724\n",
      "K=50, T=0.5: Precision=0.0313, Recall=0.5039, F1=0.0575\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, T=0.9: Precision=0.1642, Recall=0.0894, F1=0.0983\n",
      "K=80, T=0.85: Precision=0.1052, Recall=0.1678, F1=0.1104\n",
      "K=80, T=0.8: Precision=0.0668, Recall=0.2523, F1=0.0949\n",
      "K=80, T=0.7: Precision=0.0374, Recall=0.4166, F1=0.0648\n",
      "K=80, T=0.5: Precision=0.0247, Recall=0.5645, F1=0.0465\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, T=0.9: Precision=0.1642, Recall=0.0894, F1=0.0983\n",
      "K=100, T=0.85: Precision=0.1052, Recall=0.1679, F1=0.1103\n",
      "K=100, T=0.8: Precision=0.0664, Recall=0.2536, F1=0.0945\n",
      "K=100, T=0.7: Precision=0.0354, Recall=0.4259, F1=0.0616\n",
      "K=100, T=0.5: Precision=0.0221, Recall=0.5914, F1=0.0421\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, T=0.9: Precision=0.1642, Recall=0.0894, F1=0.0983\n",
      "K=120, T=0.85: Precision=0.1052, Recall=0.1679, F1=0.1103\n",
      "K=120, T=0.8: Precision=0.0663, Recall=0.2542, F1=0.0944\n",
      "K=120, T=0.7: Precision=0.0340, Recall=0.4319, F1=0.0593\n",
      "K=120, T=0.5: Precision=0.0202, Recall=0.6103, F1=0.0386\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, T=0.9: Precision=0.1642, Recall=0.0894, F1=0.0983\n",
      "K=200, T=0.85: Precision=0.1052, Recall=0.1679, F1=0.1103\n",
      "K=200, T=0.8: Precision=0.0663, Recall=0.2547, F1=0.0944\n",
      "K=200, T=0.7: Precision=0.0314, Recall=0.4424, F1=0.0550\n",
      "K=200, T=0.5: Precision=0.0156, Recall=0.6568, F1=0.0303\n",
      "\n",
      "=== Best Configuration for Recall ===\n",
      "K=200, T=0.5\n",
      "Precision=0.0156, Recall=0.6568, F1=0.0303\n",
      "\n",
      "None of the tested configurations achieved 0.9 recall.\n",
      "Suggestions for improving recall:\n",
      "1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\n",
      "2. Try higher K values (e.g., 300, 500, 1000)\n",
      "3. Consider aggregating predictions from multiple threshold levels\n",
      "4. Examine specific jobs with low recall to identify patterns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_test/epoch_3\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create label text for semantic search (format: label: definition)\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes), batch_size):\n",
    "    batch = legacy_attributes[i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_df), batch_size):\n",
    "    batch = sentences_df['Sentences'][i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Function to get predictions and metrics for a specific K and threshold\n",
    "def get_metrics(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(label)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(normalize_attribute(attr) for attr in job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Test all combinations\n",
    "K_values = [30, 50, 80, 100, 120, 200]\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5]\n",
    "\n",
    "results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for threshold in thresholds:\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics(K, threshold)\n",
    "        results.append({\n",
    "            'K': K,\n",
    "            'Threshold': threshold,\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1': avg_f1\n",
    "        })\n",
    "        print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "# Find the best configuration for recall\n",
    "best_recall = -1\n",
    "best_config = None\n",
    "\n",
    "for result in results:\n",
    "    if result['Recall'] > best_recall:\n",
    "        best_recall = result['Recall']\n",
    "        best_config = result\n",
    "\n",
    "print(\"\\n=== Best Configuration for Recall ===\")\n",
    "print(f\"K={best_config['K']}, T={best_config['Threshold']}\")\n",
    "print(f\"Precision={best_config['Precision']:.4f}, Recall={best_config['Recall']:.4f}, F1={best_config['F1']:.4f}\")\n",
    "\n",
    "# If the best recall is still below 0.9, suggest other approaches\n",
    "if best_recall < 0.9:\n",
    "    print(\"\\nNone of the tested configurations achieved 0.9 recall.\")\n",
    "    print(\"Suggestions for improving recall:\")\n",
    "    print(\"1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\")\n",
    "    print(\"2. Try higher K values (e.g., 300, 500, 1000)\")\n",
    "    print(\"3. Consider aggregating predictions from multiple threshold levels\")\n",
    "    print(\"4. Examine specific jobs with low recall to identify patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38d6f7-584f-4a8b-8beb-f7ebed35aeff",
   "metadata": {},
   "source": [
    "e5_mnrl_DDP_test/epoch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "038637dc-2c0c-4794-b42e-77a3e7aaf26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, T=0.9: Precision=0.1120, Recall=0.1194, F1=0.1002\n",
      "K=30, T=0.85: Precision=0.0752, Recall=0.1979, F1=0.0946\n",
      "K=30, T=0.8: Precision=0.0568, Recall=0.2689, F1=0.0881\n",
      "K=30, T=0.7: Precision=0.0446, Recall=0.3686, F1=0.0753\n",
      "K=30, T=0.5: Precision=0.0390, Recall=0.4248, F1=0.0695\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, T=0.9: Precision=0.1112, Recall=0.1209, F1=0.1000\n",
      "K=50, T=0.85: Precision=0.0705, Recall=0.2067, F1=0.0905\n",
      "K=50, T=0.8: Precision=0.0505, Recall=0.2889, F1=0.0805\n",
      "K=50, T=0.7: Precision=0.0372, Recall=0.4118, F1=0.0643\n",
      "K=50, T=0.5: Precision=0.0302, Recall=0.4943, F1=0.0558\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, T=0.9: Precision=0.1110, Recall=0.1211, F1=0.0998\n",
      "K=80, T=0.85: Precision=0.0692, Recall=0.2090, F1=0.0890\n",
      "K=80, T=0.8: Precision=0.0459, Recall=0.3006, F1=0.0743\n",
      "K=80, T=0.7: Precision=0.0317, Recall=0.4459, F1=0.0557\n",
      "K=80, T=0.5: Precision=0.0238, Recall=0.5576, F1=0.0450\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, T=0.9: Precision=0.1110, Recall=0.1211, F1=0.0998\n",
      "K=100, T=0.85: Precision=0.0691, Recall=0.2096, F1=0.0890\n",
      "K=100, T=0.8: Precision=0.0445, Recall=0.3051, F1=0.0723\n",
      "K=100, T=0.7: Precision=0.0295, Recall=0.4577, F1=0.0520\n",
      "K=100, T=0.5: Precision=0.0211, Recall=0.5837, F1=0.0403\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, T=0.9: Precision=0.1110, Recall=0.1211, F1=0.0998\n",
      "K=120, T=0.85: Precision=0.0690, Recall=0.2096, F1=0.0889\n",
      "K=120, T=0.8: Precision=0.0438, Recall=0.3078, F1=0.0714\n",
      "K=120, T=0.7: Precision=0.0280, Recall=0.4676, F1=0.0496\n",
      "K=120, T=0.5: Precision=0.0194, Recall=0.6095, F1=0.0372\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, T=0.9: Precision=0.1110, Recall=0.1211, F1=0.0998\n",
      "K=200, T=0.85: Precision=0.0690, Recall=0.2096, F1=0.0889\n",
      "K=200, T=0.8: Precision=0.0433, Recall=0.3099, F1=0.0708\n",
      "K=200, T=0.7: Precision=0.0248, Recall=0.4869, F1=0.0440\n",
      "K=200, T=0.5: Precision=0.0151, Recall=0.6658, F1=0.0292\n",
      "\n",
      "=== Best Configuration for Recall ===\n",
      "K=200, T=0.5\n",
      "Precision=0.0151, Recall=0.6658, F1=0.0292\n",
      "\n",
      "None of the tested configurations achieved 0.9 recall.\n",
      "Suggestions for improving recall:\n",
      "1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\n",
      "2. Try higher K values (e.g., 300, 500, 1000)\n",
      "3. Consider aggregating predictions from multiple threshold levels\n",
      "4. Examine specific jobs with low recall to identify patterns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_test/epoch_2\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create label text for semantic search (format: label: definition)\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes), batch_size):\n",
    "    batch = legacy_attributes[i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_df), batch_size):\n",
    "    batch = sentences_df['Sentences'][i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Function to get predictions and metrics for a specific K and threshold\n",
    "def get_metrics(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(label)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(normalize_attribute(attr) for attr in job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Test all combinations\n",
    "K_values = [30, 50, 80, 100, 120, 200]\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5]\n",
    "\n",
    "results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for threshold in thresholds:\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics(K, threshold)\n",
    "        results.append({\n",
    "            'K': K,\n",
    "            'Threshold': threshold,\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1': avg_f1\n",
    "        })\n",
    "        print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "# Find the best configuration for recall\n",
    "best_recall = -1\n",
    "best_config = None\n",
    "\n",
    "for result in results:\n",
    "    if result['Recall'] > best_recall:\n",
    "        best_recall = result['Recall']\n",
    "        best_config = result\n",
    "\n",
    "print(\"\\n=== Best Configuration for Recall ===\")\n",
    "print(f\"K={best_config['K']}, T={best_config['Threshold']}\")\n",
    "print(f\"Precision={best_config['Precision']:.4f}, Recall={best_config['Recall']:.4f}, F1={best_config['F1']:.4f}\")\n",
    "\n",
    "# If the best recall is still below 0.9, suggest other approaches\n",
    "if best_recall < 0.9:\n",
    "    print(\"\\nNone of the tested configurations achieved 0.9 recall.\")\n",
    "    print(\"Suggestions for improving recall:\")\n",
    "    print(\"1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\")\n",
    "    print(\"2. Try higher K values (e.g., 300, 500, 1000)\")\n",
    "    print(\"3. Consider aggregating predictions from multiple threshold levels\")\n",
    "    print(\"4. Examine specific jobs with low recall to identify patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53025df2-a291-4815-be56-33782cc21255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, T=0.9: Precision=0.1120, Recall=0.1194, F1=0.1002\n",
      "K=30, T=0.85: Precision=0.0752, Recall=0.1979, F1=0.0946\n",
      "K=30, T=0.8: Precision=0.0568, Recall=0.2689, F1=0.0881\n",
      "K=30, T=0.7: Precision=0.0446, Recall=0.3686, F1=0.0753\n",
      "K=30, T=0.5: Precision=0.0390, Recall=0.4248, F1=0.0695\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, T=0.9: Precision=0.1112, Recall=0.1209, F1=0.1000\n",
      "K=50, T=0.85: Precision=0.0705, Recall=0.2067, F1=0.0905\n",
      "K=50, T=0.8: Precision=0.0505, Recall=0.2889, F1=0.0805\n",
      "K=50, T=0.7: Precision=0.0372, Recall=0.4118, F1=0.0643\n",
      "K=50, T=0.5: Precision=0.0302, Recall=0.4943, F1=0.0558\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, T=0.9: Precision=0.1110, Recall=0.1211, F1=0.0998\n",
      "K=80, T=0.85: Precision=0.0692, Recall=0.2090, F1=0.0890\n",
      "K=80, T=0.8: Precision=0.0459, Recall=0.3006, F1=0.0743\n",
      "K=80, T=0.7: Precision=0.0317, Recall=0.4459, F1=0.0557\n",
      "K=80, T=0.5: Precision=0.0238, Recall=0.5576, F1=0.0450\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, T=0.9: Precision=0.1110, Recall=0.1211, F1=0.0998\n",
      "K=100, T=0.85: Precision=0.0691, Recall=0.2096, F1=0.0890\n",
      "K=100, T=0.8: Precision=0.0445, Recall=0.3051, F1=0.0723\n",
      "K=100, T=0.7: Precision=0.0295, Recall=0.4577, F1=0.0520\n",
      "K=100, T=0.5: Precision=0.0211, Recall=0.5837, F1=0.0403\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, T=0.9: Precision=0.1110, Recall=0.1211, F1=0.0998\n",
      "K=120, T=0.85: Precision=0.0690, Recall=0.2096, F1=0.0889\n",
      "K=120, T=0.8: Precision=0.0438, Recall=0.3078, F1=0.0714\n",
      "K=120, T=0.7: Precision=0.0280, Recall=0.4676, F1=0.0496\n",
      "K=120, T=0.5: Precision=0.0194, Recall=0.6095, F1=0.0372\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, T=0.9: Precision=0.1110, Recall=0.1211, F1=0.0998\n",
      "K=200, T=0.85: Precision=0.0690, Recall=0.2096, F1=0.0889\n",
      "K=200, T=0.8: Precision=0.0433, Recall=0.3099, F1=0.0708\n",
      "K=200, T=0.7: Precision=0.0248, Recall=0.4869, F1=0.0440\n",
      "K=200, T=0.5: Precision=0.0151, Recall=0.6658, F1=0.0292\n",
      "\n",
      "=== Best Configuration for Recall ===\n",
      "K=200, T=0.5\n",
      "Precision=0.0151, Recall=0.6658, F1=0.0292\n",
      "\n",
      "None of the tested configurations achieved 0.9 recall.\n",
      "Suggestions for improving recall:\n",
      "1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\n",
      "2. Try higher K values (e.g., 300, 500, 1000)\n",
      "3. Consider aggregating predictions from multiple threshold levels\n",
      "4. Examine specific jobs with low recall to identify patterns\n"
     ]
    }
   ],
   "source": [
    "# COSINE SIMILARITY\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_test/epoch_2\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create label text for semantic search (format: label: definition)\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes), batch_size):\n",
    "    batch = legacy_attributes[i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_df), batch_size):\n",
    "    batch = sentences_df['Sentences'][i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Function to get predictions and metrics for a specific K and threshold\n",
    "def get_metrics(K, threshold):\n",
    "    # Calculate cosine similarity between all sentence and skill embeddings\n",
    "    similarity_matrix = cosine_similarity(sentence_embeddings, legacy_embeddings)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get top K similarities for this sentence\n",
    "        top_k_indices = np.argsort(similarity_matrix[i])[-K:][::-1]\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx in top_k_indices:\n",
    "            similarity = similarity_matrix[i][idx]\n",
    "            if similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx]\n",
    "                job_predictions[job_id].add(label)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(normalize_attribute(attr) for attr in job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Test all combinations\n",
    "K_values = [30, 50, 80, 100, 120, 200]\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5]\n",
    "\n",
    "results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for threshold in thresholds:\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics(K, threshold)\n",
    "        results.append({\n",
    "            'K': K,\n",
    "            'Threshold': threshold,\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1': avg_f1\n",
    "        })\n",
    "        print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "# Find the best configuration for recall\n",
    "best_recall = -1\n",
    "best_config = None\n",
    "\n",
    "for result in results:\n",
    "    if result['Recall'] > best_recall:\n",
    "        best_recall = result['Recall']\n",
    "        best_config = result\n",
    "\n",
    "print(\"\\n=== Best Configuration for Recall ===\")\n",
    "print(f\"K={best_config['K']}, T={best_config['Threshold']}\")\n",
    "print(f\"Precision={best_config['Precision']:.4f}, Recall={best_config['Recall']:.4f}, F1={best_config['F1']:.4f}\")\n",
    "\n",
    "# If the best recall is still below 0.9, suggest other approaches\n",
    "if best_recall < 0.9:\n",
    "    print(\"\\nNone of the tested configurations achieved 0.9 recall.\")\n",
    "    print(\"Suggestions for improving recall:\")\n",
    "    print(\"1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\")\n",
    "    print(\"2. Try higher K values (e.g., 300, 500, 1000)\")\n",
    "    print(\"3. Consider aggregating predictions from multiple threshold levels\")\n",
    "    print(\"4. Examine specific jobs with low recall to identify patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e869e813-929d-492a-b49f-47b9ae46d86e",
   "metadata": {},
   "source": [
    "20250408_173921_bi_encoder_finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "478df616-bca2-4198-ad46-1b3ec4597f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, T=0.9: Precision=0.1784, Recall=0.0105, F1=0.0186\n",
      "K=30, T=0.85: Precision=0.2715, Recall=0.0334, F1=0.0540\n",
      "K=30, T=0.8: Precision=0.2350, Recall=0.0684, F1=0.0928\n",
      "K=30, T=0.7: Precision=0.1415, Recall=0.1668, F1=0.1329\n",
      "K=30, T=0.5: Precision=0.0442, Recall=0.3797, F1=0.0764\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, T=0.9: Precision=0.1784, Recall=0.0105, F1=0.0186\n",
      "K=50, T=0.85: Precision=0.2715, Recall=0.0334, F1=0.0540\n",
      "K=50, T=0.8: Precision=0.2346, Recall=0.0685, F1=0.0927\n",
      "K=50, T=0.7: Precision=0.1390, Recall=0.1689, F1=0.1312\n",
      "K=50, T=0.5: Precision=0.0359, Recall=0.4146, F1=0.0641\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, T=0.9: Precision=0.1784, Recall=0.0105, F1=0.0186\n",
      "K=80, T=0.85: Precision=0.2715, Recall=0.0334, F1=0.0540\n",
      "K=80, T=0.8: Precision=0.2346, Recall=0.0685, F1=0.0927\n",
      "K=80, T=0.7: Precision=0.1384, Recall=0.1694, F1=0.1307\n",
      "K=80, T=0.5: Precision=0.0307, Recall=0.4368, F1=0.0557\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, T=0.9: Precision=0.1784, Recall=0.0105, F1=0.0186\n",
      "K=100, T=0.85: Precision=0.2715, Recall=0.0334, F1=0.0540\n",
      "K=100, T=0.8: Precision=0.2346, Recall=0.0685, F1=0.0927\n",
      "K=100, T=0.7: Precision=0.1384, Recall=0.1696, F1=0.1306\n",
      "K=100, T=0.5: Precision=0.0287, Recall=0.4446, F1=0.0524\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, T=0.9: Precision=0.1784, Recall=0.0105, F1=0.0186\n",
      "K=120, T=0.85: Precision=0.2715, Recall=0.0334, F1=0.0540\n",
      "K=120, T=0.8: Precision=0.2346, Recall=0.0685, F1=0.0927\n",
      "K=120, T=0.7: Precision=0.1384, Recall=0.1697, F1=0.1307\n",
      "K=120, T=0.5: Precision=0.0274, Recall=0.4496, F1=0.0502\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, T=0.9: Precision=0.1784, Recall=0.0105, F1=0.0186\n",
      "K=200, T=0.85: Precision=0.2715, Recall=0.0334, F1=0.0540\n",
      "K=200, T=0.8: Precision=0.2346, Recall=0.0685, F1=0.0927\n",
      "K=200, T=0.7: Precision=0.1384, Recall=0.1697, F1=0.1307\n",
      "K=200, T=0.5: Precision=0.0251, Recall=0.4595, F1=0.0462\n",
      "\n",
      "=== Best Configuration for Recall ===\n",
      "K=200, T=0.5\n",
      "Precision=0.0251, Recall=0.4595, F1=0.0462\n",
      "\n",
      "None of the tested configurations achieved 0.9 recall.\n",
      "Suggestions for improving recall:\n",
      "1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\n",
      "2. Try higher K values (e.g., 300, 500, 1000)\n",
      "3. Consider aggregating predictions from multiple threshold levels\n",
      "4. Examine specific jobs with low recall to identify patterns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/20250408_173921_bi_encoder_finetuned\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create label text for semantic search (format: label: definition)\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes), batch_size):\n",
    "    batch = legacy_attributes[i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_df), batch_size):\n",
    "    batch = sentences_df['Sentences'][i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Function to get predictions and metrics for a specific K and threshold\n",
    "def get_metrics(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(label)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(normalize_attribute(attr) for attr in job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Test all combinations\n",
    "K_values = [30, 50, 80, 100, 120, 200]\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5]\n",
    "\n",
    "results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for threshold in thresholds:\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics(K, threshold)\n",
    "        results.append({\n",
    "            'K': K,\n",
    "            'Threshold': threshold,\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1': avg_f1\n",
    "        })\n",
    "        print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "# Find the best configuration for recall\n",
    "best_recall = -1\n",
    "best_config = None\n",
    "\n",
    "for result in results:\n",
    "    if result['Recall'] > best_recall:\n",
    "        best_recall = result['Recall']\n",
    "        best_config = result\n",
    "\n",
    "print(\"\\n=== Best Configuration for Recall ===\")\n",
    "print(f\"K={best_config['K']}, T={best_config['Threshold']}\")\n",
    "print(f\"Precision={best_config['Precision']:.4f}, Recall={best_config['Recall']:.4f}, F1={best_config['F1']:.4f}\")\n",
    "\n",
    "# If the best recall is still below 0.9, suggest other approaches\n",
    "if best_recall < 0.9:\n",
    "    print(\"\\nNone of the tested configurations achieved 0.9 recall.\")\n",
    "    print(\"Suggestions for improving recall:\")\n",
    "    print(\"1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\")\n",
    "    print(\"2. Try higher K values (e.g., 300, 500, 1000)\")\n",
    "    print(\"3. Consider aggregating predictions from multiple threshold levels\")\n",
    "    print(\"4. Examine specific jobs with low recall to identify patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d5014f-6462-4f44-82db-f7b63f437851",
   "metadata": {},
   "source": [
    "e5_mnrl_DDP_test_continued/continued_epoch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0a0d261-7cde-4f19-b051-d70a3de6bb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, T=0.9: Precision=0.0154, Recall=0.1746, F1=0.0277\n",
      "K=30, T=0.85: Precision=0.0154, Recall=0.1999, F1=0.0280\n",
      "K=30, T=0.8: Precision=0.0154, Recall=0.2065, F1=0.0280\n",
      "K=30, T=0.7: Precision=0.0153, Recall=0.2085, F1=0.0279\n",
      "K=30, T=0.5: Precision=0.0153, Recall=0.2087, F1=0.0279\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, T=0.9: Precision=0.0130, Recall=0.2090, F1=0.0241\n",
      "K=50, T=0.85: Precision=0.0130, Recall=0.2466, F1=0.0243\n",
      "K=50, T=0.8: Precision=0.0129, Recall=0.2572, F1=0.0242\n",
      "K=50, T=0.7: Precision=0.0128, Recall=0.2603, F1=0.0240\n",
      "K=50, T=0.5: Precision=0.0128, Recall=0.2606, F1=0.0240\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, T=0.9: Precision=0.0111, Recall=0.2411, F1=0.0209\n",
      "K=80, T=0.85: Precision=0.0109, Recall=0.2944, F1=0.0208\n",
      "K=80, T=0.8: Precision=0.0108, Recall=0.3107, F1=0.0207\n",
      "K=80, T=0.7: Precision=0.0107, Recall=0.3156, F1=0.0205\n",
      "K=80, T=0.5: Precision=0.0107, Recall=0.3160, F1=0.0205\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, T=0.9: Precision=0.0103, Recall=0.2559, F1=0.0196\n",
      "K=100, T=0.85: Precision=0.0100, Recall=0.3166, F1=0.0193\n",
      "K=100, T=0.8: Precision=0.0099, Recall=0.3351, F1=0.0191\n",
      "K=100, T=0.7: Precision=0.0098, Recall=0.3414, F1=0.0189\n",
      "K=100, T=0.5: Precision=0.0098, Recall=0.3420, F1=0.0189\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, T=0.9: Precision=0.0096, Recall=0.2667, F1=0.0183\n",
      "K=120, T=0.85: Precision=0.0093, Recall=0.3355, F1=0.0180\n",
      "K=120, T=0.8: Precision=0.0092, Recall=0.3561, F1=0.0179\n",
      "K=120, T=0.7: Precision=0.0091, Recall=0.3635, F1=0.0176\n",
      "K=120, T=0.5: Precision=0.0091, Recall=0.3642, F1=0.0176\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, T=0.9: Precision=0.0081, Recall=0.2982, F1=0.0157\n",
      "K=200, T=0.85: Precision=0.0078, Recall=0.3919, F1=0.0152\n",
      "K=200, T=0.8: Precision=0.0076, Recall=0.4184, F1=0.0149\n",
      "K=200, T=0.7: Precision=0.0075, Recall=0.4293, F1=0.0147\n",
      "K=200, T=0.5: Precision=0.0075, Recall=0.4305, F1=0.0147\n",
      "\n",
      "=== Best Configuration for Recall ===\n",
      "K=200, T=0.5\n",
      "Precision=0.0075, Recall=0.4305, F1=0.0147\n",
      "\n",
      "None of the tested configurations achieved 0.9 recall.\n",
      "Suggestions for improving recall:\n",
      "1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\n",
      "2. Try higher K values (e.g., 300, 500, 1000)\n",
      "3. Consider aggregating predictions from multiple threshold levels\n",
      "4. Examine specific jobs with low recall to identify patterns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_DDP_test_continued/continued_epoch_1\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create label text for semantic search (format: label: definition)\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes), batch_size):\n",
    "    batch = legacy_attributes[i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_df), batch_size):\n",
    "    batch = sentences_df['Sentences'][i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Function to get predictions and metrics for a specific K and threshold\n",
    "def get_metrics(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(label)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(normalize_attribute(attr) for attr in job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Test all combinations\n",
    "K_values = [30, 50, 80, 100, 120, 200]\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5]\n",
    "\n",
    "results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for threshold in thresholds:\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics(K, threshold)\n",
    "        results.append({\n",
    "            'K': K,\n",
    "            'Threshold': threshold,\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1': avg_f1\n",
    "        })\n",
    "        print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "# Find the best configuration for recall\n",
    "best_recall = -1\n",
    "best_config = None\n",
    "\n",
    "for result in results:\n",
    "    if result['Recall'] > best_recall:\n",
    "        best_recall = result['Recall']\n",
    "        best_config = result\n",
    "\n",
    "print(\"\\n=== Best Configuration for Recall ===\")\n",
    "print(f\"K={best_config['K']}, T={best_config['Threshold']}\")\n",
    "print(f\"Precision={best_config['Precision']:.4f}, Recall={best_config['Recall']:.4f}, F1={best_config['F1']:.4f}\")\n",
    "\n",
    "# If the best recall is still below 0.9, suggest other approaches\n",
    "if best_recall < 0.9:\n",
    "    print(\"\\nNone of the tested configurations achieved 0.9 recall.\")\n",
    "    print(\"Suggestions for improving recall:\")\n",
    "    print(\"1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\")\n",
    "    print(\"2. Try higher K values (e.g., 300, 500, 1000)\")\n",
    "    print(\"3. Consider aggregating predictions from multiple threshold levels\")\n",
    "    print(\"4. Examine specific jobs with low recall to identify patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bbca6a-e0bf-4d7a-83b2-98b8541eb0ec",
   "metadata": {},
   "source": [
    "e5_mnrl_optimized/epoch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9adfdaf-66ea-4e72-9234-f61719d55207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for SentenceTransformer: NVIDIA A10G\n",
      "\n",
      "=== Testing with K=30 ===\n",
      "K=30, T=0.9: Precision=0.2178, Recall=0.0390, F1=0.0594\n",
      "K=30, T=0.85: Precision=0.1608, Recall=0.0795, F1=0.0930\n",
      "K=30, T=0.8: Precision=0.1316, Recall=0.1260, F1=0.1114\n",
      "K=30, T=0.7: Precision=0.0825, Recall=0.2354, F1=0.1127\n",
      "K=30, T=0.5: Precision=0.0432, Recall=0.4123, F1=0.0749\n",
      "\n",
      "=== Testing with K=50 ===\n",
      "K=50, T=0.9: Precision=0.2168, Recall=0.0390, F1=0.0592\n",
      "K=50, T=0.85: Precision=0.1588, Recall=0.0804, F1=0.0925\n",
      "K=50, T=0.8: Precision=0.1288, Recall=0.1281, F1=0.1100\n",
      "K=50, T=0.7: Precision=0.0782, Recall=0.2455, F1=0.1088\n",
      "K=50, T=0.5: Precision=0.0347, Recall=0.4623, F1=0.0619\n",
      "\n",
      "=== Testing with K=80 ===\n",
      "K=80, T=0.9: Precision=0.2168, Recall=0.0390, F1=0.0592\n",
      "K=80, T=0.85: Precision=0.1583, Recall=0.0805, F1=0.0922\n",
      "K=80, T=0.8: Precision=0.1276, Recall=0.1289, F1=0.1092\n",
      "K=80, T=0.7: Precision=0.0761, Recall=0.2502, F1=0.1067\n",
      "K=80, T=0.5: Precision=0.0291, Recall=0.5040, F1=0.0527\n",
      "\n",
      "=== Testing with K=100 ===\n",
      "K=100, T=0.9: Precision=0.2168, Recall=0.0390, F1=0.0592\n",
      "K=100, T=0.85: Precision=0.1583, Recall=0.0805, F1=0.0922\n",
      "K=100, T=0.8: Precision=0.1275, Recall=0.1289, F1=0.1091\n",
      "K=100, T=0.7: Precision=0.0753, Recall=0.2511, F1=0.1059\n",
      "K=100, T=0.5: Precision=0.0271, Recall=0.5198, F1=0.0491\n",
      "\n",
      "=== Testing with K=120 ===\n",
      "K=120, T=0.9: Precision=0.2168, Recall=0.0390, F1=0.0592\n",
      "K=120, T=0.85: Precision=0.1583, Recall=0.0805, F1=0.0922\n",
      "K=120, T=0.8: Precision=0.1275, Recall=0.1289, F1=0.1091\n",
      "K=120, T=0.7: Precision=0.0750, Recall=0.2518, F1=0.1054\n",
      "K=120, T=0.5: Precision=0.0254, Recall=0.5295, F1=0.0463\n",
      "\n",
      "=== Testing with K=200 ===\n",
      "K=200, T=0.9: Precision=0.2168, Recall=0.0390, F1=0.0592\n",
      "K=200, T=0.85: Precision=0.1583, Recall=0.0805, F1=0.0922\n",
      "K=200, T=0.8: Precision=0.1275, Recall=0.1289, F1=0.1091\n",
      "K=200, T=0.7: Precision=0.0747, Recall=0.2523, F1=0.1051\n",
      "K=200, T=0.5: Precision=0.0224, Recall=0.5497, F1=0.0410\n",
      "\n",
      "=== Best Configuration for Recall ===\n",
      "K=200, T=0.5\n",
      "Precision=0.0224, Recall=0.5497, F1=0.0410\n",
      "\n",
      "None of the tested configurations achieved 0.9 recall.\n",
      "Suggestions for improving recall:\n",
      "1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\n",
      "2. Try higher K values (e.g., 300, 500, 1000)\n",
      "3. Consider aggregating predictions from multiple threshold levels\n",
      "4. Examine specific jobs with low recall to identify patterns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability for the SentenceTransformer model\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU for SentenceTransformer: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"No GPUs available for SentenceTransformer, using CPU\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Bi encoder model path\n",
    "bi_encoder_model = \"/home/jovyan/butterfly/src/notebooks/e5_mnrl_optimized/epoch_3\"\n",
    "\n",
    "# Load the bi-encoder model\n",
    "model = SentenceTransformer(bi_encoder_model)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "# Load and prepare legacy attributes from 'defs.tsv'\n",
    "legacy_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/defs.tsv', sep='\\t')\n",
    "\n",
    "# Handle NaN values by replacing with empty strings\n",
    "legacy_df['label'] = legacy_df['label'].fillna('')\n",
    "legacy_df['definition'] = legacy_df['definition'].fillna('')\n",
    "\n",
    "# Create label text for semantic search (format: label: definition)\n",
    "legacy_attributes = legacy_df['label'] + \": \" + legacy_df['definition']\n",
    "\n",
    "# Encode with normalization, batch for large datasets\n",
    "batch_size = 128\n",
    "legacy_embeddings = []\n",
    "\n",
    "for i in range(0, len(legacy_attributes), batch_size):\n",
    "    batch = legacy_attributes[i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    legacy_embeddings.append(batch_embeddings)\n",
    "\n",
    "legacy_embeddings = np.vstack(legacy_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = legacy_embeddings.shape[1]\n",
    "\n",
    "# Use simple FAISS index optimized for inner product (cosine similarity with normalized vectors)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(legacy_embeddings)\n",
    "\n",
    "# Load sentences from 'golden_dataset_sentences.csv'\n",
    "sentences_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_sentences.csv')\n",
    "\n",
    "# Rename 'Job_ID' to 'job_id' for consistency\n",
    "sentences_df.rename(columns={'Job_ID': 'job_id'}, inplace=True)\n",
    "\n",
    "# Encode sentences in batches\n",
    "sentence_embeddings = []\n",
    "for i in range(0, len(sentences_df), batch_size):\n",
    "    batch = sentences_df['Sentences'][i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch, normalize_embeddings=True)\n",
    "    sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "sentence_embeddings = np.vstack(sentence_embeddings)\n",
    "\n",
    "# Load ground truth data\n",
    "labels_df = pd.read_csv('/home/jovyan/butterfly/src/notebooks/golden_dataset_labels.csv')\n",
    "\n",
    "# Clean and filter ground truth labels\n",
    "labels_df['display_name'] = labels_df['display_name'].str.lower().str.strip()\n",
    "positive_labels = labels_df[labels_df['label'] == 'POSITIVE']\n",
    "ground_truth = positive_labels.groupby('job_id')['display_name'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "\n",
    "def normalize_attribute(attr):\n",
    "    return attr.lower().strip()\n",
    "\n",
    "# Function to get predictions and metrics for a specific K and threshold\n",
    "def get_metrics(K, threshold):\n",
    "    # Get search results\n",
    "    D_faiss, I_faiss = index.search(sentence_embeddings, K)\n",
    "    \n",
    "    # Process results\n",
    "    job_predictions = {}\n",
    "    for i, row in enumerate(sentences_df.itertuples()):\n",
    "        job_id = row.job_id\n",
    "        \n",
    "        if job_id not in job_predictions:\n",
    "            job_predictions[job_id] = set()\n",
    "        \n",
    "        # Get labels with similarity above threshold\n",
    "        for idx_pos, similarity in zip(I_faiss[i], D_faiss[i]):\n",
    "            if idx_pos >= 0 and similarity > threshold:\n",
    "                label = legacy_df['label'].iloc[idx_pos]\n",
    "                job_predictions[job_id].add(label)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for job_id in ground_truth:\n",
    "        gt = set(normalize_attribute(attr) for attr in ground_truth[job_id])\n",
    "        \n",
    "        pred = set(normalize_attribute(attr) for attr in job_predictions.get(job_id, []))\n",
    "        tp = len(gt.intersection(pred))\n",
    "        fp = len(pred - gt)\n",
    "        fn = len(gt - pred)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1s)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Test all combinations\n",
    "K_values = [30, 50, 80, 100, 120, 200]\n",
    "thresholds = [0.9, 0.85, 0.8, 0.7, 0.5]\n",
    "\n",
    "results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n=== Testing with K={K} ===\")\n",
    "    for threshold in thresholds:\n",
    "        avg_precision, avg_recall, avg_f1 = get_metrics(K, threshold)\n",
    "        results.append({\n",
    "            'K': K,\n",
    "            'Threshold': threshold,\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1': avg_f1\n",
    "        })\n",
    "        print(f\"K={K}, T={threshold}: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n",
    "\n",
    "# Find the best configuration for recall\n",
    "best_recall = -1\n",
    "best_config = None\n",
    "\n",
    "for result in results:\n",
    "    if result['Recall'] > best_recall:\n",
    "        best_recall = result['Recall']\n",
    "        best_config = result\n",
    "\n",
    "print(\"\\n=== Best Configuration for Recall ===\")\n",
    "print(f\"K={best_config['K']}, T={best_config['Threshold']}\")\n",
    "print(f\"Precision={best_config['Precision']:.4f}, Recall={best_config['Recall']:.4f}, F1={best_config['F1']:.4f}\")\n",
    "\n",
    "# If the best recall is still below 0.9, suggest other approaches\n",
    "if best_recall < 0.9:\n",
    "    print(\"\\nNone of the tested configurations achieved 0.9 recall.\")\n",
    "    print(\"Suggestions for improving recall:\")\n",
    "    print(\"1. Try even lower thresholds (e.g., 0.4, 0.3, 0.2)\")\n",
    "    print(\"2. Try higher K values (e.g., 300, 500, 1000)\")\n",
    "    print(\"3. Consider aggregating predictions from multiple threshold levels\")\n",
    "    print(\"4. Examine specific jobs with low recall to identify patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97433a8f-ad0c-438f-8d5c-b1f1f635b61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
